# 3 Redis - 集群 (Cluster)

## 3.1 数据分片的实现方式

> 1. 在客户端实现相关分片逻辑
> 2. 把分片的逻辑抽离出来, 做出一个代理的服务
> 3. 在服务端实现相关分片逻辑

### 3.1.1 在客户端实现相关分片逻辑

![Alt 'ClientShardingModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/ClientShardingModel.png)

如图: 在客户端中通过分片逻辑, 路由到具体的 Redis 组中, 具体的代表有 Jedis。

使用 ShardedJedis 之类的客户端分片代码的优势是配置简单, 不依赖于其他中间件, 分区的逻辑可以自定义, 比较灵活。  
但是基于客户端的方案, 不能实现动态的服务增减, 每个客户端需要自行维护分片策略, 存在重复代码。


### 3.1.2. 将分片逻辑抽离为一个代理服务

![Alt 'ProxyServerModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/ProxyServerModel.png)

如图: 客户连接到代理服务 (或代理服务的集群), 代理服务根据分片逻辑, 再路由到具体的 Redis 组。

典型的代理分区方案有 Twitter 开源的 Twemproxy 和国内的豌豆荚开源的 Codis

#### 3.1.2.1 Twemproxy

![Alt 'TwitterTwemproxyModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/TwitterTwemproxyModel.png)

如图就是 TwemproxyModel 常用的架构图。

优点：比较稳定，可用性高。

缺点:
> 1. 出现故障不能自动转移, 架构复杂, 需要借助其他组件 (LVS/HAProxy + Keepalived) 实现 HA
> 2. 扩缩容需要修改配置, 不能实现平滑地扩缩容 (需要重新分布数据)

#### 3.1.2.2 Codis

![Alt 'CodisModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/CodisModel.png)

如图就是 Codis 常用的架构图。

Codis 配置了 N 个槽, 槽的个数等于后面的 Redis 组的个数。Codis 对 key 进行 CRC32 运算, 得到一个 32 位的数字, 然后模以 N (槽的个数), 得到余数。  
这个余数就是对应的槽, 槽后面就是具体的 Redis 实例或 Redis 分组。  

Codis 的槽位映射关系是保存在 Proxy 中的。如果要解决单点的问题, Codis 也要做集群部署, 多个 Codis 节点之间通过一个 Coordinator (例如: ZooKeeper) 进行槽，实例之间的同步。


### 3.1.3 在服务端实现相关分片逻辑

官方实现 Redis Cluster。

Redis Cluster 是在 Redis 3.0 的版本正式推出的, 用来解决分布式的需求，同时也可以实现高可用。  
跟 Codis 不一样, 它是去中心化的, 客户端可以连接到任意一个可用节点。



数据分片有几个关键的问题需要解决
> 1. 数据怎么相对均匀地分片
> 2. 客户端怎么访问到相应的节点和数据
> 3. 重新分片的过程, 怎么保证正常服务

## 3.2 Redis Cluster

Redis Cluster 可以看成是由多个 Redis 实例组成的数据集合。客户端不需要关注数据的子集到底存储在哪个节点, 只需要关注这个集合整体。

![Alt 'RedisClusterModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/RedisClusterModel.png)

如图就是一个三主三从的集群架构。

### 3.2.1 Redis Cluster 的配置

搭建一个三主三从的 Redis Cluster。  
这里用一台集群搭建集群, Ip 地址为 192.169.10.10, 7000/7001/7002 为主节点的端口, 8000/8001/8002 为对应的从节点的端口

**启动节点**  

修改 redis.conf 配置文件中的 3 个参数
> 1. cluster-enabled yes
> 2. cluster-config-file "node-7000.conf"
> 3. cluster-node-timeout 5000

其他的参数和单个 Redis 实例的一样。

**cluster-enabled yes**: Redis 实例可以分为单机模式 (standalone) 和集群模式 (cluster)。 yes 开启为集群模式。 

**cluster-config-file**: 该参数指定了集群配置文件的位置。每个节点在运行过程中, 会维护一份集群配置文件; 每当集群信息发生变化时 (如增减节点), 集群内所有节点会将最新信息更新到自己维护的配置文件。   
当节点重启后, 会重新读取该配置文件, 获取集群信息, 可以方便的重新加入到集群中。  
也就是说当 Redis 节点以集群模式启动时, 会首先寻找是否有集群配置文件, 如果有则使用文件中的配置启动, 如果没有, 则初始化配置并将配置保存到文件中。 集群配置文件由 Redis 节点维护, 不需要人工修改。

**cluster-node-timeout**: 节点之间心跳超时时间

通过 redis-server redis.conf 配置文件启动 Redis。

可以通过 **cluster nodes** 查看当前的节点集群信息

**执行 redis-cli --cluster create 命令**

节点启动以后是相互独立的，并不知道其他节点存在, 需要进行节点握手。将独立的节点组成一个网络。**注下面的操作, 不能使用 localhost 和 127.0.0.1, 需要使用局域网 Ip 或 公网 Ip**。

```sh
redis-cli --cluster create 192.169.10.10:7000 192.169.10.10:7001 192.169.10.10:7003 192.169.10.10:8000 192.169.10.10:8001 192.169.10.10:8002 --cluster-replicas 1
```

--cluster-replicas 1 表示每个主节点有 1 个从节点  
后面的多个 {ip:port} 表示节点地址, 前面的做主节点，后面的做从节点

执行创建命令后, Redis 会给出一个预计的方案, 对 6 个节点分配 3 主 3 从, 如果认为没有问题，输入 yes 确认

```sh
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 192.169.10.10:8001 to 192.169.10.10:7000
Adding replica 192.169.10.10:8002 to 192.169.10.10:7001
Adding replica 192.169.10.10:8003 to 192.169.10.10:7002
>>> Trying to optimize slaves allocation for anti-affinity
[WARNING] Some slaves are in the same host as their master
M: dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1c 192.169.10.10:7000
   slots:[0-5460] (5461 slots) master
M: 8c878b45905bba3d7366c89ec51bd0cd7ce959f8 192.169.10.10:7001
   slots:[5461-10922] (5462 slots) master
M: aeeb7d7076d9b25a7805ac6f508497b43887e599 192.169.10.10:7002
   slots:[10923-16383] (5461 slots) master
S: ebc479e609ff8f6ca9283947530919c559a08f80 192.169.10.10:8000
   replicates aeeb7d7076d9b25a7805ac6f508497b43887e599
S: 49385ed6e58469ef900ec48e5912e5f7b7505f6e 192.169.10.10:8001
   replicates dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1c
S: 8d6227aefc4830065624ff6c1dd795d2d5ad094a 192.169.10.10:8002
   replicates 8c878b45905bba3d7366c89ec51bd0cd7ce959f8
Can I set the above configuration? (type 'yes' to accept): 
```

### 3.2.2 Redis Cluster 相关的命令

集群命令

| 命令 |  效果 |
| :-: | :-: |
| cluster info| 打印集群的信息|
| cluster nodes | 列出集群当前已知的所有节点 (node), 以及这些节点的相关信息|
| cluster meet <ip> <port> | 将 ip 和 port 所指定的节点添加到集群当中, 让它成为集群的一份子, 这时候没有主从关系|
| cluster forget <node_id> | 从集群中移除 node_id 指定的节点 (保证空槽道) |
| cluster replicate <node_id> | 将当前节点设置为 node_id 指定的节点的从节点|
| cluster saveconfig | 将节点的配置文件保存到硬盘里面|

槽slot命令

| 命令 | 效果|
| :-: | :-:|
| cluster addslots [slot …] | 将一个或多个槽 (slot) 指派 (assign) 给当前节点|
| cluster delslots [slot …] | 移除一个或多个槽对当前节点的指派 |
| cluster flushslots | 移除指派给当前节点的所有槽, 让当前节点变成一个没有指派任何槽的节点 |
| cluster setslot node <node_id> | 将槽 slot 指派给 node_id 指定的节点, 如果槽已经指派给另一个节点, 那么先让另一个节点删除该槽, 然后再进行指派|
| cluster setslot migrating <node_id> | 将本节点的槽 slot 迁移到 node_id 指定的节点中|
| cluster setslot importing <node_id> | 从 node_id 指定的节点中导入槽 slot 到本节点 |
| cluster setslot stable | 取消对槽 slot 的导入 (imort) 或者迁移 (migrate) |

键命令

| 命令 | 效果 |
| :-: | :-: |
| cluster keyslot <key>| 计算键 key 应该被放置在哪个槽上 |
| cluster countkeysinslot <slot> | 返回槽 slot 目前包含的键值对数量 |
| cluster getkeysinslot <slot> <count> | 返回 count 个 slot 槽中的键 |



### 3.2.3 Redis Cluster 的配置方案

**高可用要求**  
根据故障转移的原理, 至少需要 3 个主节点才能完成故障转移, 且 3 个主节点不应在同一台物理机上。   
每个主节点至少需要 1 个从节点, 且主从节点不应在一台物理机上。因此高可用集群至少包含 6 个节点。

**数据量和访问量**  
估算应用需要的数据量和总访问量 (考虑业务发展, 留有冗余), 结合每个主节点的容量和能承受的访问量 (可以通过 benchmark 得到较准确估计), 计算需要的主节点数量

**节点数量限制**  
Redis 官方给出的节点数量限制为 1000, 主要是考虑节点间通信带来的消耗。在实际应用中应尽量避免大集群。  
如果节点数量不足以满足应用对 Redis 数据量和访问量的要求, 可以考虑: (1) 业务分割, 大集群分为多个小集群 (2) 减少不必要的数据 (3) 调整数据过期策略等

**适度冗余** 
Redis 可以在不影响集群服务的情况下增加节点, 因此节点数量适当冗余即可, 不用太大


### 3.2.3 Redis Cluster 的基本原理


#### 3.2.3.1 数据分区方案

数据分区有顺序分区, 哈希分区等。其中哈希分区由于其天然的随机性, 使用广泛。集群的分区方案便是哈希分区的一种。

哈希分区的基本思路是: 对数据的特征值 (如 Key) 进行哈希, 然后根据哈希值决定数据落在哪个节点。  
常见的哈希分区包括: 哈希取余分区, 一致性哈希分区, 带虚拟节点的一致性哈希分区等。

衡量数据分区方法好坏的标准有很多, 其中比较重要的两个因素是 **数据分布是否均匀**, **增加或删减节点对数据分布的影响**。  
由于哈希的随机性, 哈希分区基本可以保证数据分布均匀。 因此在比较哈希分区方案时, 重点要看增减节点对数据分布的影响。

**哈希取余分区**  
哈希取余分区思路非常简单: 计算 key 的 hash 值, 然后对节点数量进行取余, 从而决定数据映射到哪个节点上。  
该方案最大的问题是: 当新增或删减节点时, 节点数量发生变化, 系统中所有的数据都需要重新计算映射关系, 引发大规模数据迁移。

**一致性哈希分区**

![Alt 'ConsistentHashPartition'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/ConsistentHashPartition.png)

如图, 一致性哈希算法将整个哈希值空间组织成一个虚拟的圆环。 对于每个数据, 根据 key 计算 hash 值, 确定数据在环上的位置, 然后从此位置沿环顺时针行走, 找到的第一台服务器就是其应该映射到的服务器。

与 哈希取余分区相比, 一致性哈希分区将增减节点的影响限制在相邻节点。
以上图为例, 如果在 node1 和 node2 之间增加 node4 则只有 node2 中的一部分数据会迁移到 node4。  
如果去掉 node2, 则原 node2 中的数据只会迁移到 node3 中, 只有 node3 会受影响。

**带虚拟节点的一致性哈希分区**

该方案在一致性哈希分区的基础上, 引入了虚拟节点的概念, **Redis 集群使用的便是该方案, 其中的虚拟节点称为槽 (slot)**。   
槽是介于数据和实际节点之间的虚拟概念, 每个实际节点包含一定数量的槽, 每个槽包含哈希值在**一定范围内**的数据。  
引入槽以后, 数据的映射关系由数据 hash -> 实际节点, 变成了数据 hash -> 槽 -> 实际节点。









