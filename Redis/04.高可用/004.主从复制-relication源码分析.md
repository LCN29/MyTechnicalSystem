# 4 主从复制 - 源码

## 4.1 replicationUnsetMaster -  断开主从关系

```C
/**
 * 断开主从关系
 */ 
void replicationUnsetMaster(void) {

     // 没有设置主节点的, 直接返回
    if (server.masterhost == NULL) 
        return;

    // 释放主节点的 host 配置
    sdsfree(server.masterhost);
    server.masterhost = NULL;

    // 把当前的 replid 复制到 replid2
    // 重新生成一个新的 40 位随机 id 并赋值到 replid
    shiftReplicationId();

    // 释放代表主节点的 client 对象
    if (server.master) 
        freeClient(server.master);

    // 将 server.cached_server 设置为空, 同时设置释放对应的内存
    replicationDiscardCachedMaster();

    // 取消主从复制的握手
    cancelReplicationHandshake();

    // 关闭所有的从节点客户端
    disconnectSlaves();

    // 当前节点的主从复制状态设置为 none
    server.repl_state = REPL_STATE_NONE;

    // 从节点选中的数据库为 - 1
    server.slaveseldb = -1;
    server.repl_no_slaves_since = server.unixtime;
}
```

### 4.1.1 shiftReplicationId - 切换第一组 replid 和 offset 到第二组

```C
/**
 * 将第一组 replid 和  offset 复制到第二组
 * 重新生成一个新的 replid, 赋值给第一组的 replid 
 */
void shiftReplicationId(void) {

    // 将 replid 复制到 replid2
    memcpy(server.replid2,server.replid,sizeof(server.replid));

    // second_replid_offset = master_repl_offset + 1;
    server.second_replid_offset = server.master_repl_offset+1;
    
    // 重新生成一个 runId, 并赋值到 server.replid
    changeReplicationId();
    serverLog(LL_WARNING,"Setting secondary replication ID to %s, valid up to offset: %lld. New replication ID is %s", server.replid2, server.second_replid_offset, server.replid);
}
```

### 4.1.2 changeReplicationId - 重新生成一个 replid

```C
/**
 * 重新生成一个 40 位的 id, 并赋值到 replid
 */  
void changeReplicationId(void) {

    // CONFIG_RUN_ID_SIZE = 40, 重新随机生成一个 40 位的 id 并赋值到 replid
    getRandomHexChars(server.replid, CONFIG_RUN_ID_SIZE);
    server.replid[CONFIG_RUN_ID_SIZE] = '\0';
}
```

### 4.1.3 replicationDiscardCachedMaster - 清空缓存的 cached_master 

```C
/**
 * 置空 server.cached_master 
 */
void replicationDiscardCachedMaster(void) {

    if (server.cached_master == NULL) 
        return;

    serverLog(LL_NOTICE,"Discarding previously cached master state.");

    // 先设置为 cashed_master 的 flags = flag & (~CLIENT_MASTER), 其中 CLIENT_MASTER = 1<<1 = 1
    server.cached_master->flags &= ~CLIENT_MASTER;

    // 然后再释放 cached_master
    freeClient(server.cached_master);
    server.cached_master = NULL;
}
```

### 4.1.4 cancelReplicationHandshake -  取消主从复制的握手

```C
/**
 *  如果在传输 RDB 文件或者握手阶段, 进行操作的中断
 */ 
int cancelReplicationHandshake(void) {

    if (server.repl_state == REPL_STATE_TRANSFER) {
        // 1. 当前从节点的状态为 REPL_STATE_TRANSFER (正在接收从主节点发送过来的 RDB 文件)

        // 停止同步传输
        // 关闭从节点同步主节点的 socket 和临时文件描述符 repl_transfer_tmpfile
        replicationAbortSyncTransfer();

        // 更新状态为 REPL_STATE_CONNECT (未连接上主节点)
        server.repl_state = REPL_STATE_CONNECT;

    } else if (server.repl_state == REPL_STATE_CONNECTING || slaveIsInHandshakeState()) {
        // 2. 当前的状态为 REPL_STATE_CONNECTING (正在建立连接) 或者处于握手阶段

        // 关闭从节点同步主节点信息的 Socket, 对应的 Socket 文件描述符为 repl_transfer_s
        undoConnectWithMaster();

        // 更新状态为 REPL_STATE_CONNECT (未连接上主节点)
        server.repl_state = REPL_STATE_CONNECT;

    } else {
        // 3. 其他的情况 

        // 其他状态返回 0
        return 0;
    }

    return 1;
}
```

### 4.1.5 replicationAbortSyncTransfer - 停止文件同步传输

```C
/**
 * 停止同步传输, 
 * 关闭从节点同步主节点的 socket 和临时文件描述符 repl_transfer_tmpfile
 */ 
void replicationAbortSyncTransfer(void) {

    serverAssert(server.repl_state == REPL_STATE_TRANSFER);

    // 关闭从节点同步主节点信息的 Socket repl_transfer_s 和对应的文件描述符 repl_transfer_tmpfile
    undoConnectWithMaster();
    close(server.repl_transfer_fd);
    unlink(server.repl_transfer_tmpfile);
    zfree(server.repl_transfer_tmpfile);
}
```

### 4.1.6 undoConnectWithMaster - 断开和主节点的连接

```C
/**
 *  停止监听主节点 socket 的信息
 */ 
void undoConnectWithMaster(void) {

    int fd = server.repl_transfer_s;

    // 删除 repl_transfer_s 这个文件描述符的事件
    aeDeleteFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE);

    // 关闭这个文件描述符
    close(fd);

    // 重置为 - 1 
    server.repl_transfer_s = -1;
}
```

### 4.1.7 slaveIsInHandshakeState - 判断当前从节点是否处于握手阶段

```C
// 判断当前的从节点的状态是否处于握手阶段中
int slaveIsInHandshakeState(void) {

    // 3 <= server.repl_state <= 13 
    return server.repl_state >= REPL_STATE_RECEIVE_PONG && server.repl_state <= REPL_STATE_RECEIVE_PSYNC;
}
```

### 4.1.8 disconnectSlaves - 关闭所有的从节点

```C
/**
 * 关闭所有的从节点客户端 
 */
void disconnectSlaves(void) {

    while (listLength(server.slaves)) {

        listNode *ln = listFirst(server.slaves);
        freeClient((client*)ln->value);
    }
}
```

## 4.2 replicationSetMaster - 保存主节点信息

```C
void replicationSetMaster(char *ip, int port) {

    / 获取旧的主节点 IP 配置
    int was_master = server.masterhost == NULL;

    // 将入参的 IP 和 端口赋值给对应的字段
    sdsfree(server.masterhost);
    server.masterhost = sdsnew(ip);
    server.masterport = port;

    // 旧的主节点客户端存在, 进行释放
    if (server.master) {
        freeClient(server.master);
    }

    // 解除所有阻塞状态的客户端
    disconnectAllBlockedClients(); 

    // 释放所有的从节点信息 
    disconnectSlaves();

    //  取消主从复制的握手
    cancelReplicationHandshake();

    // 上次有主节点了, 可能的情景 (假设 A 原本是 B 的从节点, 现在通过 salveof 将 A 变成 C 的从节点)
    // 第一次执行时, 是不会走到这一步的
    if (was_master) {

        // 将 server.cached_server 设置为空, 同时设置释放对应的内存
        replicationDiscardCachedMaster();

        // 根据当前参数合成出一个 client, 同时将其放到 cached_master
        replicationCacheMasterUsingMyself();
    }

    // 设置当前的主从复制状态为待连接上主节点 (REPL_STATE_CONNECT)
    server.repl_state = REPL_STATE_CONNECT;
}
```

### 4.2.1 disconnectAllBlockedClients - 释放所有阻塞状态的客户端

```C
/**
 * 释放所有阻塞状态的客户端
 */ 
void disconnectAllBlockedClients(void) {

    listNode *ln;
    listIter li;

    // 将 server.clients 的信息转移到 li 上
    listRewind(server.clients,&li);

    // 遍历 li
    while((ln = listNext(&li))) {

        client *c = listNodeValue(ln);

        // 客户端为阻塞状态
        if (c->flags & CLIENT_BLOCKED) {
            
            addReplySds(c,sdsnew("-UNBLOCKED force unblock from blocking operation, instance state changed (master -> replica?)\r\n"));

            // 解除所有阻塞状态的客户端
            // unblockClient 会先将入参的 client 的标识 flag 进行，然后放到 server.unblocked_clients 这个列表中, 
            // 在下次事件循环中进行处理, 发送缓存区数据同时从列表中删除
            unblockClient(c);
            c->flags |= CLIENT_CLOSE_AFTER_REPLY;
        }
    }
}
```

### 4.2.2 replicationCacheMasterUsingMyself - 为 server.cached_master 赋值一个默认合成的客户端

```C
void replicationCacheMasterUsingMyself(void) {

    // 设置当前的 master_initial_offset 等于 master_repl_offset
    server.master_initial_offset = server.master_repl_offset;

    // 创建一个新的主节点客户端, 并存放发到 server.master 中
    replicationCreateMasterClient(-1,-1);

    // 将 server.replid 拷贝到创建出来的节点的 replid
    memcpy(server.master->replid, server.replid, sizeof(server.replid));

    // 通过 replicationCreateMasterClient 创建出来的客户端, unlinkClient 里面的逻辑都不符合条件, 直接结束了
    // 可以看成是没有这个方法的逻辑
    unlinkClient(server.master);

    // 设置 cached_master = master, 同时置空 master 
    server.cached_master = server.master;
    server.master = NULL;
    serverLog(LL_NOTICE,"Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.");
}
```

### 4.2.3 replicationCreateMasterClient - 创建主节点客户端

```C
/**
 *  创建主节点客户端
 */
void replicationCreateMasterClient(int fd, int dbid) {

    // 创建新的客户端, 赋值到 master, createClient 创建出来的 flag 默认为 0 
    // 具体的创建逻辑可以查看 networking.c 中的 createClient 函数
    server.master = createClient(fd);

    // 此时 flag = CLIENT_MASTER = 1
    server.master->flags |= CLIENT_MASTER;
    server.master->authenticated = 1;
    server.master->reploff = server.master_initial_offset;
    server.master->read_reploff = server.master->reploff;
    memcpy(server.master->replid, server.master_replid, sizeof(server.master_replid));

    // 如果一个主节点的偏移量为 -1, 那么这个主节点是旧的, 并且是无法进行 psync 的, 所以将其设置为 CLIENT_PRE_PSYNC (1 << 16, 65536)
    if (server.master->reploff == -1)
        // | CLIENT_PRE_PSYNC 后, flags 等于 65537
        server.master->flags |= CLIENT_PRE_PSYNC;

    if (dbid != -1) 
        selectDb(server.master, dbid);
}
```

## 4.3 connectWithMaster -  尝试和主节点建立 TCP 连接

```C
/**
 * 尝试和主节点建立 TCP 连接
 */ 
int connectWithMaster(void) {

    int fd;

    // 建立非阻塞的 Tcp 连接
    fd = anetTcpNonBlockBestEffortBindConnect(NULL,server.masterhost,server.masterport,NET_FIRST_BIND_ADDR);

    // 文件描述符为 -1, 建立连接失败
    if (fd == -1) {
        serverLog(LL_WARNING,"Unable to connect to MASTER: %s", strerror(errno));
        return C_ERR;
    }

    // 添加 fd 的可读和可写事件, 处理的逻辑函数为 syncWithMaster
    // 注意这里注册的类型为 AE_READABLE|AE_WRITABLE, 在 epoll 中, 底层注册的是 EPOLLIN | EPOLLOUT
    // epoll 有个机制, 同时注册 EPOLLIN | EPOLLOUT 事件, 会触发一次 EPOLLOUT 事件, 也就是 AE_WRITABLE 事件
    // 也就是在下次事件轮询中会执行一次 syncWithMaster 函数

    if (aeCreateFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE, syncWithMaster,NULL) == AE_ERR) {
        close(fd);
        serverLog(LL_WARNING,"Can't create readable event for SYNC");
        return C_ERR;
    }

    // 更新从节点和主节点最近一次进行数据同步的时间, 默认为当前时间
    server.repl_transfer_lastio = server.unixtime;
    server.repl_transfer_s = fd;
    server.repl_state = REPL_STATE_CONNECTING;
    return C_OK;
}
```

