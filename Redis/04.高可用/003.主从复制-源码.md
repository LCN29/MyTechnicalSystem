# 3 主从复制 - 源码

因为主从复制的过程很复杂, 同时集中在 **replication.c** 这个文件中, 所以为了篇幅, 单独的把功能中涉及的大部分代码都集中到了另一篇文章。
这篇文章中, 这篇主要涉及主从复制的大体代码逻辑, 如果需要了解整体的过程, 可以配合 [03.主从复制-replication源码分析](https://www.baidu.com/) 这篇文章。

## 3.1 主从节点建立连接

Redis 主从节点建立连接的 3 种方式, 本质都是从节点执行 **slaveof** 命令, 和父节点建立初步的关联关系。   
这个命令执行的方法为 **replicaofCommand**。(高版本的 Redis 可以通过 replicaof 达到 slaveof 的效果)

```C
void replicaofCommand(client *c) {

    // 开启了集群功能, 直接返回, 集群模式不允许执行 slaveof 
    if (server.cluster_enabled) {
        addReplyError(c,"REPLICAOF not allowed in cluster mode.");
        return;
    }

    // 第一参数为 no, 第二个参数为 one
    // slaveof no one, 可以让从节点和主节点断开连接, 停止主从复制 
    if (!strcasecmp(c->argv[1]->ptr,"no") && !strcasecmp(c->argv[2]->ptr,"one")) {

        // 如果保存了主节点 IP, 当前节点为某个节点的从节点
        if (server.masterhost) {
            
            // 取消复制操作, 同时设置当前节点为主节点
            // 1. 置空 server.masterhost 
            // 2. 将第一组 replid 和 offset 赋值到第二组, 重试生成一个 replid
            // 3. 置空 server.cached_server
            // 4. 如果在传输 RDB 文件中或者处于握手阶段, 进行取消, 同时取消和主节点的连接
            // 5. 如果有从节点, 释放所有的从节点客户端, 也就是断开从节点的连接
            // 6. 当前节点的状态变为 REPL_STATE_NONE (普通状态, 无主从复制状态)
            replicationUnsetMaster();

            // 获取 client 的每种信息, 并以 sds 形式返回, 并打印到日志中
            sds client = catClientInfoString(sdsempty(),c);
            serverLog(LL_NOTICE,"MASTER MODE enabled (user request from '%s')", client);
            sdsfree(client);
        }

    } else {

        // 当前的客户端的标识为从节点标识
        // 本身是一个从节点了, 无法在执行 salveof ip 端口
        if (c->flags & CLIENT_SLAVE) {

            addReplyError(c, "Command is not valid when client is a replica.");
            return;
        }

        // 从入参中获取端口
        if ((getLongFromObjectOrReply(c, c->argv[2], &port, NULL) != C_OK))
            return;

        // 已经有主节点了, 同时主节点的的 host 和 ip 和入参的相同
        if (server.masterhost && !strcasecmp(server.masterhost,c->argv[1]->ptr) && server.masterport == port) {    

            serverLog(LL_NOTICE,"REPLICAOF would result into synchronization with the master we are already connected with. No operation performed.");
            addReplySds(c,sdsnew("+OK Already connected to specified master\r\n"));
            return;
        }

        // 1. 保存主节点的 IP 和 端口到 server.masterhost 和 server.masterport
        // 2. 解除所有阻塞状态的客户端
        // 3. 释放所有的从节点信息
        // 4. 取消主从复制的握手操作
        // 5. 上次有主节点了, 为 server.cached_server 赋值一个默认生成的 client
        // 6. 设置当前的节点的状态为 REPL_STATE_CONNECT (待连接上主节点)
        replicationSetMaster(c->argv[1]->ptr, port);

        // 生成当前客户端的信息的字符串
        sds client = catClientInfoString(sdsempty(),c);
        serverLog(LL_NOTICE,"REPLICAOF %s:%d enabled (user request from '%s')", server.masterhost, server.masterport, client);
        sdsfree(client);    
    }

    addReply(c,shared.ok);
}
```

当前第一步主要的逻辑就是将当前的主节点的 IP 和 Post 保存起来, 同时经过这一步, 当前实例的复制状态设置为 REPL_STATE_CONNECT。
执行完上的逻辑后, salveof (replicaof) 就结束的, 但是整个的主从复制还没有开始, 可以得出 salveof 是一个异步的命令。 接下来的步骤则是由定时函数 serverCron 定时的调用。

## 3.2 主从网络连接建立

在第一步中, 只是将主节点的信息保存到从节点中就结束了, 之间还是没有建立起相关的网络连接的, 第二步就是完成这个网络连接的操作。
而这个网络连接建立的触发是通过定时函数执行的

```C
int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {

    // 省略

    // 定时 1 秒执行一次
    run_with_period(1000) replicationCron();

    // 省略
}
```

replicationCron 里面涉及到了大量的逻辑, 基本整个复制运行阶段的状态判断等都是在里面判断的, 这里只截取了涉及到当前步骤相关的逻辑。
在第一步操作完成后, 可以知道从节点当前的状态为 **REPL_STATE_CONNECT**。

```C
void replicationCron(void) {

    // 省略

    // 当前的状态为 REPL_STATE_CONNECT (开启了主从复制, 但是还没连接上主节点), 顺利执行了 salveof 后, 从节点的默认状态
    if (server.repl_state == REPL_STATE_CONNECT) {

        // 尝试连接主节点, 连接成功后, 从节点的状态会变为 REPL_STATE_CONNECTING (正在连接主节点)

        // 1. 通过保存的 IP 和 Port 和主节点建立一个 TCP 连接
        // 2. 向事件轮询添加对应的 Socket 通道的读写事件
        // 3. 更新当前节点的状态为 REPL_STATE_CONNECTING (正在连接主节点)
        if (connectWithMaster() == C_OK) {
            serverLog(LL_NOTICE, "MASTER <-> REPLICA sync started");
        }
    }

    // 省略
}
```


第二步的逻辑很简单, 和主节点建立起了 Socket 连接, 同时将当前节点的状态更新为 REPL_STATE_CONNECTING

## 3.3 发送 PING 命令

在第二步的步骤中, 通过保存的主节点 IP 和端口建立起连接后, 会向事件轮询中注册一个 AE_READABLE|AE_WRITABLE 的事件, 
在底层的 epollo 中就是同时注册了一个 EPOLLIN | EPOLLOUT 事件, 这样会触发一次 EPOLLOUT 事件, 也就是触发一次  AE_WRITABLE 事件,
也就是在下次事件轮询中会执行到其注册的函数 syncWithMaster 函数, 所以第三步的入口就是这个函数了。

同样的这个方法涉及到了大量的主从通信复制相关的逻辑, 整个逻辑很复杂, 所以也截取了相关的代码

```C
// 入参中的 fd 就是和主节点建立的 Socket 连接的文件描述符
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    int sockerr = 0;
    socklen_t errlen = sizeof(sockerr);
    
    UNUSED(el);
    UNUSED(privdata);
    UNUSED(mask);

    // 状态为 REPL_STATE_NONE, 关闭对应的文件描述符
    if (server.repl_state == REPL_STATE_NONE) {
        close(fd);
        return;
    }

    // 检查当前的 Socket 通道的状态
    if (getsockopt(fd, SOL_SOCKET, SO_ERROR, &sockerr, &errlen) == -1)
        // 获取异常信息
        sockerr = errno;

    // 有异常信息
    if (sockerr) {
        serverLog(LL_WARNING,"Error condition on socket for SYNC: %s", strerror(sockerr));
        goto error;
    }

    // 从节点和父节点建立了 Socket 后的第一个状态为 REPL_STATE_CONNECTING
    if (server.repl_state == REPL_STATE_CONNECTING) {

        serverLog(LL_NOTICE,"Non blocking connect for SYNC fired the event.");
        
        // 删除当前这个 Socket 的可写事件, 不关心写事件
        aeDeleteFileEvent(server.el,fd,AE_WRITABLE);

        // 状态修改为 REPL_STATE_RECEIVE_PONG (发送 pong, 等待 ping 回答)
        server.repl_state = REPL_STATE_RECEIVE_PONG;
        
        // 发送同步命令, 也就是 ping 到主节点, SYNC_CMD_WRITE = 1
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"PING",NULL);
        if (err) 
            goto write_error;
        return;
    }
}
```

从节点向主节点发送了一个 Ping 的命令, 这时候主节点收到了从节点的 Ping 命令后, 处理正常后, 会响应一个 Pong 的命令。

**主节点**执行的 Ping 命令的逻辑如下

```C
void pingCommand(client *c) {

    // ping 命令的参数只能是 1 个或者 0 个
    if (c->argc > 2) {
        addReplyErrorFormat(c,"wrong number of arguments for '%s' command", c->cmd->name);
        return;
    }

    // 对应的客户端处于 Pub/Sub 模式
    if (c->flags & CLIENT_PUBSUB) {
        addReply(c,shared.mbulkhdr[2]);
        addReplyBulkCBuffer(c,"pong",4);
        if (c->argc == 1)
            addReplyBulkCBuffer(c,"",0);
        else
            addReplyBulk(c,c->argv[1]);
    } else {

        // 其他模式
        // 参数是 1 个, 响应一个 pong
        if (c->argc == 1)
            addReply(c,shared.pong);
        else
            // 响应入参的第 2 个参数
            addReplyBulk(c,c->argv[1]);
    }
}
```

**从节点** 收到了主节点发送过来的 Pone 响应命令, 同时在上面第 2 步就建立了对应的可读事件, 这时事件轮询循环中找到了可读事件, 又执行到 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略
    
    // 当前从节点处于 REPL_STATE_RECEIVE_PONG 状态 (发送 ping, 等待 pong  应答)
    if (server.repl_state == REPL_STATE_RECEIVE_PONG) {

        // 读取主节点响应的信息
        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);

        // 异常情况
        if (err[0] != '+' && strncmp(err,"-NOAUTH",7) != 0 && strncmp(err,"-ERR operation not permitted",28) != 0) {
            serverLog(LL_WARNING,"Error reply to PING from master: '%s'",err);
            sdsfree(err);
            goto error;
        } else {
            // 响应的是 Pong, 响应了 Ping 请求, 能继续处理
            serverLog(LL_NOTICE, "Master replied to PING, replication can continue...");
        }

        sdsfree(err);
        // 状态切换到 REPL_STATE_SEND_AUTH, 等待认证结果应答
        server.repl_state = REPL_STATE_SEND_AUTH;
    }
}
```

## 3.4 认证权限

在从节点发送 Ping, 主节点响应 Pong , 从节点收到 Pong 响应后, 进入处理时 (syncWithMaster 函数), 
状态修改为 **REPL_STATE_SEND_AUTH** 后, 方法继续执行下去, 立即进入认证权限的过程。

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 当前从节点处于 REPL_STATE_RECEIVE_PONG 状态 (发送 ping, 等待 pong  应答)
    if (server.repl_state == REPL_STATE_RECEIVE_PONG) {

        // 省略

        // 状态切换到 REPL_STATE_SEND_AUTH, 等待认证结果应答
        server.repl_state = REPL_STATE_SEND_AUTH;
    }

    // 进入认证, 如果需要的话
    if (server.repl_state == REPL_STATE_SEND_AUTH) {

        // 配置了主节点的密码
        if (server.masterauth) {

            // 发送认证请求和密码到主节点
            err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"AUTH",server.masterauth,NULL);
            if (err) 
                goto write_error;

            // 状态切换为 REPL_STATE_RECEIVE_AUTH (等待认证结果响应)
            server.repl_state = REPL_STATE_RECEIVE_AUTH;
            return;
        } else {
            // 不需要认证, 状态之间切换为 REPL_STATE_SEND_PORT 准备发送端口
            server.repl_state = REPL_STATE_SEND_PORT;
        }
    }
}
```

**从节点**根据是否配置了主节点认证密码, 走不同的逻辑
> 1. 配置了认证密码, 发送 auth 密码, 同时带上密码到主节点, 同时状态变为 REPL_STATE_RECEIVE_AUTH (等待主节点响应认证结果应答)
> 2. 没有配置认证密码, 直接将状态变为 REPL_STATE_SEND_PORT (准备发送从节点的监听的端口)


**主节点**收到从节点的认证请求 auth, 就会进入到权限认证的过程,  执行的逻辑如下:

```C
void authCommand(client *c) {

    // 主节点不需要密码认证
    if (!server.requirepass) {
        addReplyError(c,"Client sent AUTH, but no password is set");
    } else if (!time_independent_strcmp(c->argv[1]->ptr, server.requirepass)) {
        // 密码认证成功  
        c->authenticated = 1;
        addReply(c,shared.ok);
    } else {
        // 密码认证失败  
        c->authenticated = 0;
        addReplyError(c,"invalid password");
    }
}
```

**主节点**收到从节点的 auth 命令后
> 1. 本身没有设置密码, 直接返回错误
> 2. 收到的密码和自身配置的密码一样, 返回成功
> 3. 收到的密码和自身配置的密码不一样, 返回错误

同 **Ping Pong** 的处理逻辑一样, 这时**从节点**读取到主节点的响应, 事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 当前从节点处于 REPL_STATE_RECEIVE_PONG 状态 (发送 ping, 等待 pong  应答)
    if (server.repl_state == REPL_STATE_RECEIVE_PONG) {
        // 省略

         // 状态切换为 REPL_STATE_RECEIVE_AUTH (等待认证结果响应)
        server.repl_state = REPL_STATE_RECEIVE_AUTH;
        return;
    }

    // 接收到请求认证的响应
    if (server.repl_state == REPL_STATE_RECEIVE_AUTH) {

        // 读取响应信息
        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);
        
        // 认证失败
        if (err[0] == '-') {
            serverLog(LL_WARNING,"Unable to AUTH to MASTER: %s",err);
            sdsfree(err);
            goto error;
        }

        // 认证成功
        sdsfree(err);
        // 状态变为 REPL_STATE_SEND_PORT (准备发送从节点的监听的端口)
        server.repl_state = REPL_STATE_SEND_PORT;
    }
}
```

## 3.5 发送端口号

在不需要权限认证或者从节点收到主节点的权限认证成功后, 此时从节点的状态为 **REPL_STATE_SEND_PORT**, 顺着上一步的处理逻辑中, 继续处理

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 收到权限认证的响应
    if (server.repl_state == REPL_STATE_RECEIVE_AUTH) {

        // 配置了主节点的密码
        if (server.masterauth) {
            // 省略
        } else {
            // 不需要认证, 状态之间切换为 REPL_STATE_SEND_PORT 准备发送端口
            server.repl_state = REPL_STATE_SEND_PORT;
        }
    }
    
    // 接收到请求认证的响应
    if (server.repl_state == REPL_STATE_RECEIVE_AUTH) {
        // 省略
        // 状态变为 REPL_STATE_SEND_PORT (准备发送从节点的监听的端口)
        server.repl_state = REPL_STATE_SEND_PORT;
    }

    // 进入发送端口阶段
    if (server.repl_state == REPL_STATE_SEND_PORT) {

        // 如果有配置一个专门复制的端口的话, 使用配置的端口, 没有使用当前服务器的端口
        sds port = sdsfromlonglong(server.slave_announce_port ? server.slave_announce_port : server.port);

        // 发送端口信息给主节点 命令: replconf listening-port 端口
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"REPLCONF", "listening-port",port, NULL);
        sdsfree(port);
        if (err) 
            goto write_error;
        sdsfree(err);
        // 切换状态为 REPL_STATE_RECEIVE_PORT ()
        server.repl_state = REPL_STATE_RECEIVE_PORT;
        return;
    }
}
```

**主节点**收到从节点的发送端口请求 REPLCONF, 执行的逻辑如下

```C
void replconfCommand(client *c) {

    int j;

    // 参数需要是 2 的倍数
    if ((c->argc % 2) == 0) {
        addReply(c,shared.syntaxerr);
        return;
    }

    for (j = 1; j < c->argc; j+=2) {

        // 每个循环使用 2 个参数

        // replconf listening-port port
        if (!strcasecmp(c->argv[j]->ptr,"listening-port")) {

            long port;

            // 获取下一个项, 也就是端口号
            if ((getLongFromObjectOrReply(c,c->argv[j+1], &port,NULL) != C_OK))
                return;

            // 保存到对应的客户端的 slave_listening_port     
            c->slave_listening_port = port;
        } else if (!strcasecmp(c->argv[j]->ptr,"ip-address")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"capa")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ack")) {
            // 省略
        }  else if (!strcasecmp(c->argv[j]->ptr,"getack")) {
            // 省略
        } else {
            // 响应错误
            addReplyErrorFormat(c,"Unrecognized REPLCONF option: %s", (char*)c->argv[j]->ptr);
            return;
        }
    }

    // 响应 OK 
    addReply(c,shared.ok);
}
```

可以看到主节点收到从节点发送过来的端口, 会保存到从节点客户端 client 的 slave_listening_port 字段。

收到主节点的响应后, 从节点同样是事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 进入发送端口阶段
    if (server.repl_state == REPL_STATE_SEND_PORT) {

        // 省略

        // 切换状态为 REPL_STATE_RECEIVE_PORT ()
        server.repl_state = REPL_STATE_RECEIVE_PORT;
        return;
    }

    // REPL_STATE_RECEIVE_PORT 等待主节点响应发送 IP 请求的响应
    if (server.repl_state == REPL_STATE_RECEIVE_PORT) {

        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);
        if (err[0] == '-') {
            serverLog(LL_NOTICE,"(Non critical) Master does not understand REPLCONF listening-port: %s", err);
        }
        sdsfree(err);
        // 状态变为 REPL_STATE_SEND_IP, 准备发送 IP 到主节点
        server.repl_state = REPL_STATE_SEND_IP;
    }

}
```

## 3.6 发送 IP 地址

收到主节点对 **REPLCONF listening-port 端口** 的响应后, 从节点会将状态修改为 **REPL_STATE_SEND_IP**, 然后顺着逻辑走下去

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // REPL_STATE_RECEIVE_PORT 等待主节点响应发送 IP 请求的响应
    if (server.repl_state == REPL_STATE_RECEIVE_PORT) {
        // 省略

        // 状态变为 REPL_STATE_SEND_IP, 准备发送 IP 到主节点
        server.repl_state = REPL_STATE_SEND_IP;
    }

    // 没有配置宣布的 IP, slave_announce_ip 为空, 直接跳过发送 IP 的阶段
    if (server.repl_state == REPL_STATE_SEND_IP && server.slave_announce_ip == NULL) {
        // 进入下一个节点 准备发送从节点的发送能力
        server.repl_state = REPL_STATE_SEND_CAPA;
    }

    if (server.repl_state == REPL_STATE_SEND_IP) {

        // 发送 REPLCONF ip-address ip
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"REPLCONF", "ip-address",server.slave_announce_ip, NULL);
        if (err) 
            goto write_error;
        sdsfree(err);

        // 状态变为 REPL_STATE_RECEIVE_IP (等待主节点响应收到从节点的 IP 地址)
        server.repl_state = REPL_STATE_RECEIVE_IP;
        return;
    }
}

```

从节点进入发送 IP 地址阶段时, 除了状态需要为 REPL_STATE_SEND_IP (准备发送 IP 地址阶段), 还必须有指定 slave_announce_ip, 从节点的 IP (对应配置文件的 slave-announce-ip),  
2 个条件都满足的情况下, 才会真正的进入发送 IP 地址, 否则直接进入下一阶段。

**主节点**收到从节点的发送的 **REPLCONF ip-address IP** 请求, 执行的逻辑如下

```C
void replconfCommand(client *c) {

    int j;

    // 参数需要是 2 的倍数
    if ((c->argc % 2) == 0) {
        addReply(c,shared.syntaxerr);
        return;
    }

    
    for (j = 1; j < c->argc; j+=2) {
        // 每个循环使用 2 个参数

         if (!strcasecmp(c->argv[j]->ptr,"listening-port")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ip-address")) {

            // replconf ip-address ip

            // 获取对应的从节点发送的 IP 
            sds ip = c->argv[j+1]->ptr;

            // IP 的长度判断
            if (sdslen(ip) < sizeof(c->slave_ip)) {
                // 保存到客户端的 client 的 slave_ip 属性
                memcpy(c->slave_ip,ip,sdslen(ip)+1);
            } else {
                // 错误提示
                addReplyErrorFormat(c,"REPLCONF ip-address provided by replica instance is too long: %zd bytes", sdslen(ip));
                return;
            }

        } else if (!strcasecmp(c->argv[j]->ptr,"capa")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ack")) {
            // 省略
        }  else if (!strcasecmp(c->argv[j]->ptr,"getack")) {
            // 省略
        } else {
            // 响应错误
            addReplyErrorFormat(c,"Unrecognized REPLCONF option: %s", (char*)c->argv[j]->ptr);
            return;
        }

    }

    // 响应 OK 
    addReply(c,shared.ok);
}
```

主节点收到从节点发送的 IP 地址, 会将其保存到从节点的客户端 client 的 slave_ip 字段。

**从节点**收到主节点的响应后, 同样是由事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 收到主节点对发送 IP 请求的响应
    if (server.repl_state == REPL_STATE_RECEIVE_IP) {

        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);

        if (err[0] == '-') {
            serverLog(LL_NOTICE,"(Non critical) Master does not understand REPLCONF ip-address: %s", err);
        }
        sdsfree(err);

        // 状态变为等待发送发送能力状态
        server.repl_state = REPL_STATE_SEND_CAPA;
    }
}
```

## 3.7 发送同步能力 (数据同步的方式)

收到主节点响应的 IP 请求, 从节点的状态切换为了 REPL_STATE_SEND_CAPA, 
如果从节点没有配置  slave-announce-ip, 也就不会有发送 IP 相关的操作, 也会直接过度到 REPL_STATE_SEND_CAPA,

状态切换到 REPL_STATE_SEND_CAPA 后, 会继续下面的逻辑, 同样在 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 没有配置宣布的 IP, slave_announce_ip 为空, 直接跳过发送 IP 的阶段
    if (server.repl_state == REPL_STATE_SEND_IP && server.slave_announce_ip == NULL) {
        // 进入下一个节点 准备发送从节点的发送能力
        server.repl_state = REPL_STATE_SEND_CAPA;
    }

    // 省略

    // 收到主节点对发送 IP 请求的响应
    if (server.repl_state == REPL_STATE_RECEIVE_IP) {

        // 状态变为等待发送发送能力状态
        server.repl_state = REPL_STATE_SEND_CAPA;
    }


    if (server.repl_state == REPL_STATE_SEND_CAPA) {

        // 发送从节点支持的发送能力到主节点 
        // REPLCONF capa eof capa psync2
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"REPLCONF", "capa","eof","capa","psync2",NULL);
        if (err) goto write_error;
        sdsfree(err);
        // 状态修改为 REPL_STATE_RECEIVE_CAPA, 等待主节点响应发送能力的应答
        server.repl_state = REPL_STATE_RECEIVE_CAPA;
        return;
    }
}
```

从节点发送过去的支持的 2 种发送能力
> 1. eof: 全量复制, 能够解析出 RDB 文件的 EOF 流格式
> 2. psync2: 部分复制, 利用复制积压缓冲区等实现部分同步


**主节点**收到从节点的发送能力请求 REPLCONF, 执行的逻辑如下

```C
void replconfCommand(client *c) {

    int j;

    // 参数需要是 2 的倍数
    if ((c->argc % 2) == 0) {
        addReply(c,shared.syntaxerr);
        return;
    }

    for (j = 1; j < c->argc; j+=2) {

        // 每个循环使用 2 个参数

        if (!strcasecmp(c->argv[j]->ptr,"listening-port")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ip-address")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"capa")) {

            if (!strcasecmp(c->argv[j+1]->ptr,"eof"))
                // SLAVE_CAPA_EOF = 1
                c->slave_capa |= SLAVE_CAPA_EOF;
            else if (!strcasecmp(c->argv[j+1]->ptr,"psync2"))
                // SLAVE_CAPA_PSYNC2 = 2 
                c->slave_capa |= SLAVE_CAPA_PSYNC2;

            // 如果不支持的能力, 不做处理
        } else if (!strcasecmp(c->argv[j]->ptr,"ack")) {
            // 省略
        }  else if (!strcasecmp(c->argv[j]->ptr,"getack")) {
            // 省略
        } else {
            // 响应错误
            addReplyErrorFormat(c,"Unrecognized REPLCONF option: %s", (char*)c->argv[j]->ptr);
            return;
        }
    }
    
    // 响应 OK 
    addReply(c,shared.ok);
}
```

**从节点**收到主节点的响应后, 从节点同样是事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {
    
    // 省略

    // 读取主节点发送过来的信息
    if (server.repl_state == REPL_STATE_RECEIVE_CAPA) {
        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);

        if (err[0] == '-') {
            serverLog(LL_NOTICE,"(Non critical) Master does not understand REPLCONF capa: %s", err);
        }
        sdsfree(err);

        // 向主节点发送 psync 命令, 请求全量复制
        server.repl_state = REPL_STATE_SEND_PSYNC;
    }

    // 省略
}
```

## 3.8 发送 PSYNC 命令

**从节点**收到了主节点对其同步能力的响应后, 这是会发送一个 psync 的命令给主节点, 这个请求就是同步复制的真正开始了

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 读取主节点发送过来的信息
    if (server.repl_state == REPL_STATE_RECEIVE_CAPA) {

        // 省略

        // 向主节点发送 psync 命令, 请求全量复制
        server.repl_state = REPL_STATE_SEND_PSYNC;
    }

    // 发送 psync 命令
    if (server.repl_state == REPL_STATE_SEND_PSYNC) {

        // 入参的 0 表示写消息给主节点, 1 表示从主节点读取数据
        // 入参 0 的逻辑, 根据当前是否缓存了主节点, 既 cached_master 是否为空, 来发送 psync 命令, 为空, 发送全量同步请求, 不为空, 发送部分同步请求
        if (slaveTryPartialResynchronization(fd,0) == PSYNC_WRITE_ERROR) {
            err = sdsnew("Write error sending the PSYNC command.");
            goto write_error;
        }

        // 切换状态为 REPL_STATE_RECEIVE_PSYNC (等待 psync 应答)
        server.repl_state = REPL_STATE_RECEIVE_PSYNC;
        return;
    }
}
```

slaveTryPartialResynchronization 函数可以根据入参进行写消息给主节点和读取主节点消息, 因为上面只涉及到写的操作，下面梳理的是写部分的逻辑
> 1. 声明了 2 个变量 psync_replid 和 psync_offset, 前面的用来存储节点的 replid, 后者存储的是同步复制积压缓冲区的同步到的位置
> 2. 首先根据自身保存的 server.cached_master 为空, 得出 2 个变量的值
> 3. 如果 server.cached_master 不为空, psync_replid 等于 cached_master.replid, psync_offset 等于 cached_master.reploff + 1
> 4. 如果 server.cached_master 为空, psync_replid 等于 ?, psync_offset 等于 -1
> 5. server.cached_master 不为空, 表示从节点上次是有主节点的, 当前可能是重启等情况, 导致从节点重新走了一次复制的流程, 可以尝试进行部分复制, 不进行全量复制
> 6. 向主节点发送 psync <psync_replid> <psync_offset>, 也就是 psync ? -1 或者 psync replid offset, 后者主节点收到后会直接判定为全量复制, 后者主节点会判断是否可进行部分复制

上面就是 slaveTryPartialResynchronization 写操作的逻辑

**主节点**收到从节点的发送过来的同步请求命令 psync, 执行的逻辑如下

```C
void syncCommand(client *c) {

    // 客户端不是从节点, 直接返回
    if (c->flags & CLIENT_SLAVE) 
        return;

    // 当前节点是另一个节点的从节点, 同时节点的状态不是 REPL_STATE_CONNECTED (已经连接状态), 直接返回
    if (server.masterhost && server.repl_state != REPL_STATE_CONNECTED) {
        addReplySds(c,sdsnew("-NOMASTERLINK Can't SYNC while not connected with my master\r\n"));
        return;
    }   

    // 判断 client c 的 bufpos != 0 || reply 有数据
    // 也就是判断当前节点有数据准备发送给从节点, 是的话, 直接返回
    if (clientHasPendingReplies(c)) {
        addReplyError(c,"SYNC and PSYNC are invalid with pending output");
        return;
    }

    // 打日志
    serverLog(LL_NOTICE,"Replica %s asks for synchronization",replicationGetSlaveName(c)); 

    // 执行的命令为 psync
    if (!strcasecmp(c->argv[0]->ptr,"psync")) {

        // psync repl_id repl_offset

        // masterTryPartialResynchronization 的逻辑大体如下
        // 1. psync 第 1 个参数 repl_id 和当前节点的 repl_id 不一致
        // 2. psync 第 1 个参数 repl_id 和当前节点的 repl_id 一致，但是和当前节点保存的 repl_id2 不一致或者请求的偏移量 repl_offset 大于 second_replid_offset (replid2 能接受的最大偏移量)
        // 3. 请求的偏移量 repl_offset 小于 repl_backlog_off (复制积压缓冲区最老的位置) , 说明 backlog 所备份的数据的已经太新了, 需要的已经不在复制积压缓冲区中
        // 4. 请求的偏移量 repl_offset 大于 repl_backlog_off + repl_backlog_histlen (backlog 的实际数据大小) , 表示当前 backlog 的数据不够全，则需要进行全量复制
        // 上面的 4 种情况不能进行部分复制, 直接返回失败
        // 其他情况尝试进行部分复制


        // 主节点尝试进行部分同步复制, 
        // 部分复制成功了 stat_sync_partial_ok 部分同步成功次数 + 1, 然后直接结束
        if (masterTryPartialResynchronization(c) == C_OK) {
            server.stat_sync_partial_ok++;
            return;
        }

        char *master_replid = c->argv[1]->ptr;
        
        // 从节点指定了 replid, 但是现在部分复制失败了
        if (master_replid[0] != '?') 
            // 部分同步复制失败次数 + 1
            server.stat_sync_partial_err++;
    } else {
        // 执行的命令不是 psync, 按照 sync 命令处理
        c->flags |= CLIENT_PRE_PSYNC;
    }

    // 全量复制

    // 全量复制次数 + 1
    server.stat_sync_full++;
    // 修改从节点的状态为等待 bgsave 的开始
    c->replstate = SLAVE_STATE_WAIT_BGSAVE_START;

    // 关闭了 TCP_NODELAY 功能
    if (server.repl_disable_tcp_nodelay)
        // 启用 nagle 算法
        anetDisableTcpNoDelay(NULL, c->fd);

    c->repldbfd = -1;
    // 客户端设置从节点标识
    c->flags |= CLIENT_SLAVE;
    // 把当前的客户端添加到从节点列表
    listAddNodeTail(server.slaves,c);       

    // 如果有需要, 创建复制积压缓冲区
    // 从节点只有 1 个, 复制积压缓冲区为空
    if (listLength(server.slaves) == 1 && server.repl_backlog == NULL) {

        // 生成 replid, 存放到 server.replid 中
        changeReplicationId();
        // 清除 replid2 和 second_replid_offset
        clearReplicationId2();
        // 创建复制积压缓冲区
        createReplicationBacklog();
    }

    if (server.rdb_child_pid != -1 && server.rdb_child_type == RDB_CHILD_TYPE_DISK) {
        // 正在执行 RDB, 同时类型是写入磁盘, 也就是普通的 RDB 

        client *slave;
        listNode *ln;
        listIter li;

        listRewind(server.slaves,&li);

        // 遍历所有的从节点, 找到第一个节点的状态为 SLAVE_STATE_WAIT_BGSAVE_END (等待 bgsave 的结束, 即等待 RDB 文件的创建结束)
        while((ln = listNext(&li))) {
            slave = ln->value;
            if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_END) 
                break;
        }

        // 有找到对应的节点, 当前客户端的节点的复制能力和找到的节点的复制能力一样
        if (ln && ((c->slave_capa & slave->slave_capa) == slave->slave_capa)) {
            
            // 把找到的节点的输出缓存复制到当前的客户端
            // 将 slave 的 buf 拷贝到 c 的 buf
            // 将 slave 的 reply_bytes 拷贝到 c 的 reply_bytes
            // 将 slave 的 bufpos 设置等于 c 的 bufpos
            copyClientOutputBuffer(c,slave);

            // 更新从节点客户端的偏移量, 状态和发送全量复制消息给从节点
            // 这个命令会发送 FULLRESYNC master_run_id offset\r\n 的响应给从节点, 可以看做是对 psync 命令的响应
            replicationSetupSlaveForFullResync(c,slave->psync_initial_offset);
            serverLog(LL_NOTICE,"Waiting for end of BGSAVE for SYNC");

        } else {
            // 找不到 或者找到的节点和当前的客户端的能力不一样, 只能等待下次的 bgsave 
            serverLog(LL_NOTICE,"Can't attach the replica to the current BGSAVE. Waiting for next BGSAVE for SYNC");
        }

    } else if (server.rdb_child_pid != -1 && server.rdb_child_type == RDB_CHILD_TYPE_SOCKET) {

        // 正在执行 RDB, 同时类型是写入 Socket, 也就是复制同步的 RDB
        // 提示等待下次同步
        serverLog(LL_NOTICE,"Current BGSAVE has socket target. Waiting for next BGSAVE for SYNC");
    } else { 

        // 没有在执行 RDB 
        // 复制类型为无盘同步 同时 当前的客户端执行 EOF 的同步方式, 也就是 RDB 文件流

        if (server.repl_diskless_sync && (c->slave_capa & SLAVE_CAPA_EOF)) {

            // 支持延迟无盘同步, 打印日志后结束
            // 后续在定时器执行的 replicationCron 函数时, 会创建出子进程进行同步
            // 延迟一段时间, 可以等待几个从节点, 后面同步处理

            if (server.repl_diskless_sync_delay)
                serverLog(LL_NOTICE,"Delay next BGSAVE for diskless SYNC");
        } else {

            // 没有子进程正在执行 BGSAVE, 且没有进行写 AOF 文件, 则开始为复制执行 BGSAVE, 并且是将 RDB 文件写到磁盘上
            if (server.aof_child_pid == -1) {
                // 内部和 RDB 的操作类型, 分为主子进程, 子进程进行 RDB 的生成, fork 出子进程后,
                // 主进程也会通过 replicationSetupSlaveForFullResync 函数进行 psync 的应答
                startBgsaveForReplication(c->slave_capa);
            } else {
                // 延迟执行 
                serverLog(LL_NOTICE, "No BGSAVE in progress, but an AOF rewrite is active. BGSAVE for replication delayed");
            }
        }
    }
    return;
}
```

上面就是主节点收到从节点的 **psync** 命令后的执行步骤, 很长, 整理如下
> 1. 如果可以部分复制, 进行部分复制同步
> 2. 无法部分部分复制, 进行全量复制同步
> 3. 全量复制同步时, 如果从节点支持无盘复制的话, 直接将数据写入到对应的 socket 中
> 4. 全量复制同步时, 如果从节点不支持无盘复制的话, 则和普通的 RDB 一样, 生成 RDB 文件, 然后发送过去
> 5. 整个全量复制的同步都是在子进程进行的, 主进程 fork 出子进程后, 会对 psync 命令进行响应

**从节点**收到主节点的响应后, 从节点同样是事件轮询触发 **syncWithMaster** 函数。

在从节点发送出 **psync** 命令后, 状态为 **REPL_STATE_RECEIVE_PSYNC**, 继续从这个状态走下去

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 状态不是 REPL_STATE_RECEIVE_PSYNC 直接失败
    if (server.repl_state != REPL_STATE_RECEIVE_PSYNC) {
        serverLog(LL_WARNING,"syncWithMaster(): state machine error, state should be RECEIVE_PSYNC but is %d", server.repl_state);
        goto error;
    }
    
        // 读取主节点的响应, 如果是全量复制, 响应为 PSYNC_FULLRESYNC
    psync_result = slaveTryPartialResynchronization(fd,1);

    // 等待重新执行
    if (psync_result == PSYNC_WAIT_REPLY) 
        return; 

    // 主节点正处于暂时性错误状态
    if (psync_result == PSYNC_TRY_LATER) 
        goto error;

    // psync 命令主节点直接完成, 从节点可以进行执行
    if (psync_result == PSYNC_CONTINUE) {
        serverLog(LL_NOTICE, "MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.");
        return;
    }

    // 释放所有的从节点连接
    disconnectSlaves();

    // 释放复制积压缓冲区
    freeReplicationBacklog();

    // 主节点无法识别 psync, 需要尝试执行 sync 
    if (psync_result == PSYNC_NOT_SUPPORTED) {

        // 不支持 psync, 则发送 sync
        serverLog(LL_NOTICE,"Retrying with SYNC...");

        if (syncWrite(fd,"SYNC\r\n",6,server.repl_syncio_timeout*1000) == -1) {
            serverLog(LL_WARNING,"I/O error writing to MASTER: %s", strerror(errno));
            goto error;
        }
    }
        
    char tmpfile[256], *err = NULL;
    int dfd = -1, maxtries = 5;

    // 尝试创建一个临时文件, 失败的话, 进行重试, 可以重试 5 次
    while(maxtries--) {
        // 创建一个临时文件
        snprintf(tmpfile,256, "temp-%d.%ld.rdb",(int)server.unixtime,(long int)getpid());
        dfd = open(tmpfile,O_CREAT|O_WRONLY|O_EXCL,0644);
        if (dfd != -1) break;
        sleep(1);
    }

    // 创建文件失败
    if (dfd == -1) {
        serverLog(LL_WARNING,"Opening the temp file needed for MASTER <-> REPLICA synchronization: %s",strerror(errno));
        goto error;
    }

    // 为创建的文件描述符 添加可读事件, 用来下载监听到的数据, 也就是父级发送过来的 RDB, 处理函数为 readSyncBulkPayload
    if (aeCreateFileEvent(server.el,fd, AE_READABLE,readSyncBulkPayload,NULL) == AE_ERR) {
        serverLog(LL_WARNING,
            "Can't create readable event for SYNC: %s (fd=%d)",
            strerror(errno),fd);
        goto error;
    }

    // 修改状态为 REPL_STATE_TRANSFER (正在接收 RDB 文件)
    server.repl_state = REPL_STATE_TRANSFER;

    // 更新复制相关变量的值
    server.repl_transfer_size = -1;
    server.repl_transfer_read = 0;
    server.repl_transfer_last_fsync_off = 0;
    server.repl_transfer_fd = dfd;
    server.repl_transfer_lastio = server.unixtime;
    server.repl_transfer_tmpfile = zstrdup(tmpfile);
    return;

}
```

上面的步骤基本就是主从节点为数据同步做的前期准备, 主从节点只是做好了发送同步数据的准备, 实际此时还是没有数据的复制的。  
前期的准备完成后, 主节点会尝试发送数据到从节点, 触发发送的链路如下:
> 1. 定时函数 serverCron, 判断出当前的 RDB 子进程不为空, 触发 backgroundSaveDoneHandler 函数
> 2. backgroundSaveDoneHandler 中根据当前的子进程 RDB 类型, Socket 走 backgroundSaveDoneHandlerSocket 函数, 无盘文件的走 backgroundSaveDoneHandlerDisk
> 3. 走 backgroundSaveDoneHandlerSocket 函数时, 更新各种复制相关的信息, 然后走到 updateSlavesWaitingBgsave 函数
> 4. 走 backgroundSaveDoneHandlerDisk 函数时, 先更新各种复制相关的信息, 然后走到 updateSlavesWaitingBgsave 函数
> 5. 走 updateSlavesWaitingBgsave 函数时, 遍历所有的从节点, 同时根据入参的 type 是磁盘同步还是 Socket 同步, 做处理
> 6. 如果是 Socket 同步, 更新各种信息, 更新从节点的状态为 SLAVE_STATE_ONLINE, 然后等待收到从节点的 ack 响应
> 7. 如果是磁盘同步, 更新各种信息, 打开临时文件, 注册一个可读的事件, 执行的函数 sendBulkToSlave, 从节点的状态切换为 SLAVE_STATE_SEND_BULK
> 8. 触发了 sendBulkToSlave 函数, 发送数据给从节点

**主节点**发送数据给从节点的逻辑

```C
void sendBulkToSlave(aeEventLoop *el, int fd, void *privdata, int mask) {
    client *slave = privdata;
    UNUSED(el);
    UNUSED(mask);
    char buf[PROTO_IOBUF_LEN];
    ssize_t nwritten, buflen;

    /* Before sending the RDB file, we send the preamble as configured by the
     * replication process. Currently the preamble is just the bulk count of
     * the file in the form "$<length>\r\n". */

    // 发送序言, 这里只是简单发送出了文件的大小, 格式 $<length>\r\n"
    if (slave->replpreamble) {

        nwritten = write(fd,slave->replpreamble,sdslen(slave->replpreamble));
        if (nwritten == -1) {
            serverLog(LL_VERBOSE,"Write error sending RDB preamble to replica: %s", strerror(errno));
            freeClient(slave);
            return;
        }

        // 更新已经写到网络的字节数
        server.stat_net_output_bytes += nwritten;
        // 保留未写的字节, 删除已写的字节
        sdsrange(slave->replpreamble,nwritten,-1);

        // 如果已经写完了, 则释放replpreamble
        if (sdslen(slave->replpreamble) == 0) {
            sdsfree(slave->replpreamble);
            slave->replpreamble = NULL;
        } else {
            return;
        }
    }

    // 将文件指针移动到刚才发送 replpreamble 的下一个字节, 准备写操作
    lseek(slave->repldbfd,slave->repldboff,SEEK_SET);

    // 将 repldbfd 读出 RDB 文件中的内容保存在 buf 中
    buflen = read(slave->repldbfd,buf,PROTO_IOBUF_LEN);
 
    if (buflen <= 0) {
        serverLog(LL_WARNING,"Read error sending DB to replica: %s", (buflen == 0) ? "premature EOF" : strerror(errno));
        freeClient(slave);
        return;
    }

    // 将保存 RDB 文件数据的 buf 写到从节点中
    if ((nwritten = write(fd,buf,buflen)) == -1) {
        if (errno != EAGAIN) {
            serverLog(LL_WARNING,"Write error sending DB to replica: %s", strerror(errno));
            freeClient(slave);
        }
        return;
    }
    // 更新从节点读取主服务器传来的 RDB 文件的字节数
    slave->repldboff += nwritten;
    // 更新服务器已经写到网络的字节数
    server.stat_net_output_bytes += nwritten;

    // 如果写入完成, 即从网络读到的大小等于文件大小
    if (slave->repldboff == slave->repldbsize) {
        close(slave->repldbfd);
        slave->repldbfd = -1;
        // 删除等待从节点的文件可读事件
        aeDeleteFileEvent(server.el,slave->fd,AE_WRITABLE);
        // 将从节点置于在线状态
        putSlaveOnline(slave);
    }
}
```

## 3.9 发送输出缓冲区数据

在上面的发送 psync 命令中, 主节点在 **sendBulkToSlave** 函数中, 发送完成 RDB 文件后, 会调用 putSlaveOnline 函数, 函数内部会
> 1. 将对应的从节点的客户端状态设置为 SLAVE_STATE_ONLINE (在线状态)
> 2. 创建一个可写事件, 执行的函数为 sendReplyToClient

创建完可写事件, 会立即触发一次, 然后在事件轮询中, 会接收到对应的通知, 执行 sendReplyToClient 函数

```C
void sendReplyToClient(aeEventLoop *el, int fd, void *privdata, int mask) {
    UNUSED(el);
    UNUSED(mask);
    writeToClient(fd,privdata,1);
}

// 将输出缓冲区的数据发送给对应的 client
int writeToClient(int fd, client *c, int handler_installed) {

    ssize_t nwritten = 0, totwritten = 0;
    size_t objlen;
    clientReplyBlock *o;

    // 当前客户端有数据要发送, 判断入参的 client 的 bufpos 大于 0 或者 reply 中有数据
    while(clientHasPendingReplies(c)) {

        // bufpos 大于 0
        // 也就是固定缓存区有数据未写出
        if (c->bufpos > 0) {
            
            // write(fd, buf, count) 把 buf 指向的内存地址中的数据写入 count 个字节到 fd, 返回值我写入的字节数
            // buf 可以看做缓存区的起始位置 bufpos 缓存区的结束位置, setlen 已经发送的字节数
            nwritten = write(fd,c->buf+c->sentlen,c->bufpos-c->sentlen);

            // 写入失败
            if (nwritten <= 0) 
                break;
            // 更新已经发送的字节数    
            c->sentlen += nwritten;
            // 更新当前已经发送的字节数
            totwritten += nwritten;

            // 当前已经发送的字节数等于结束位置, 也就是没有缓存区数据, 进行重置
            if ((int)c->sentlen == c->bufpos) {
                c->bufpos = 0;
                c->sentlen = 0;
            }

        } else {

            // 获取回复链表的头节点
            o = listNodeValue(listFirst(c->reply));

            // 回复对象的长度
            objlen = o->used;

            // 为 0, 表示没有数据, 跳过
            if (objlen == 0) {
                c->reply_bytes -= o->size;
                listDelNode(c->reply,listFirst(c->reply));
                continue;
            }

            // 发送数据到客户端
            nwritten = write(fd, o->buf + c->sentlen, objlen - c->sentlen);

            if (nwritten <= 0) 
                break;
            // 更新客户端里面已经发送数据 + nwritten    
            c->sentlen += nwritten;
            
            totwritten += nwritten;

            // 发送出去的数据和当前对象的数据一样, 当前对象发送完成
            if (c->sentlen == objlen) {
                // 需要应答的数据 - 当前对象的数据大小
                c->reply_bytes -= o->size;
                // 删除头对象
                listDelNode(c->reply,listFirst(c->reply));
                // 已经发送的数据变为 0 
                c->sentlen = 0;
                
                // 恢复链表为空了, 需要回复的数据长度需要等于 0  
                if (listLength(c->reply) == 0)
                    serverAssert(c->reply_bytes == 0);
            }
        }

        // NET_MAX_WRITES_PER_EVENT = 1024*64
        // 如果这次写的总量大于 NET_MAX_WRITES_PER_EVENT 的限制, 则会中断本次的写操作, 将处理时间让给其他的 client, 以免一个非常大的回复独占服务器, 剩余的数据下次继续在写
        // 但是, 如果当服务器的内存数已经超过 maxmemory, 即使超过最大写 NET_MAX_WRITES_PER_EVENT 的限制, 也会继续执行写入操作, 是为了尽快写入给客户端
        if (totwritten > NET_MAX_WRITES_PER_EVENT && (server.maxmemory == 0 ||zmalloc_used_memory() < server.maxmemory) && !(c->flags & CLIENT_SLAVE)) 
            break;
    }    


    // 更新写到网络的字节数
    server.stat_net_output_bytes += totwritten;

    //  处理写入失败
    if (nwritten == -1) {
        if (errno == EAGAIN) {
            nwritten = 0;
        } else {
            serverLog(LL_VERBOSE, "Error writing to client: %s", strerror(errno));
            freeClient(c);
            return C_ERR;
        }
    }

    // 发送给客户端的数据大于 0 了, 也就是有数据发送成功
    if (totwritten > 0) {

        // 客户端不算主节点, 更新最近和服务器交互的时间
        if (!(c->flags & CLIENT_MASTER)) 
            c->lastinteraction = server.unixtime;
    }

    //  当前客户端没有数据要发送
    if (!clientHasPendingReplies(c)) {
        // 已经发送的数据长度为 0 
        c->sentlen = 0;
        // 需要删除写事件, 进行删除
        if (handler_installed) 
            aeDeleteFileEvent(server.el,c->fd,AE_WRITABLE);

        // 客户端的标识为 在响应后需要关闭的话, 释放客户端
        if (c->flags & CLIENT_CLOSE_AFTER_REPLY) {
            freeClient(c);
            return C_ERR;
        }
    }

    return C_OK;
}
```

## 3.10 命令传播

经过 psync 命令后, 也就是第一次全量同步后, 主从节点之间数据都同步了, 但是后续如果主节点继续数据的变更, 又会不一致。为了数据的一致, 主节点应该有种方式将自身的变更同步到  
从节点, 这个实现的步骤就是命令传播。

在执行 Redis 命令的函数 call 中, 里面会根据执行的命令和客户端的类型等元素, 判断是否需要执行 propagate 函数, propagate 函数就是命令传播的方法。  propagate 函数很简单
根据入参的标识判断是否需要进行 AOF 相关需要的命令的传播, 然后根据标识判断是否需要复制相关需要的传播, 如果判断复制相关的传播, 会执行 replicationFeedSlaves 方法

replicationFeedSlaves 方法的执行逻辑如下:

```C
void replicationFeedSlaves(list *slaves, int dictid, robj **argv, int argc) {
    listNode *ln;
    listIter li;
    int j, len;
    char llstr[LONG_STR_SIZE];

    // 有配置主节点, 直接返回
    if (server.masterhost != NULL) 
        return;

    // 没有复制积压缓冲区 backlog 且没有从节点服务器, 直接返回
    if (server.repl_backlog == NULL && listLength(slaves) == 0) 
        return;    

    serverAssert(!(listLength(slaves) != 0 && server.repl_backlog == NULL));

    if (server.slaveseldb != dictid) {    

        // 如果当前从节点使用的数据库不是目标的数据库, 则要生成一个 select 命令
        robj *selectcmd;

        // // 0 <= id < 10, 可以使用共享的 select 命令对象
        if (dictid >= 0 && dictid < PROTO_SHARED_SELECT_CMDS) {
            selectcmd = shared.select[dictid];
        } else {
            // 按照协议格式构建 select 命令对象
            int dictid_len;
            dictid_len = ll2string(llstr,sizeof(llstr),dictid);
            selectcmd = createObject(OBJ_STRING, sdscatprintf(sdsempty(), "*2\r\n$6\r\nSELECT\r\n$%d\r\n%s\r\n", dictid_len, llstr));
        }

        // 把当前的 select 命令写入到复制积压缓冲区
        if (server.repl_backlog) 
            feedReplicationBacklogWithObject(selectcmd);

        listRewind(slaves,&li);

        // 遍历所有的从服务器节点
        while((ln = listNext(&li))) {
            client *slave = ln->value;
            // 从节点服务器状态为等待 bgsave 的开始, 因此跳过回复, 遍历下一个节点
            if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START) 
                continue;
            // 添加 select 命令到当前从节点的回复中    
            addReply(slave,selectcmd);
        }

        // 释放临时对象
        if (dictid < 0 || dictid >= PROTO_SHARED_SELECT_CMDS)
            decrRefCount(selectcmd);
    }

    // 设置当前从节点使用的数据库 ID
    server.slaveseldb = dictid;

    // 将命令写到 backlog 中
    if (server.repl_backlog) {

        char aux[LONG_STR_SIZE+3];

        // 拼写命令 *<argc>\r\n
        aux[0] = '*';
        len = ll2string(aux+1,sizeof(aux)-1,argc);
        aux[len+1] = '\r';
        aux[len+2] = '\n';
        // 写入复制积压缓冲区
        feedReplicationBacklog(aux,len+3);

        // 遍历所有的参数个数
        for (j = 0; j < argc; j++) {

            // 参数对象的长度
            long objlen = stringObjectLen(argv[j]);
            
            // 构建成协议标准的字符串, 并添加到 backlog 中
            // $<len>\r\n<argv>\r\n 
            aux[0] = '$';
            len = ll2string(aux+1,sizeof(aux)-1,objlen);
            aux[len+1] = '\r';
            aux[len+2] = '\n';

            // 添加 $<len>\r\n
            feedReplicationBacklog(aux,len+3);
            // 添加参数对象 <argv>
            feedReplicationBacklogWithObject(argv[j]);
            // 添加\r\n
            feedReplicationBacklog(aux+len+1,2);
        }
    }

    listRewind(slaves,&li);
    while((ln = listNext(&li))) {

        client *slave = ln->value;

        // 从节点的状态为等待 bgsave 开始跳过
        if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START) 
            continue;

        // 将命令写给正在等待初次 SYNC 的从节点 (所以这些命令在输出缓冲区中排队, 直到初始 SYNC 完成), 或已经与主节点同步

        // 添加回复的长度
        addReplyMultiBulkLen(slave,argc);

        // 将所有的参数列表添加到从节点的输出缓冲区
        for (j = 0; j < argc; j++)
            addReplyBulk(slave,argv[j]);
    }
}
```

和 AOF 持久化一样, 再给从节点 client 写命令时, 会将 SELECT 命令强制写入, 以保证命令正确读到数据库中。
同时不仅写入了从节点 client 的输出缓冲区, 而且还会将命令记录到主节点服务器的复制积压缓冲区 server.repl_backlog 中, 这是为了网络闪断后进行部分重同步。


## 3.11 部分复制实现

上面是全量同步的过程, 如果在传输 RDB 文件的过程中, 网络发生故障, 主节点和从节点的连接中断, Redis 会咋么做呢？ 
Redis 2.8 版本之前会在进行一次连接然后进行全量复制, 但是这样效率非常地下, 之后的版本都提供了部分重同步的实现。

部分重同步在复制的过程中, 相当于步骤 3.8 的发送 PSYNC 命令的部分, 其他所有的部分都要进行, 他只是主节点回复从节点的命令不同, 
回复 +CONTINUE 则执行部分重同步, 回复 +FULLRESYNC 则执行全量同步。


### 3.11.1 心跳机制

在主从节点建立连接后, 他们之间都维护者长连接并彼此发送心跳命令。主从节点彼此都有心跳机制, 各自模拟成对方的客户端进行通信。

主节点默认每隔10秒发送PING命令, 判断从节点的连接状态, 可以通过 **repl-ping-salve-period** 进行时间的配置, 默认为 10 秒

在定时函数 serverCron 中

```C
// 如果当前节点是某以节点的主节点, 那么发送 PING 给从节点
if ((replication_cron_loops % server.repl_ping_slave_period) == 0) {
    // 创建 PING 命令对象
    ping_argv[0] = createStringObject("PING",4);
    // 将 PING 发送给从服务器
    replicationFeedSlaves(server.slaves, server.slaveseldb, ping_argv, 1);
    // 对象的引用次数 - 1
    decrRefCount(ping_argv[0]);
}
```

从节点在主线程中每隔 1 秒发送 **REPLCONF ACK <offset>** 命令, 给主节点报告自己当前复制偏移量

```C
// 定期发送 ack 给主节点, 旧版本的Redis除外
if (server.masterhost && server.master && !(server.master->flags & CLIENT_PRE_PSYNC))
    // 发送一个 REPLCONF ACK 命令给主节点去报告关于当前处理的 offset。
    replicationSendAck();

```

**主节点** 收到后, 同样是在 replconfCommand 中处理

```C
void replconfCommand(client *c) {

    int j;

    if ((c->argc % 2) == 0) {
        addReply(c,shared.syntaxerr);
        return;
    }

    for (j = 1; j < c->argc; j+=2) {
        if (!strcasecmp(c->argv[j]->ptr,"listening-port")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ip-address")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"capa")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ack")) {
            
            // 
            long long offset;

            // 不是从节点不做处理
            if (!(c->flags & CLIENT_SLAVE)) 
                return;

            // 获取第二个参数 repl_offset
            if ((getLongLongFromObject(c->argv[j+1], &offset) != C_OK))
                return;

            // 更新客户端对应的偏移量
            if (offset > c->repl_ack_off)
                c->repl_ack_off = offset;    
            
            // 更新收到 ack 的时间为当前时间
            c->repl_ack_time = server.unixtime;

            // 客户端设置了收到 ack 时需要变更为在线状态 同时当前客户端的状态为 SLAVE_STATE_ONLINE
            if (c->repl_put_online_on_ack && c->replstate == SLAVE_STATE_ONLINE)
                putSlaveOnline(c);

            // 结束, 这个命令不需要响应任何信息
            return;

        } else if (!strcasecmp(c->argv[j]->ptr,"getack")) {
            // 省略
        }  else {
            addReplyErrorFormat(c,"Unrecognized REPLCONF option: %s", (char*)c->argv[j]->ptr);
            return;
        }
    }
    addReply(c,shared.ok);
}
```

**从节点**还会在周期性函数 replicationCron() 中, 每次都要检查和主节点处于连接状态的从节点和主节点的交互时间是否超时, 
如果超时则会调用 cancelReplicationHandshake() 函数, 取消和主节点的连接。 等到下一个周期在和主节点重新建立连接, 进行复制。

### 3.11.2 复制积压缓冲区 (backlog)

复制积压缓冲区是一个大小为 1M 的循环队列。主节点在命令传播时, 不仅会将命令发送给所有的从节点, 还会将命令写入复制积压缓冲区中 (具体请看步骤 3.10) 。  
复制积压缓冲区最多可以备份 1M 大小的数据, 如果主从节点断线时间过长, 复制积压缓冲区的数据会被新数据覆盖, 那么当从主从中断连接起, 主节点接收到的数据超过 1M 
大小, 那么从节点就无法进行部分重同步, 只能进行全量复制。

在上面步骤中 3.8 中，可以知道, 从节点会将上次的主节点缓存起来 (如果有的话), 然后在和主节点建立联系后, 发送 psync 命令时, 如果有缓存的主节点, 发送出去的内容为
**psync repl_id repl_offset**, 没有则发送为 **psync ? -1**, 主节点接收到第一种情况时, 就会进行部分复制的尝试。  
如果可以进行部分重同步, 主节点则会发送 "+CONTINUE\r\n" 作为从节点发送 PSYNC 回复。


## 3.12 参考
[Redis 复制(replicate)实现](https://blog.csdn.net/men_wen/article/details/72628439)