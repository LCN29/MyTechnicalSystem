
```C

// 没有开启主从复制
#define REPL_STATE_NONE 0
// 开启了主从复制, 但是还没连接上主节点
#define REPL_STATE_CONNECT 1
// 正在连接主节点
#define REPL_STATE_CONNECTING 2

/* --- 握手阶段的状态开始, 整个握手过程必须按照下面的顺序进行 --- */

// 等待 ping 应答 (正常情况, 主节点会回复一个 pong)
#define REPL_STATE_RECEIVE_PONG 3
// 发送认证密码给主节点
#define REPL_STATE_SEND_AUTH 4
// 等待认证结果应答
#define REPL_STATE_RECEIVE_AUTH 5
// 发送主从复制配置的监听的端口
#define REPL_STATE_SEND_PORT 6 
// 等待主从复制配置响应
#define REPL_STATE_RECEIVE_PORT 7
// 发送主从复制配置的监听的 IP 地址
#define REPL_STATE_SEND_IP 8 
// 等待主从复制配置响应
#define REPL_STATE_RECEIVE_IP 9
// 发送主从复制配置的 capa
#define REPL_STATE_SEND_CAPA 10 
// 等待主从复制配置响应
#define REPL_STATE_RECEIVE_CAPA 11
// 向主节点发送 psync 命令, 请求同步复制
#define REPL_STATE_SEND_PSYNC 12 
// 等待 psync 应答
#define REPL_STATE_RECEIVE_PSYNC 13 /* Wait for PSYNC reply */

/* --- 握手阶段的状态结束 --- */

// 正在接收从主节点发送过来的 RDB 文件
#define REPL_STATE_TRANSFER 14 
// 已经连接状态
#define REPL_STATE_CONNECTED 15 

```

主节点保存从节点的转态

```C

// 等待 bgsave 的开始, 需要发送一个 RDB 文件给从节点
#define SLAVE_STATE_WAIT_BGSAVE_START 6

// 等待 bgsave 的结束, 等待 RDB 文件的创建结束
#define SLAVE_STATE_WAIT_BGSAVE_END 7

// 发送一个 RDB 文件到从节点
#define SLAVE_STATE_SEND_BULK 8

// 从节点在线
#define SLAVE_STATE_ONLINE 9
```


```C
struct redisServer {

    /** 网络相关的 */
    
    // 从节点列表
    list *slaves,

    /** 主从配置 主配置 */

    // 当前节点的复制 Id, CONFIG_RUN_ID_SIZE = 40
    // 也就是 run id, 每个主节点都会有一个, 如果是从节点, 这里保存的是主节点的 run id
    char replid[CONFIG_RUN_ID_SIZE+1]; 
    
    // 从主节点复制下来的复制 Id
    // 当前从节点脱离主节点, 自己成为主节点, 会将上次的父节点的 run id 保存到这里
    char replid2[CONFIG_RUN_ID_SIZE+1]; 

    // 当前节点的主从复制偏移量
    long long master_repl_offset;

    // 从 replid2 中接收的偏移量 
    long long second_replid_offset;

    // 上次复制输出选择的数据库
    int slaveseldb;            

    // 主节点多长时间就 ping 一次从节点, 单位秒
    int repl_ping_slave_period; 

    // 部分复制的缓冲区 backlog
    char *repl_backlog;
    // 复制缓冲区 backlog 的大小
    long long repl_backlog_size; 
    // 复制缓冲区 backlog 的实际数据大小
    long long repl_backlog_histlen;
    // 复制缓冲区 backlog 当前的偏移量, 下一个字节就是下次写入数据的地方
    long long repl_backlog_idx;
    // 记录复制的偏移量, 复制缓冲区 backlog 的第一个字节的逻辑位置是下次复制的开始位置
    long long repl_backlog_off;
    // 没有从节点, 复制缓冲区 backlog 被释放的时间
    time_t repl_backlog_time_limit; 

    // 没有从节点的时间, 
    time_t repl_no_slaves_since;  

    // 执行写操作的最少的从节点个数
    int repl_min_slaves_to_write;
    
    // 最小数量的从节点的最大延迟值
    int repl_min_slaves_max_lag; 

    // 延迟小于 repl_min_slaves_max_lag 的从节点的个数
    int repl_good_slaves_count;

    // 无盘同步，直接将 RDB 文件发送给从节点
    int repl_diskless_sync;

    // 延迟开始无盘同步的时间
    int repl_diskless_sync_delay; 



    /** ---- 主从配置 从配置 ---- */

    // 主节点认证密码 
    char *masterauth;
    
    // 主节点的 Ip 或者 host 名
    char *masterhost;          
    
    // 主节点的端口
    int masterport;
    
    // 主从复制超时时间, 单位秒
    int repl_timeout; 

    // 主节点的客户端
    client *master;

    // 主节点缓存用于重新 psync
    client *cached_master;

    // 节点的复制状态
    int repl_state; 

    // 从节点从主节点同步的 socket
    int repl_transfer_s;    

    // 从节点从主节点同步的临时文件描述符
    int repl_transfer_fd;


    // 主节点同步的 run id 
    char master_replid[CONFIG_RUN_ID_SIZE+1];

    // 主节点同步的偏移量
    long long master_initial_offset;

    // 
    time_t repl_transfer_lastio;

}
```


```C
void replicationUnsetMaster(void) {

    // 没有设置主节点的, 直接返回
    if (server.masterhost == NULL) 
        return;

    // 释放主节点的 IP 或者 host 名配置
    sdsfree(server.masterhost);
    server.masterhost = NULL;

    // 把当前的 replid 复制到 replid2
    // 重新生成一个新的 40 位随机 id 并赋值到 replid
    shiftReplicationId();

    // 释放代表主节点的 client 对象
    if (server.master) 
        freeClient(server.master);

    // 将 server.cached_server 设置为空, 同时设置释放对应的内存
    replicationDiscardCachedMaster();

    // 取消主从复制的握手
    cancelReplicationHandshake();

    // 关闭所有的从节点客户端
    disconnectSlaves();

    // 当前节点的主从复制状态设置为 none
    server.repl_state = REPL_STATE_NONE;

    // 从节点选中的数据库为 - 1
    server.slaveseldb = -1;
    server.repl_no_slaves_since = server.unixtime;
}
```



```C
// 将 replid 复制到 replid2
// 同时重新生成一个新的 run id 赋值到 replid
void shiftReplicationId(void) {

    // 将 replid 复制到 replid2
    memcpy(server.replid2,server.replid,sizeof(server.replid));

    // second_replid_offset = master_repl_offset + 1;
    server.second_replid_offset = server.master_repl_offset+1;
    
    // 重新生成一个 runId, 并赋值到 server.replid
    changeReplicationId();
    serverLog(LL_WARNING,"Setting secondary replication ID to %s, valid up to offset: %lld. New replication ID is %s", server.replid2, server.second_replid_offset, server.replid);
}

// 重新生成一个 run id, 并赋值到 replid
void changeReplicationId(void) {

    // CONFIG_RUN_ID_SIZE = 40
    // 重新随机生成一个 40 位的 id 并赋值到 replid
    getRandomHexChars(server.replid,CONFIG_RUN_ID_SIZE);
    server.replid[CONFIG_RUN_ID_SIZE] = '\0';
}

// 置空 server.cached_master
void replicationDiscardCachedMaster(void) {

    if (server.cached_master == NULL) 
        return;

    serverLog(LL_NOTICE,"Discarding previously cached master state.");
    // 先设置为 cashed_master 的 flags = flag & (~CLIENT_MASTER), CLIENT_MASTER = 1<<1
    server.cached_master->flags &= ~CLIENT_MASTER;
    // 然后再释放 cached_master
    freeClient(server.cached_master);
    server.cached_master = NULL;
}

// 取消主从复制的握手
int cancelReplicationHandshake(void) {

    // 当前的状态为 正在接收从主节点发送过来的 RDB 文件
    if (server.repl_state == REPL_STATE_TRANSFER) {
        
        // 停止同步传输
        // 关闭从节点同步主节点的 socket 和临时文件描述符 repl_transfer_tmpfile
        replicationAbortSyncTransfer();
        // 更新状态为 未连接上主节点
        server.repl_state = REPL_STATE_CONNECT;

        // 当前的状态为正在建立连接或者处于握手阶段
    } else if (server.repl_state == REPL_STATE_CONNECTING || slaveIsInHandshakeState()) {

        // 关闭 从节点同步主节点信息的 socket : repl_transfer_s
        undoConnectWithMaster();
        server.repl_state = REPL_STATE_CONNECT;
    } else {
        // 其他状态返回 0
        return 0;
    }
    // 正在接收从主节点发送过来的 RDB 文件 或者 正在建立连接或者处于握手阶段, 处理完成后, 返回 1
    return 1;
}

int slaveIsInHandshakeState(void) {

    // 3 <= server.repl_state <= 13 
    return server.repl_state >= REPL_STATE_RECEIVE_PONG && server.repl_state <= REPL_STATE_RECEIVE_PSYNC;
}

void replicationAbortSyncTransfer(void) {
    serverAssert(server.repl_state == REPL_STATE_TRANSFER);

    // 关闭 从节点同步主节点信息的 socket : repl_transfer_s
    undoConnectWithMaster();
    close(server.repl_transfer_fd);
    unlink(server.repl_transfer_tmpfile);
    zfree(server.repl_transfer_tmpfile);
}

void undoConnectWithMaster(void) {

    int fd = server.repl_transfer_s;
    // 删除 repl_transfer_s 这个文件描述符的事件
    aeDeleteFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE);
    // 关闭这个文件描述符
    close(fd);
    // 重置为 - 1 
    server.repl_transfer_s = -1;
}

// 关闭所有的从节点客户端
void disconnectSlaves(void) {

    while (listLength(server.slaves)) {
        listNode *ln = listFirst(server.slaves);
        freeClient((client*)ln->value);
    }
}
```

```C

void replicationSetMaster(char *ip, int port) {

    int was_master = server.masterhost == NULL;

    // 将入参的 IP 和 端口赋值给对应的字段
    sdsfree(server.masterhost);
    server.masterhost = sdsnew(ip);
    server.masterport = port;

    if (server.master) {
        freeClient(server.master);
    }

    // 解除所有阻塞状态的客户端
    disconnectAllBlockedClients(); 

    // 释放所有的从节点信息 
    disconnectSlaves();

    //  取消主从复制的握手
    cancelReplicationHandshake();

    // 上次有主节点了
    // 假设 A 原本是 B 的从节点, 现在通过 salveof 将 A 变成 C 的从节点
    if (was_master) {

        // 将 server.cached_server 设置为空, 同时设置释放对应的内存
        replicationDiscardCachedMaster();
        // 根据当前参数合成出一个 client, 同时将其放到 cached_master
        replicationCacheMasterUsingMyself();
    }
    // 设置当前的主从复制状态为 REPL_STATE_CONNECT
    server.repl_state = REPL_STATE_CONNECT;
}

```

```C
void disconnectAllBlockedClients(void) {

    listNode *ln;
    listIter li;

    // 将 server.clients 的信息转移到 li 上
    listRewind(server.clients,&li);

    // 遍历 li
    while((ln = listNext(&li))) {

        client *c = listNodeValue(ln);

        if (c->flags & CLIENT_BLOCKED) {
            
            addReplySds(c,sdsnew("-UNBLOCKED force unblock from blocking operation, instance state changed (master -> replica?)\r\n"));

            // 解除所有阻塞状态的客户端
            // unblockClient 会先将入参的 client 先处理后，放到 server.unblocked_clients 这个列表中, 在下次事件循环中进行处理里面的客户端请求处理
            unblockClient(c);
            c->flags |= CLIENT_CLOSE_AFTER_REPLY;
        }
    }
}

// 根据当前参数合成出一个 client, 同时将其放到 cached_master
void replicationCacheMasterUsingMyself(void) {

    // 设置当前的 master_initial_offset 等于 master_repl_offset
    server.master_initial_offset = server.master_repl_offset;

    // 创建一个新的主节点客户端
    replicationCreateMasterClient(-1,-1);

    // 将 server.replid 拷贝到创建处理的主节点的 replid
    memcpy(server.master->replid, server.replid, sizeof(server.replid));

    // 将新创建处理的节点保存为缓存节点, 重新将 master 设置为空
    unlinkClient(server.master);
    server.cached_master = server.master;
    server.master = NULL;
    serverLog(LL_NOTICE,"Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.");
}

void replicationCreateMasterClient(int fd, int dbid) {
    
    // 新的主节点
    server.master = createClient(fd);
    server.master->flags |= CLIENT_MASTER;
    server.master->authenticated = 1;
    server.master->reploff = server.master_initial_offset;
    server.master->read_reploff = server.master->reploff;
    memcpy(server.master->replid, server.master_replid, sizeof(server.master_replid));

    /* If master offset is set to -1, this master is old and is not
     * PSYNC capable, so we flag it accordingly. */

    // 如果一个主节点的 偏移量为 -1, 那么这个主节点是旧的, 并且是无法进行 psync 的, 所以将其设置为 CLIENT_PRE_PSYNC
    if (server.master->reploff == -1)
        server.master->flags |= CLIENT_PRE_PSYNC;

    if (dbid != -1) 
        selectDb(server.master,dbid);
}

```