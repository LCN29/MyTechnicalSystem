

# 3 主从复制 - replication 源码分析

## 3.1 主从节点运行数据保存位置

```C
struct redisServer {

    /******** 网络相关的 ******/
    
    // 从节点列表
    list *slaves;


    /******** 主从配置 主节点配置 ******/

    // CONFIG_RUN_ID_SIZE = 40
    // 当前节点的复制 Id, 也就是 run id, 每个主节点都会有一个, 如果是从节点, 这里保存的是主节点的 run id
    char replid[CONFIG_RUN_ID_SIZE + 1]; 


    // 从主节点复制下来的复制 Id,  当前从节点脱离主节点, 自己成为主节点, 会将上次的父节点的 run id 保存到这里
    char replid2[CONFIG_RUN_ID_SIZE+1];

    // 当前节点的主从复制偏移量
    long long master_repl_offset;

    // 从 replid2 中接收的偏移量 
    long long second_replid_offset;

    // 上次复制输出选择的数据库
    int slaveseldb;            

    // 主节点多长时间就 ping 一次从节点, 单位秒
    int repl_ping_slave_period; 

    // 复制积压缓冲区 backlog
    char *repl_backlog;

    // 复制积压缓冲区 backlog 的大小
    long long repl_backlog_size; 

    // 复制积压缓冲区backlog 的实际数据大小
    long long repl_backlog_histlen;

    // 复制积压缓冲区 backlog 当前的偏移量, 下一个字节就是下次写入数据的地方
    long long repl_backlog_idx;

    // 记录复制的偏移量, 复制积压缓冲区 backlog 的第一个字节的逻辑位置是下次复制的开始位置
    long long repl_backlog_off;

    // 没有从节点, 复制积压缓冲区 backlog 被释放的时间
    time_t repl_backlog_time_limit; 

    // 没有从节点的时间, 
    time_t repl_no_slaves_since;  

    // 执行写操作的最少的从节点个数
    int repl_min_slaves_to_write;
    
    // 最小数量的从节点的最大延迟值
    int repl_min_slaves_max_lag; 

    // 延迟小于 repl_min_slaves_max_lag 的从节点的个数
    int repl_good_slaves_count;

    // 无盘同步，直接将 RDB 文件发送给从节点
    int repl_diskless_sync;

    // 延迟开始无盘同步的时间
    int repl_diskless_sync_delay; 


    /******** 主从配置 从节点配置 ******/

    // 主节点认证密码 
    char *masterauth;
    
    // 主节点的 IP 或者 host 名
    char *masterhost;          
    
    // 主节点的端口
    int masterport;
    
    // 主从复制超时时间, 单位秒
    int repl_timeout; 

    // 主节点的客户端
    client *master;

    // 主节点缓存用于重新 psync
    client *cached_master;

    // 节点的复制状态
    int repl_state; 

    // 从节点从主节点同步的 socket
    int repl_transfer_s;    

    // 从节点同主节点同步的临时文件描述符
    int repl_transfer_fd;

    // 主节点同步的 run id 
    char master_replid[CONFIG_RUN_ID_SIZE+1];

    // 主节点同步的偏移量
    long long master_initial_offset;

    // 从节点和主节点最近一次进行数据同步的时间
    time_t repl_transfer_lastio;

}
```

## 3.2 主从复制中涉及相关的常量配置

### 3.2.1 主从复制中节点的状态常量

```C

/***************** 主从配置中节点的状态 **********************/

// 没有开启主从复制
#define REPL_STATE_NONE 0

// 开启了主从复制, 但是还没连接上主节点
#define REPL_STATE_CONNECT 1

// 正在连接主节点
#define REPL_STATE_CONNECTING 2

/* --- 握手阶段的状态开始, 整个握手过程必须按照下面的顺序进行 --- */

// 发送 ping, 等待 pong  应答 (正常情况, 主节点会回复一个 pong)
#define REPL_STATE_RECEIVE_PONG 3

// 准备发送认证密码给主节点
#define REPL_STATE_SEND_AUTH 4

// 等待主节点响应认证结果应答
#define REPL_STATE_RECEIVE_AUTH 5

// 准备发送从节点的监听的端口
#define REPL_STATE_SEND_PORT 6 

// 等待主节点响应收到从节点端口
#define REPL_STATE_RECEIVE_PORT 7

// 发送主从复制配置的监听的 IP 地址
#define REPL_STATE_SEND_IP 8 

// 等待主节点响应收到从节点的 IP 地址
#define REPL_STATE_RECEIVE_IP 9

// 准备发送从节点支持的同步能力 
#define REPL_STATE_SEND_CAPA 10 

// 等待主节点响应收到支持的同步能力的应答
#define REPL_STATE_RECEIVE_CAPA 11

// 向主节点发送 psync 命令, 请求同步复制
#define REPL_STATE_SEND_PSYNC 12 

// 等待 psync 应答
#define REPL_STATE_RECEIVE_PSYNC 13

/* --- 握手阶段的状态结束 --- */

// 正在接收从主节点发送过来的 RDB 文件
#define REPL_STATE_TRANSFER 14 

// 已经连接状态
#define REPL_STATE_CONNECTED 15 

```

### 3.2.2 主节点保存从节点的状态常量

```C

// 等待 bgsave 的开始, 需要发送一个 RDB 文件给从节点
#define SLAVE_STATE_WAIT_BGSAVE_START 6

// 等待 bgsave 的结束, 等待 RDB 文件的创建结束
#define SLAVE_STATE_WAIT_BGSAVE_END 7

// 发送一个 RDB 文件到从节点
#define SLAVE_STATE_SEND_BULK 8

// 从节点在线
#define SLAVE_STATE_ONLINE 9
```


## 3.3 源码

### 3.3.1 replicationUnsetMaster - 执行 salveof no one, 断开主从关系的逻辑

```C
void replicationUnsetMaster(void) {

    // 没有设置主节点的, 直接返回
    if (server.masterhost == NULL) 
        return;

    // 释放主节点的 host 配置
    sdsfree(server.masterhost);
    server.masterhost = NULL;

    // 把当前的 replid 复制到 replid2
    // 重新生成一个新的 40 位随机 id 并赋值到 replid
    shiftReplicationId();

    // 释放代表主节点的 client 对象
    if (server.master) 
        freeClient(server.master);

    // 将 server.cached_server 设置为空, 同时设置释放对应的内存
    replicationDiscardCachedMaster();

    // 取消主从复制的握手
    cancelReplicationHandshake();

    // 关闭所有的从节点客户端
    disconnectSlaves();

    // 当前节点的主从复制状态设置为 none
    server.repl_state = REPL_STATE_NONE;

    // 从节点选中的数据库为 - 1
    server.slaveseldb = -1;
    server.repl_no_slaves_since = server.unixtime;
}
```

#### 3.3.1.1 shiftReplicationId

```C
// 将 replid 复制到 replid2
// 同时重新生成一个新的 run id 赋值到 replid
void shiftReplicationId(void) {

    // 将 replid 复制到 replid2
    memcpy(server.replid2,server.replid,sizeof(server.replid));

    // second_replid_offset = master_repl_offset + 1;
    server.second_replid_offset = server.master_repl_offset+1;
    
    // 重新生成一个 runId, 并赋值到 server.replid
    changeReplicationId();
    serverLog(LL_WARNING,"Setting secondary replication ID to %s, valid up to offset: %lld. New replication ID is %s", server.replid2, server.second_replid_offset, server.replid);
}

// 重新生成一个 run id, 并赋值到 replid
void changeReplicationId(void) {

    // CONFIG_RUN_ID_SIZE = 40
    // 重新随机生成一个 40 位的 id 并赋值到 replid
    getRandomHexChars(server.replid, CONFIG_RUN_ID_SIZE);
    server.replid[CONFIG_RUN_ID_SIZE] = '\0';
}
```

#### 3.3.1.2 replicationDiscardCachedMaster

```C

// 置空 server.cached_master
void replicationDiscardCachedMaster(void) {

    if (server.cached_master == NULL) 
        return;

    serverLog(LL_NOTICE,"Discarding previously cached master state.");

    // 先设置为 cashed_master 的 flags = flag & (~CLIENT_MASTER), CLIENT_MASTER = 1<<1
    server.cached_master->flags &= ~CLIENT_MASTER;

    // 然后再释放 cached_master
    freeClient(server.cached_master);

    server.cached_master = NULL;
}

```

#### 3.3.1.3 cancelReplicationHandshake

```C

// 取消主从复制的握手
int cancelReplicationHandshake(void) {

    // 当前的状态为 正在接收从主节点发送过来的 RDB 文件
    if (server.repl_state == REPL_STATE_TRANSFER) {
        
        // 停止同步传输
        // 关闭从节点同步主节点的 socket 和临时文件描述符 repl_transfer_tmpfile
        replicationAbortSyncTransfer();
        // 更新状态为 未连接上主节点
        server.repl_state = REPL_STATE_CONNECT;

        // 当前的状态为正在建立连接或者处于握手阶段
    } else if (server.repl_state == REPL_STATE_CONNECTING || slaveIsInHandshakeState()) {

        // 关闭 从节点同步主节点信息的 socket : repl_transfer_s
        undoConnectWithMaster();
        server.repl_state = REPL_STATE_CONNECT;
    } else {
        // 其他状态返回 0
        return 0;
    }
    // 正在接收从主节点发送过来的 RDB 文件 或者 正在建立连接或者处于握手阶段, 处理完成后, 返回 1
    return 1;
}

// 停止同步传输, 关闭从节点同步主节点的 socket 和临时文件描述符 repl_transfer_tmpfile
void replicationAbortSyncTransfer(void) {
    serverAssert(server.repl_state == REPL_STATE_TRANSFER);

    // 关闭 从节点同步主节点信息的 socket : repl_transfer_s
    undoConnectWithMaster();
    close(server.repl_transfer_fd);
    unlink(server.repl_transfer_tmpfile);
    zfree(server.repl_transfer_tmpfile);
}

// 停止监听主节点 socket 的信息
void undoConnectWithMaster(void) {

    int fd = server.repl_transfer_s;
    // 删除 repl_transfer_s 这个文件描述符的事件
    aeDeleteFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE);
    // 关闭这个文件描述符
    close(fd);
    // 重置为 - 1 
    server.repl_transfer_s = -1;
}

// 判断当前的从节点的状态是否处于握手阶段中
int slaveIsInHandshakeState(void) {

    // 3 <= server.repl_state <= 13 
    return server.repl_state >= REPL_STATE_RECEIVE_PONG && server.repl_state <= REPL_STATE_RECEIVE_PSYNC;
}
```

#### 3.3.1.4 disconnectSlaves

```C
// 关闭所有的从节点客户端
void disconnectSlaves(void) {

    while (listLength(server.slaves)) {
        listNode *ln = listFirst(server.slaves);
        freeClient((client*)ln->value);
    }
}
```



### 3.3.2 replicationSetMaster - 执行 salveof ip port, 保存主节点的信息

```C
void replicationSetMaster(char *ip, int port) {

    // 获取旧的主节点 host 配置
    int was_master = server.masterhost == NULL;

    // 将入参的 IP 和 端口赋值给对应的字段
    sdsfree(server.masterhost);
    server.masterhost = sdsnew(ip);
    server.masterport = port;

    // 旧的主节点客户端存在, 进行释放
    if (server.master) {
        freeClient(server.master);
    }

    // 解除所有阻塞状态的客户端
    disconnectAllBlockedClients(); 

    // 释放所有的从节点信息 
    disconnectSlaves();

    //  取消主从复制的握手
    cancelReplicationHandshake();

    // 上次有主节点了
    // 假设 A 原本是 B 的从节点, 现在通过 salveof 将 A 变成 C 的从节点
    // 第一次执行时, 是不会走到这一步的
    if (was_master) {

        // 将 server.cached_server 设置为空, 同时设置释放对应的内存
        replicationDiscardCachedMaster();

        // 根据当前参数合成出一个 client, 同时将其放到 cached_master
        replicationCacheMasterUsingMyself();
    }
    // 设置当前的主从复制状态为 REPL_STATE_CONNECT
    server.repl_state = REPL_STATE_CONNECT;
}
```

#### 3.3.2.1 disconnectAllBlockedClients

```C
void disconnectAllBlockedClients(void) {

    listNode *ln;
    listIter li;

    // 将 server.clients 的信息转移到 li 上
    listRewind(server.clients,&li);

    // 遍历 li
    while((ln = listNext(&li))) {

        client *c = listNodeValue(ln);

        if (c->flags & CLIENT_BLOCKED) {
            
            addReplySds(c,sdsnew("-UNBLOCKED force unblock from blocking operation, instance state changed (master -> replica?)\r\n"));

            // 解除所有阻塞状态的客户端
            // unblockClient 会先将入参的 client 先处理后，放到 server.unblocked_clients 这个列表中, 在下次事件循环中进行处理里面的客户端请求处理
            unblockClient(c);
            c->flags |= CLIENT_CLOSE_AFTER_REPLY;
        }
    }

}

```

#### 3.3.2.2 replicationCacheMasterUsingMyself

```C

// 根据当前参数合成出一个 client, 同时将其放到 cached_master
void replicationCacheMasterUsingMyself(void) {

    // 设置当前的 master_initial_offset 等于 master_repl_offset
    server.master_initial_offset = server.master_repl_offset;

    // 创建一个新的主节点客户端, 并存放发到 server.master 中
    replicationCreateMasterClient(-1,-1);

    // 将 server.replid 拷贝到创建出来的节点的 replid
    memcpy(server.master->replid, server.replid, sizeof(server.replid));

    // 通过 replicationCreateMasterClient 创建出来的客户端, unlinkClient 里面的逻辑都不符合条件, 直接结束了
    // 可以看成是没有这个方法的逻辑
    unlinkClient(server.master);

    // 设置 cached_master = master, 同时置空 master 
    server.cached_master = server.master;
    server.master = NULL;
    serverLog(LL_NOTICE,"Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.");
}

// 创建主节点客户端
void replicationCreateMasterClient(int fd, int dbid) {
    
    // 创建新的客户端, 赋值到 master, createClient 创建出来的 flag 默认为 0 
    // 具体的创建逻辑可以查看 networking.c 中的 createClient 函数
    server.master = createClient(fd);

    // 此时 flag = CLIENT_MASTER = 1
    server.master->flags |= CLIENT_MASTER;
    server.master->authenticated = 1;
    server.master->reploff = server.master_initial_offset;
    server.master->read_reploff = server.master->reploff;
    memcpy(server.master->replid, server.master_replid, sizeof(server.master_replid));

    // 如果一个主节点的偏移量为 -1, 那么这个主节点是旧的, 并且是无法进行 psync 的, 所以将其设置为 CLIENT_PRE_PSYNC (1 << 16, 65536)
    if (server.master->reploff == -1)
        // | CLIENT_PRE_PSYNC 后, flags 等于 65537
        server.master->flags |= CLIENT_PRE_PSYNC;

    if (dbid != -1) 
        selectDb(server.master, dbid);
}
```


### 3.3.3 connectWithMaster - 尝试和主节点建立 TCP 连接

```C
int connectWithMaster(void) {

    int fd;

    // 建立非阻塞的 Tcp 连接
    fd = anetTcpNonBlockBestEffortBindConnect(NULL,server.masterhost,server.masterport,NET_FIRST_BIND_ADDR);

    // 文件描述符为 -1, 建立连接失败
    if (fd == -1) {
        serverLog(LL_WARNING,"Unable to connect to MASTER: %s", strerror(errno));
        return C_ERR;
    }

    // 添加 fd 的可读和可写事件, 处理的逻辑函数为 syncWithMaster
    // 注意这里注册的类型为 AE_READABLE|AE_WRITABLE, 在 epoll 中, 底层注册的是 EPOLLIN | EPOLLOUT
    // epoll 有个机制, 同时注册 EPOLLIN | EPOLLOUT 事件, 会触发一次 EPOLLOUT 事件, 也就是 AE_WRITABLE 事件
    // 也就是在下次事件轮询中会执行一次 syncWithMaster 函数

    if (aeCreateFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE,syncWithMaster,NULL) == AE_ERR) {
        close(fd);
        serverLog(LL_WARNING,"Can't create readable event for SYNC");
        return C_ERR;
    }

    // 更新从节点和主节点最近一次进行数据同步的时间, 默认为当前时间
    server.repl_transfer_lastio = server.unixtime;
    server.repl_transfer_s = fd;
    server.repl_state = REPL_STATE_CONNECTING;
    return C_OK;
}
```

### 3.3.4 sendSynchronousCommand - 发送同步命令到主节点

```C
char *sendSynchronousCommand(int flags, int fd, ...) {
    
    // 发送写命令
    if (flags & SYNC_CMD_WRITE) {

        // 发送 ping 命令时, 入参为 (SYNC_CMD_WRITE,fd,"PING",NULL)
        // flags = SYNC_CMD_WRITE
        
        char *arg;

        // 下面的 va_ 开头的函数, 都是用来处理动态入参的
        va_list ap;
        sds cmd = sdsempty();
        sds cmdargs = sdsempty();
        size_t argslen = 0;
        va_start(ap,fd);

        while(1) {
            arg = va_arg(ap, char*);

            if (arg == NULL) 
                break;
            // 拼接 cmd 命令的参数
            cmdargs = sdscatprintf(cmdargs,"$%zu\r\n%s\r\n",strlen(arg),arg);
            argslen++;
        }

        va_end(ap);

        cmd = sdscatprintf(cmd,"*%zu\r\n",argslen);
        cmd = sdscatsds(cmd,cmdargs);
        sdsfree(cmdargs);

        // 发送给对应的文件描述符, 这里的 fd 是 Socket 通道
        if (syncWrite(fd, cmd, sdslen(cmd), server.repl_syncio_timeout*1000) == -1) {
            sdsfree(cmd);
            return sdscatprintf(sdsempty(),"-Writing to master: %s",strerror(errno));
        }

        sdsfree(cmd);
    }

    // 接受读操作
    if (flags & SYNC_CMD_READ) {
        char buf[256];

        // 读取数据到 buf 中
        if (syncReadLine(fd,buf,sizeof(buf),server.repl_syncio_timeout*1000) == -1) {
            return sdscatprintf(sdsempty(),"-Reading from master: %s", strerror(errno));
        }
        server.repl_transfer_lastio = server.unixtime;
        return sdsnew(buf);
    }
    return NULL;
}
```



### 

## 3. 参考

[epoll简介及触发模式（accept、read、send）](https://www.cnblogs.com/zl-graduate/articles/6724446.html)






















## 

```C

// 清除 replid2 的值
void clearReplicationId2(void) {
    memset(server.replid2,'0',sizeof(server.replid));
    server.replid2[CONFIG_RUN_ID_SIZE] = '\0';
    server.second_replid_offset = -1;
}
```




```C

// 释放 repl_backlog 占的内存, 同时置为空
void freeReplicationBacklog(void) {
    serverAssert(listLength(server.slaves) == 0);
    zfree(server.repl_backlog);
    server.repl_backlog = NULL;
}

// 释放 repl_scriptcache_dict 缓存, 清空 repl_scriptcache_fifo
void replicationScriptCacheFlush(void) {
    dictEmpty(server.repl_scriptcache_dict,NULL);
    listRelease(server.repl_scriptcache_fifo);
    server.repl_scriptcache_fifo = listCreate();
}

// 更新当前从节点中延迟值小于配置的
void refreshGoodSlavesCount(void) {

    listIter li;
    listNode *ln;
    int good = 0;

    // 没有配置 repl_min_slaves_to_write 和 repl_min_slaves_to_write 直接返回
    if (!server.repl_min_slaves_to_write || !server.repl_min_slaves_max_lag) 
        return;

    listRewind(server.slaves,&li);
    while((ln = listNext(&li))) {

        client *slave = ln->value;

        // 当前的时间 - 复制 ack 的时间
        time_t lag = server.unixtime - slave->repl_ack_time;

        // 当前从节点的状态为在线状态 
        // 小于配置的从节点最小应答时间
        if (slave->replstate == SLAVE_STATE_ONLINE && lag <= server.repl_min_slaves_max_lag) 
            good++;
    }

    server.repl_good_slaves_count = good;
}


// 连接主节点
int connectWithMaster(void) {
    int fd;

    // 和父节点建立 tcp 连接, 也就是 socket 连接
    fd = anetTcpNonBlockBestEffortBindConnect(NULL, server.masterhost,server.masterport,NET_FIRST_BIND_ADDR);

    if (fd == -1) {
        serverLog(LL_WARNING,"Unable to connect to MASTER: %s", strerror(errno));
        return C_ERR;
    }

    // 注册事件, 执行的逻辑为 syncWithMaster,
    // 注意这里注册的类型为 AE_READABLE|AE_WRITABLE, 在 epoll 中, 底层注册的是 EPOLLIN | EPOLLOUT
    // epoll 有个机制, 同时注册 EPOLLIN | EPOLLOUT 事件, 会触发一次 EPOLLOUT 事件, 也就是 AE_WRITABLE 事件
    // 也就是在下次事件轮询中会执行一次 syncWithMaster 函数
    if (aeCreateFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE,syncWithMaster,NULL) ==  AE_ERR) {
        close(fd);
        serverLog(LL_WARNING,"Can't create readable event for SYNC");
        return C_ERR;
    }

    // 更新从节点和主节点最近一次进行数据同步的时间, 默认为当前时间
    server.repl_transfer_lastio = server.unixtime;
    // 保存文件描述符
    server.repl_transfer_s = fd;
    // 修改状态为 REPL_STATE_CONNECTING (真正连接中)
    server.repl_state = REPL_STATE_CONNECTING;
    return C_OK;
}

```

复制开始

```C

int startBgsaveForReplication(int mincapa) {

    int retval;
    int socket_target = server.repl_diskless_sync && (mincapa & SLAVE_CAPA_EOF);
    listIter li;
    listNode *ln;

    serverLog(LL_NOTICE,"Starting BGSAVE for SYNC with target: %s", socket_target ? "replicas sockets" : "disk");

    rdbSaveInfo rsi, *rsiptr;

    rsiptr = rdbPopulateSaveInfo(&rsi);

    if (rsiptr) {
        if (socket_target)
            retval = rdbSaveToSlavesSockets(rsiptr);
        else
            retval = rdbSaveBackground(server.rdb_filename,rsiptr);
    } else {
        serverLog(LL_WARNING,"BGSAVE for replication: replication information not available, can't generate the RDB file right now. Try later.");
        retval = C_ERR;
    }

    // TODO 

}
```





## 



```C
void replicationCron(void) {


    static long long replication_cron_loops = 0;

    // 非阻塞连接超时判断, 下面 3 个条件都满足, 取消握手状态
    // 1. 有主节点配置
    // 2. 当前的状态为 REPL_STATE_CONNECTING (真正建立连接中) 或者处于握手阶段
    // 3. 当前的时间 - 上次读取数据开始的时间 > 配置的超时时间
    if (server.masterhost && (server.repl_state == REPL_STATE_CONNECTING || slaveIsInHandshakeState()) && (time(NULL)-server.repl_transfer_lastio) > server.repl_timeout) {
        serverLog(LL_WARNING,"Timeout connecting to the MASTER...");
        // 取消握手
        cancelReplicationHandshake();
    }

    // 批量传输 I/O 超时, 下面 3 个条件都满足, 取消握手状态
    // 1. 有主节点配置
    // 2. 当前的状态为 REPL_STATE_CONNECTING (正在接收从主节点发送过来的 RDB 文件)
    // 3. 当前的时间 - 上次读取数据开始的时间 > 配置的超时时间
    if (server.masterhost && server.repl_state == REPL_STATE_TRANSFER && (time(NULL)-server.repl_transfer_lastio) > server.repl_timeout) {
        serverLog(LL_WARNING,"Timeout receiving bulk data from MASTER... If the problem persists try to set the 'repl-timeout' parameter in redis.conf to a larger value.");
        cancelReplicationHandshake();
    }

    // 主从节点已经建立连接了, 但是主节点的心跳超时, 每隔 10 s，主节点从节点发送 PING 命令，从节点每隔 1s, 向主节点报告复制偏移量
    // 1. 有主节点配置
    // 2. 当前的状态为 REPL_STATE_CONNECTED (主从节点已经存于连接状态)
    // 3. 当前的时间 - 主节点的上次心跳时间 > 配置的超时时间
    if (server.masterhost && server.repl_state == REPL_STATE_CONNECTED && (time(NULL)-server.master->lastinteraction) > server.repl_timeout) {
        serverLog(LL_WARNING,"MASTER timeout: no data nor PING received...");
        // 释放主节点
        freeClient(server.master);
    }

    // 当前的状态为 REPL_STATE_CONNECT (开启了主从复制, 但是还没连接上主节点), 顺利执行了 salveof 后, 从节点的默认状态
    if (server.repl_state == REPL_STATE_CONNECT) {
        serverLog(LL_NOTICE,"Connecting to MASTER %s:%d", server.masterhost, server.masterport);

        // 连接主节点
        if (connectWithMaster() == C_OK) {
            serverLog(LL_NOTICE,"MASTER <-> REPLICA sync started");
        }
    }

    // 有主节点配置, 同时主节点的标识不是 CLIENT_PRE_PSYNC, 发送 ack 到主节点
    if (server.masterhost && server.master && !(server.master->flags & CLIENT_PRE_PSYNC))
        replicationSendAck();

    // 如果服务器有从节点, 需要定期发送 PING 命令
    // 所以从节点可以实现一个显式的 timeout 去判断和主节点的超时连接，即使 TCP 连接不会中断，也能察觉一个连接的重新连接, 上面的超时判断就是基于这个操作

    listIter li;
    listNode *ln;
    robj *ping_argv[1];

    
    // 这个是集群 cluster 的功能逻辑, 可以跳过
    // replication_cron_loops 方法开头声明的变量, 初始为 0 , repl_ping_slave_period 主节点 PING 的间隔, 默认 10s 
    if ((replication_cron_loops % server.repl_ping_slave_period) == 0 && listLength(server.slaves)) {
        
        // 开启了集群功能
        // 当前处于手动故障转移中, (mf_end, mf_end 为 0, 表示没有正在进行手动故障转移, 大于 0 了, 表示进行中)
        // 当前所有的客户端 (从节点和 阻塞客户端不算) 都是处于暂停状态
        int manual_failover_in_progress = server.cluster_enabled && server.cluster->mf_end && clientsArePaused();

        if (!manual_failover_in_progress) {

            // 创建出一个 PING
            ping_argv[0] = createStringObject("PING",4);
            // 发送 PING 到从节点
            replicationFeedSlaves(server.slaves, server.slaveseldb, ping_argv, 1);
            // 减少引用次数
            decrRefCount(ping_argv[0]);
        }
    }

    listRewind(server.slaves,&li);
    while((ln = listNext(&li))) {
        
        client *slave = ln->value;

        // 如果从节点的复制状态处于等待 RDB 文件被主节点创建的状态
        // 从节点的复制状态处于等待 RDB 文件创建节点 并且 RDB 文件的类型不是无盘传输
        int is_presync = (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START || (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_END && server.rdb_child_type != RDB_CHILD_TYPE_SOCKET));

        if (is_presync) {
            // 发送一个'\n'，保持活跃主从连接的状态
            if (write(slave->fd, "\n", 1) == -1) {
            }
        }
    }

    // 断开所有超时的客户端
    if (listLength(server.slaves)) {
        listIter li;
        listNode *ln;

        listRewind(server.slaves,&li);
        while((ln = listNext(&li))) {
            client *slave = ln->value;

            if (slave->replstate != SLAVE_STATE_ONLINE) continue;

            if (slave->flags & CLIENT_PRE_PSYNC) continue;

            // 当前时间 - 客户端上次 ack 的时间 > 配置的超时时间
            if ((server.unixtime - slave->repl_ack_time) > server.repl_timeout) {
                serverLog(LL_WARNING, "Disconnecting timedout replica: %s",replicationGetSlaveName(slave));
                freeClient(slave);
            }
        }    
    }

    // 没有从节点
    // 缓冲区不为空
    // 没有配置的主节点
    if (listLength(server.slaves) == 0 && server.repl_backlog_time_limit && server.repl_backlog && server.masterhost == NULL) {

        // 计算出当前时间到没有从节点的时间
        time_t idle = server.unixtime - server.repl_no_slaves_since;

        //超过 repl_backlog_time_limit 限制则释放 backlog
        if (idle > server.repl_backlog_time_limit) {

            //  重新生成一个 runId, 并赋值到 server.replid
            changeReplicationId();
            // 清除 replid2 的值
            clearReplicationId2();
            // 释放 repl_backlog 占的内存, 同时置为空
            freeReplicationBacklog();
            serverLog(LL_NOTICE, "Replication backlog freed after %d seconds without connected replicas.", (int) server.repl_backlog_time_limit);
        }
    }

    // 没有从节点
    // aof 功能关闭
    // 复制脚本缓存有数据
    if (listLength(server.slaves) == 0 && server.aof_state == AOF_OFF && listLength(server.repl_scriptcache_fifo) != 0) {
        // 清空复制脚本缓存
        replicationScriptCacheFlush();
    }

    // 没有后台进程在 RDB 或者 AOF
    if (server.rdb_child_pid == -1 && server.aof_child_pid == -1) {

        time_t idle, max_idle = 0;
        int slaves_waiting = 0;
        int mincapa = -1;
        listNode *ln;
        listIter li;

        listRewind(server.slaves,&li);
        // 遍历所有的从节点，记录等待 BGSAVE 的开始的从节点个数
        while((ln = listNext(&li))) {

            client *slave = ln->value;
            if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START) {

                // 空闲的时间, 当前的时间 - 当前从节点上次心跳的时间
                idle = server.unixtime - slave->lastinteraction;

                // 最大空闲时间
                if (idle > max_idle) 
                    max_idle = idle;
                
                slaves_waiting++;

                // slave_capa 从节点支持的能力 0：什么都不支持 1：支持通过解析 RDB 文件流 2：支持 PSYNC2 协议
                // 获取所有从节点都支持的最小能力
                mincapa = (mincapa == -1) ? slave->slave_capa : (mincapa & slave->slave_capa);
            }

        }

        // 存在等待 bgsave 的从节点 
        // 服务器不支持无盘复制 或 最大空闲时间超过无盘复制的延迟时限
        if (slaves_waiting && (!server.repl_diskless_sync || max_idle > server.repl_diskless_sync_delay)) {
            // 开启后台的复制
            startBgsaveForReplication(mincapa);
        }
    }

    // 更新延迟小于 min-slaves-max-lag 的从服务器数量
    refreshGoodSlavesCount();
    replication_cron_loops++;
}
```
