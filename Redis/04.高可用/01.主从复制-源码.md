# 2 主从复制 - 源码

因为主从复制的过程很复杂, 同时集中在 replication.c 这个文件中, 所以为了篇幅, 单独的把功能中涉及的大部分代码都集中到了 [03.主从复制-replication源码分析] 这篇文章中,  
这篇主要涉及主从复制的大体代码逻辑, 如果需要了解整体的过程, 可以配合 [03.主从复制-replication源码分析] 这篇文章。


## 2.1 主从节点建立连接

Redis 主从节点建立连接的 3 种方式, 本质都是从节点执行 **slaveof** 命令, 和父节点建立初步的关联关系。   
这个命令执行的方法为 **replicaofCommand**。

```C
void replicaofCommand(client *c) {

    // 开启了集群功能, 直接返回, 集群模式不允许执行 slaveof 
    if (server.cluster_enabled) {
        addReplyError(c,"REPLICAOF not allowed in cluster mode.");
        return;
    }
    
    // 第一参数为 no, 第二个参数为 one
    // slaveof no one, 可以让从节点和主节点断开连接, 停止主从复制 
    if (!strcasecmp(c->argv[1]->ptr,"no") && !strcasecmp(c->argv[2]->ptr,"one")) {

        // 如果保存了主节点IP
        if (server.masterhost) {
            
            // 取消复制操作, 同时设置当前节点为主节点
            replicationUnsetMaster();

            // 获取 client 的每种信息, 并以 sds 形式返回, 并打印到日志中
            sds client = catClientInfoString(sdsempty(),c);
            serverLog(LL_NOTICE,"MASTER MODE enabled (user request from '%s')", client);
            sdsfree(client);
        }

    } else {

        long port;

        // 当前的客户端的标识为从节点标识
        // 本身是一个从节点了, 无法在执行 salveof ip 端口
        if (c->flags & CLIENT_SLAVE) {

            addReplyError(c, "Command is not valid when client is a replica.");
            return;
        }

        // 从入参中获取端口
        if ((getLongFromObjectOrReply(c, c->argv[2], &port, NULL) != C_OK))
            return;

        // 已经有主节点了, 同时主节点的的 host 和 ip 和入参的相同
        if (server.masterhost && !strcasecmp(server.masterhost,c->argv[1]->ptr) && server.masterport == port) {    

            serverLog(LL_NOTICE,"REPLICAOF would result into synchronization with the master we are already connected with. No operation performed.");
            addReplySds(c,sdsnew("+OK Already connected to specified master\r\n"));
            return;
        }

        // 保存主节点的信息
        replicationSetMaster(c->argv[1]->ptr, port);

        sds client = catClientInfoString(sdsempty(),c);
        serverLog(LL_NOTICE,"REPLICAOF %s:%d enabled (user request from '%s')", server.masterhost, server.masterport, client);
        sdsfree(client);
    }

    addReply(c,shared.ok);
}

```

这个函数的逻辑涉及很多细节
> 释放之前被阻塞的客户端, 这些通常是使用 Redis 阻塞列表而被阻塞的客户端
> 断开当前实例的所有 slave
> 清除缓存的 master 信息, 释放复制积压缓冲区 backlog
> 取消正在进行的握手过程
> 等等

但是当前第一步主要的逻辑就是将当前的主节点的 IP 和 Post 保存起来, 同时经过这一步, 当前实例的复制状态设置为 REPL_STATE_CONNECT。

执行完上的逻辑后, salveof 就结束的, 但是整个的主从复制还没有开始, 可以得出 salveof 是一个异步的命令。 接下来的步骤则是由定时函数 serverCron 定时的调用。

## 2.2 主从网络连接建立

在第一步中, 只是将主节点的信息保存到从节点中就结束了, 之间还是没有建立起相关的网络连接的, 第二步就是完成这个网络连接的操作。


```C
int serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) {

    // 省略

    // 定时 1 秒执行一次
    run_with_period(1000) replicationCron();

    // 省略
}
```

replicationCron 里面涉及到了大量的逻辑, 基本整个复制运行阶段的状态判断等都是在里面判断的, 这里只截取了涉及到当前步骤相关的逻辑。
在第一步操作完成后, 可以知道从节点当前的状态为 **REPL_STATE_CONNECT**。

```C
void replicationCron(void) {

    // 省略

    // 当前的状态为 REPL_STATE_CONNECT (开启了主从复制, 但是还没连接上主节点), 顺利执行了 salveof 后, 从节点的默认状态
    if (server.repl_state == REPL_STATE_CONNECT) {

        // 尝试连接主节点, 连接成功后, 从节点的状态会变为 REPL_STATE_CONNECTING (正在连接主节点)

        // 1. 通过保存的 IP 和 Port 和主节点建立一个 TCP 连接
        // 2. 向事件轮询添加对应的 Socket 通道的读写事件
        // 3. 更新当前节点的状态为 REPL_STATE_CONNECTING (正在连接主节点)
        if (connectWithMaster() == C_OK) {
            serverLog(LL_NOTICE, "MASTER <-> REPLICA sync started");
        }
    }
    // 省略
}
```

第二步的逻辑很简单, 和主节点建立起了 Socket 连接, 同时将当前节点的状态更新为 REPL_STATE_CONNECTING


## 2.3 发送 PING 命令

在第二步的步骤中, 通过保存的主节点 IP 和端口建立起连接后, 会向事件轮询中注册一个 AE_READABLE|AE_WRITABLE 的事件, 
在底层的 epollo 中就是同时注册了一个 EPOLLIN | EPOLLOUT 事件, 这样会触发一次 EPOLLOUT 事件, 也就是触发一次  AE_WRITABLE 事件,
也就是在下次事件轮询中会执行到其注册的函数 syncWithMaster 函数, 所以第三步的入口就是这个函数了。

同样的这个方法涉及到了大量的主从通信复制相关的逻辑, 整个逻辑很复杂, 所以也截取了相关的代码

```C
// 入参中的 fd 就是和主节点建立的 Socket 连接的文件描述符
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    int sockerr = 0;
    socklen_t errlen = sizeof(sockerr);
    
    UNUSED(el);
    UNUSED(privdata);
    UNUSED(mask);

    // 状态为 REPL_STATE_NONE, 关闭对应的文件描述符
    if (server.repl_state == REPL_STATE_NONE) {
        close(fd);
        return;
    }

    // 检查当前的 Socket 通道的状态
    if (getsockopt(fd, SOL_SOCKET, SO_ERROR, &sockerr, &errlen) == -1)
        // 获取异常信息
        sockerr = errno;

    // 有异常信息
    if (sockerr) {
        serverLog(LL_WARNING,"Error condition on socket for SYNC: %s", strerror(sockerr));
        goto error;
    }


    if (server.repl_state == REPL_STATE_CONNECTING) {

        serverLog(LL_NOTICE,"Non blocking connect for SYNC fired the event.");
        
        // 删除当前这个 Socket 的可写事件, 不关心写事件
        aeDeleteFileEvent(server.el,fd,AE_WRITABLE);

        // 状态修改为 REPL_STATE_RECEIVE_PONG (发送 pong, 等待 ping 回答)
        server.repl_state = REPL_STATE_RECEIVE_PONG;
        
        // 发送同步命令, 也就是 ping 到主节点, SYNC_CMD_WRITE = 1
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"PING",NULL);
        if (err) 
            goto write_error;
        return;
    }

    // 省略

error:
    // 删除对应的事件和文件描述符
    aeDeleteFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE);
    if (dfd != -1) close(dfd);
    close(fd);
    server.repl_transfer_s = -1;
    // 重新将状态设置为 REPL_STATE_CONNECT, 重新走一遍建立连接的过程
    server.repl_state = REPL_STATE_CONNECT;
    return;

write_error:
    // 打印日志
    serverLog(LL_WARNING,"Sending command to master in replication handshake: %s", err);
    sdsfree(err);
    goto error;

}
```

从节点向主节点发送了一个 Ping 的命令, 这时候主节点收到了从节点的 Ping 命令后, 处理正常后, 会响应一个 Pong 的命令。
这时候从节点的状态为 REPL_STATE_RECEIVE_PONG

**主节点**执行的 Ping 命令的逻辑如下

```C
void pingCommand(client *c) {

    // ping 命令的参数只能是 1 个或者 0 个
    if (c->argc > 2) {
        addReplyErrorFormat(c,"wrong number of arguments for '%s' command",
            c->cmd->name);
        return;
    }

    // 对应的客户端处于 Pub/Sub 模式
    if (c->flags & CLIENT_PUBSUB) {
        addReply(c,shared.mbulkhdr[2]);
        addReplyBulkCBuffer(c,"pong",4);
        if (c->argc == 1)
            addReplyBulkCBuffer(c,"",0);
        else
            addReplyBulk(c,c->argv[1]);
    } else {

        // 其他模式
        
        // 参数是 1 个, 响应一个 pong
        if (c->argc == 1)
            addReply(c,shared.pong);
        else
            // 响应入参的第 2 个参数
            addReplyBulk(c,c->argv[1]);
    }
}
```

**从节点** 收到了主节点发送过来的 Ping 命令, 同时在上面第 2 步就建立了对应的可读事件, 这时事件轮询循环中找到了监听到了可读事件, 又执行到 **syncWithMaster** 函数

```C

void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 当前从节点处于 REPL_STATE_RECEIVE_PONG 状态 (发送 ping, 等待 pong  应答)
    if (server.repl_state == REPL_STATE_RECEIVE_PONG) {
        
        // 读取主节点响应的信息
        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);

        // 异常情况
        if (err[0] != '+' && strncmp(err,"-NOAUTH",7) != 0 && strncmp(err,"-ERR operation not permitted",28) != 0) {
            serverLog(LL_WARNING,"Error reply to PING from master: '%s'",err);
            sdsfree(err);
            goto error;
        } else {
            // 响应的是 Pong, 响应了 Ping 请求, 能继续处理
            serverLog(LL_NOTICE, "Master replied to PING, replication can continue...");
        }
        sdsfree(err);
        // 状态切换到 REPL_STATE_SEND_AUTH, 等待认证结果应答
        server.repl_state = REPL_STATE_SEND_AUTH;
    }
}
```


## 2.4 认证权限

在从节点发送 Ping, 主节点响应 Pong , 从节点收到 Pong 响应后, 进入处理时 (syncWithMaster 函数), 
状态修改为 **REPL_STATE_SEND_AUTH** 后, 方法继续执行下去, 立即进入认证权限的过程。

```C

void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 当前从节点处于 REPL_STATE_RECEIVE_PONG 状态 (发送 ping, 等待 pong  应答)
    if (server.repl_state == REPL_STATE_RECEIVE_PONG) {

        // 省略

        // 状态切换到 REPL_STATE_SEND_AUTH, 等待认证结果应答
        server.repl_state = REPL_STATE_SEND_AUTH;
    }

    // 进入认证, 如果需要的话
    if (server.repl_state == REPL_STATE_SEND_AUTH) {
        // 配置了主节点的密码
        if (server.masterauth) {
            // 发送认证请求和密码到主节点
            err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"AUTH",server.masterauth,NULL);
            if (err) 
                goto write_error;
            // 状态切换为 REPL_STATE_RECEIVE_AUTH
            server.repl_state = REPL_STATE_RECEIVE_AUTH;
            return;
        } else {
            // 不需要认证, 状态之间切换为 REPL_STATE_SEND_PORT 准备发送端口
            server.repl_state = REPL_STATE_SEND_PORT;
        }
    }

}
```

**从节点**根据是否配置了主节点认证密码, 走不通的逻辑
> 1. 配置了认证密码, 发送 AUTH 密码, 同时带上密码到主节点, 同时状态变为 REPL_STATE_RECEIVE_AUTH (等待主节点响应认证结果应答)
> 2. 没有配置认证密码, 直接将状态变为 REPL_STATE_SEND_PORT (准备发送从节点的监听的端口)


**主节点**收到从节点的认证请求 auth, 就会进入到权限认证的过程,  执行的逻辑如下:

```C
void authCommand(client *c) {

    // 主节点不需要密码认证
    if (!server.requirepass) {
        addReplyError(c,"Client sent AUTH, but no password is set");
    } else if (!time_independent_strcmp(c->argv[1]->ptr, server.requirepass)) {

      // 密码认证成功  
      c->authenticated = 1;
      addReply(c,shared.ok);

    } else {

      // 密码认证失败  
      c->authenticated = 0;
      addReplyError(c,"invalid password");
    }
}
```

同 Ping Pong 的处理逻辑一样, 这时从节点读取到主节点的响应, 事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 接收到请求认证的响应
    if (server.repl_state == REPL_STATE_RECEIVE_AUTH) {

        // 读取响应信息
        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);
        
        // 认证失败
        if (err[0] == '-') {
            serverLog(LL_WARNING,"Unable to AUTH to MASTER: %s",err);
            sdsfree(err);
            goto error;
        }

        // 认证成功
        sdsfree(err);
        // 状态变为 REPL_STATE_SEND_PORT (准备发送从节点的监听的端口)
        server.repl_state = REPL_STATE_SEND_PORT;
    }
}
```

## 2.5 发送端口号

在不需要权限认证或者从节点收到主节点的权限认证成功后, 此时从节点的状态为 **REPL_STATE_SEND_PORT**, 同时是在上一步的处理逻辑中, 继续处理

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 收到权限认证的响应
    if (server.repl_state == REPL_STATE_RECEIVE_AUTH) {

        // 省略

        server.repl_state = REPL_STATE_SEND_PORT;
    }

    // 进入发送端口阶段
    if (server.repl_state == REPL_STATE_SEND_PORT) {

        // 如果有配置一个专门复制的端口的话, 使用配置的端口, 没有使用当前服务器的端口
        sds port = sdsfromlonglong(server.slave_announce_port ? server.slave_announce_port : server.port);

        // 发送端口信息给主节点
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"REPLCONF", "listening-port",port, NULL);
        sdsfree(port);
        if (err) 
            goto write_error;
        sdsfree(err);
        // 切换状态为 REPL_STATE_RECEIVE_PORT ()
        server.repl_state = REPL_STATE_RECEIVE_PORT;
        return;
    }
}
```

**主节点**收到从节点的发送端口请求 REPLCONF, 执行的逻辑如下

```C
void replconfCommand(client *c) {

    int j;

    // 参数需要是 2 的倍数
    if ((c->argc % 2) == 0) {
        addReply(c,shared.syntaxerr);
        return;
    }

    for (j = 1; j < c->argc; j+=2) {
        // 每个循环使用 2 个参数

        // replconf listening-port port
        if (!strcasecmp(c->argv[j]->ptr,"listening-port")) {

            long port;

            // 获取下一个项, 也就是端口号
            if ((getLongFromObjectOrReply(c,c->argv[j+1], &port,NULL) != C_OK))
                return;

            // 保存到对应的客户端的 slave_listening_port     
            c->slave_listening_port = port;
        } else if (!strcasecmp(c->argv[j]->ptr,"ip-address")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"capa")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ack")) {
            // 省略
        }  else if (!strcasecmp(c->argv[j]->ptr,"getack")) {
            // 省略
        } else {
            // 响应错误
            addReplyErrorFormat(c,"Unrecognized REPLCONF option: %s", (char*)c->argv[j]->ptr);
            return;
        }
    }

    // 响应 OK 
    addReply(c,shared.ok);
}
```


可以看到主节点收到从节点发送过来的端口, 会保存到从节点客户端 client 的 slave_listening_port 字段。

收到主节点的响应后, 从节点同样是事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // REPL_STATE_RECEIVE_PORT 等待主节点响应发送 IP 请求的响应
    if (server.repl_state == REPL_STATE_RECEIVE_PORT) {

        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);
        if (err[0] == '-') {
            serverLog(LL_NOTICE,"(Non critical) Master does not understand REPLCONF listening-port: %s", err);
        }
        sdsfree(err);
        // 状态变为 REPL_STATE_SEND_IP, 准备发送 IP 到主节点
        server.repl_state = REPL_STATE_SEND_IP;
    }
}
```

## 2.6 发送 IP 地址

发送端口和收到主节点对应的响应, 从节点的状态变为 REPL_STATE_SEND_IP (准备发送 IP 地址), 然后顺着函数的逻辑, 继续执行下去

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // REPL_STATE_RECEIVE_PORT 等待主节点响应发送 IP 请求的响应
    if (server.repl_state == REPL_STATE_RECEIVE_PORT) {
        // 省略
        // 状态变为 REPL_STATE_SEND_IP, 准备发送 IP 到主节点
        server.repl_state = REPL_STATE_SEND_IP;
    }

    // 没有配置宣布的 IP, slave_announce_ip 为空, 直接跳过发送 IP 的阶段
    if (server.repl_state == REPL_STATE_SEND_IP && server.slave_announce_ip == NULL) {
        // 进入下一个节点 准备发送从节点的发送能力
        server.repl_state = REPL_STATE_SEND_CAPA;
    }

    if (server.repl_state == REPL_STATE_SEND_IP) {
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"REPLCONF", "ip-address",server.slave_announce_ip, NULL);
        if (err) 
            goto write_error;
        sdsfree(err);
        // 状态变为 REPL_STATE_RECEIVE_IP 切换为 等待主节点响应收到从节点的 IP 地址
        server.repl_state = REPL_STATE_RECEIVE_IP;
        return;
    }
}
```

从节点进入发送 IP 地址阶段时, 除了状态需要为 REPL_STATE_SEND_IP (准备发送 IP 地址阶段), 还必须有指定 slave_announce_ip, 从节点的 IP (对应配置文件的 slave-announce-ip),  
2 个条件都满足的情况下, 才会真正的进入发送 IP 地址, 否则直接进入下一阶段。

**主节点**收到从节点的发送端口请求 REPLCONF, 执行的逻辑如下

```C
void replconfCommand(client *c) {

    int j;

    // 参数需要是 2 的倍数
    if ((c->argc % 2) == 0) {
        addReply(c,shared.syntaxerr);
        return;
    }

    for (j = 1; j < c->argc; j+=2) {
        // 每个循环使用 2 个参数

        if (!strcasecmp(c->argv[j]->ptr,"listening-port")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ip-address")) {
            
            // replconf ip-address ip

            // 获取对应的从节点发送的 IP 
            sds ip = c->argv[j+1]->ptr;

            // IP 的长度判断
            if (sdslen(ip) < sizeof(c->slave_ip)) {
                // 保存到客户端的 client 的 slave_ip 属性
                memcpy(c->slave_ip,ip,sdslen(ip)+1);
            } else {
                // 错误提示
                addReplyErrorFormat(c,"REPLCONF ip-address provided by replica instance is too long: %zd bytes", sdslen(ip));
                return;
            }

        } else if (!strcasecmp(c->argv[j]->ptr,"capa")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ack")) {
            // 省略
        }  else if (!strcasecmp(c->argv[j]->ptr,"getack")) {
            // 省略
        } else {
            // 响应错误
            addReplyErrorFormat(c,"Unrecognized REPLCONF option: %s", (char*)c->argv[j]->ptr);
            return;
        }
    }
    
    // 响应 OK 
    addReply(c,shared.ok);
}
```

主节点收到从节点发送的 IP 地址, 会将其保存到从节点的客户端 client 的 slave_ip 字段。

收到主节点的响应后, 从节点同样是事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 收到主节点对发送 IP 请求的响应
    if (server.repl_state == REPL_STATE_RECEIVE_IP) {

        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);

        if (err[0] == '-') {
            serverLog(LL_NOTICE,"(Non critical) Master does not understand REPLCONF ip-address: %s", err);
        }
        sdsfree(err);

        // 状态变为等待发送发送能力状态
        server.repl_state = REPL_STATE_SEND_CAPA;
    }
}
```

## 2.7 发送同步能力 (数据同步的方式)

收到主节点响应的 IP 请求, 从节点的状态切换为了 REPL_STATE_SEND_CAPA, 当然如果从节点没有配置  slave-announce-ip, 也就不会有发送 IP 相关的操作, 直接过度到 REPL_STATE_SEND_CAPA。
然后继续下面的逻辑, 同样在 **syncWithMaster** 函数

```C

void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    if (server.repl_state == REPL_STATE_SEND_CAPA) {

        // 发送从节点支持的发送能力到主节点
        err = sendSynchronousCommand(SYNC_CMD_WRITE,fd,"REPLCONF", "capa","eof","capa","psync2",NULL);
        if (err) goto write_error;
        sdsfree(err);
        // 状态修改为 REPL_STATE_RECEIVE_CAPA, 等待主节点响应发送能力的应答
        server.repl_state = REPL_STATE_RECEIVE_CAPA;
        return;
    }
}
```

从节点发送过去的支持的 2 种发送能力
> 1. eof: 全量复制, 能够解析出 RDB 文件的 EOF 流格式, 用于无盘复制的方式中
> 2. psync2: 部分复制, 利用复制积压缓冲区等实现部分同步

**主节点**收到从节点的发送端口请求 REPLCONF, 执行的逻辑如下

```C
void replconfCommand(client *c) {

    int j;

    // 参数需要是 2 的倍数
    if ((c->argc % 2) == 0) {
        addReply(c,shared.syntaxerr);
        return;
    }

    for (j = 1; j < c->argc; j+=2) {
        // 每个循环使用 2 个参数

        if (!strcasecmp(c->argv[j]->ptr,"listening-port")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"ip-address")) {
            // 省略
        } else if (!strcasecmp(c->argv[j]->ptr,"capa")) {

            if (!strcasecmp(c->argv[j+1]->ptr,"eof"))
                // SLAVE_CAPA_EOF = 1
                c->slave_capa |= SLAVE_CAPA_EOF;
            else if (!strcasecmp(c->argv[j+1]->ptr,"psync2"))
                // SLAVE_CAPA_PSYNC2 = 2 
                c->slave_capa |= SLAVE_CAPA_PSYNC2;

            // 如果不支持的能力, 不做处理
        } else if (!strcasecmp(c->argv[j]->ptr,"ack")) {
            // 省略
        }  else if (!strcasecmp(c->argv[j]->ptr,"getack")) {
            // 省略
        } else {
            // 响应错误
            addReplyErrorFormat(c,"Unrecognized REPLCONF option: %s", (char*)c->argv[j]->ptr);
            return;
        }
    }
    
    // 响应 OK 
    addReply(c,shared.ok);
}
```

收到从节点发送过来的支持的发送的同步能力后, 将其放到从节点客户端的 slave_capa 中。

从节点收到主节点的响应后, 从节点同样是事件轮询触发 **syncWithMaster** 函数

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    if (server.repl_state == REPL_STATE_RECEIVE_CAPA) {
        err = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL);

        if (err[0] == '-') {
            serverLog(LL_NOTICE,"(Non critical) Master does not understand REPLCONF capa: %s", err);
        }
        sdsfree(err);

        // 向主节点发送 psync 命令, 请求全量复制
        server.repl_state = REPL_STATE_SEND_PSYNC;
    }
    // 省略
}
```

到了这一步, 相关的配置发送基本完成了, 下一步就是真正的同步复制的开始了

## 2.8 发送 PSYNC 命令

从节点收到了主节点对其同步能力的响应后, 这是会发送一个 psync 的命令给主节点, 这个请求就是同步复制的真正开始了

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    if (server.repl_state == REPL_STATE_SEND_PSYNC) {

        // 入参的 0 表示写消息给主节点, 1 表示从主节点读取数据
        // 入参 0 的逻辑, 根据当前是否缓存了主节点, 既 cached_master 是否为空, 来发送 psync 命令, 为空, 发送全量同步请求, 不为空, 发送部分同步请求
        if (slaveTryPartialResynchronization(fd,0) == PSYNC_WRITE_ERROR) {
            err = sdsnew("Write error sending the PSYNC command.");
            goto write_error;
        }

        // 切换状态为 REPL_STATE_RECEIVE_PSYNC (等待 psync 应答)
        server.repl_state = REPL_STATE_RECEIVE_PSYNC;
        return;
    }
}
```

**主节点**收到从节点的发送过来的同步请求命令 psync, 执行的逻辑如下

```C
void syncCommand(client *c) {

    // 不是从节点, 直接返回
    if (c->flags & CLIENT_SLAVE) 
        return;

    // 当前节点是另一个节点的从节点, 同时节点的状态不是 REPL_STATE_CONNECTED (已经连接状态), 直接返回
    if (server.masterhost && server.repl_state != REPL_STATE_CONNECTED) {
        addReplySds(c,sdsnew("-NOMASTERLINK Can't SYNC while not connected with my master\r\n"));
        return;
    }

    // 判断 client c 的 bufpos != 0 || reply 有数据
    // 也就是判断当前节点有数据准备发送给从节点, 是的话, 直接返回
    if (clientHasPendingReplies(c)) {
        addReplyError(c,"SYNC and PSYNC are invalid with pending output");
        return;
    }
    // 打日志
    serverLog(LL_NOTICE,"Replica %s asks for synchronization",replicationGetSlaveName(c));

    // 执行的命令为 psync
    if (!strcasecmp(c->argv[0]->ptr,"psync")) {

        // 主节点尝试进行部分同步复制, 成功了 stat_sync_partial_ok 部分同步成功次数 + 1
        if (masterTryPartialResynchronization(c) == C_OK) {
            server.stat_sync_partial_ok++;
            return;
        }

        char *master_replid = c->argv[1]->ptr;
        
        // 从节点指定了 run_id, 但是现在部分同步失败了
        if (master_replid[0] != '?') 
            // 部分同步复制失败次数 + 1
            server.stat_sync_partial_err++;

    } else {
        // 执行的命令不是 psync, 也就是 sync 命令
        c->flags |= CLIENT_PRE_PSYNC;
    }

    // 全量复制

    // 全量复制次数 + 1
    server.stat_sync_full++;
    // 修改从节点的状态为等待 bgsave 的开始
    c->replstate = SLAVE_STATE_WAIT_BGSAVE_START;

    // 关闭了 TCP_NODELAY 功能
    if (server.repl_disable_tcp_nodelay)
        // 启用 nagle 算法
        anetDisableTcpNoDelay(NULL, c->fd);

    c->repldbfd = -1;
    c->flags |= CLIENT_SLAVE;
    listAddNodeTail(server.slaves,c);   

    // 如果有需要创建复制积压缓冲区
    // 从节点只有 1 个, 复制积压缓冲区为空
    if (listLength(server.slaves) == 1 && server.repl_backlog == NULL) {

        // 生成 replid, 即 run_id
        changeReplicationId();
        // 清除 replid2 
        clearReplicationId2();
        // 创建复制积压缓冲区
        createReplicationBacklog();
    } 

    if (server.rdb_child_pid != -1 && server.rdb_child_type == RDB_CHILD_TYPE_DISK) {
        // 正在执行 RDB, 同时类型是写入磁盘, 也就是普通的 RDB 

        client *slave;
        listNode *ln;
        listIter li;

        listRewind(server.slaves,&li);

        // 遍历所有的从节点, 找到第一个节点的状态为 SLAVE_STATE_WAIT_BGSAVE_END (等待 bgsave 的结束, 即等待 RDB 文件的创建结束)
        while((ln = listNext(&li))) {
            slave = ln->value;
            if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_END) 
                break;
        }

        // 有找到对应的节点, 当前客户端的节点的复制能力和找到的节点的复制能力一样
        if (ln && ((c->slave_capa & slave->slave_capa) == slave->slave_capa)) {
            
            // 把找到的节点的输出缓存复制到当前的客户端
            // 将 slave 的 buf 拷贝到 c 的 buf
            // 将 slave 的 reply_bytes 拷贝到 c 的 reply_bytes
            // 将 slave 的 bufpos 设置等于 c 的 bufpos
            copyClientOutputBuffer(c,slave);

            // 更新从节点客户端的偏移量, 状态和发送全量复制消息给从节点
            // 这个命令会发送 FULLRESYNC master_run_id offset\r\n 的响应给从节点, 可以看做是对 psync 命令的响应
            replicationSetupSlaveForFullResync(c,slave->psync_initial_offset);
            serverLog(LL_NOTICE,"Waiting for end of BGSAVE for SYNC");

        } else {
            // 找不到 或者找到的节点和当前的客户端的能力不一样, 只能等待下次的 bgsave 
            serverLog(LL_NOTICE,"Can't attach the replica to the current BGSAVE. Waiting for next BGSAVE for SYNC");
        }

    } else if (server.rdb_child_pid != -1 && server.rdb_child_type == RDB_CHILD_TYPE_SOCKET) {

        // 正在执行 RDB, 同时类型是写入 Socket, 也就是复制同步的 RDB
        // 提示等待下次同步
        serverLog(LL_NOTICE,"Current BGSAVE has socket target. Waiting for next BGSAVE for SYNC");

    } else { 

        // 没有在执行 RDB 
        // 复制类型为无盘同步 同时 当前的客户端执行 EOF 的同步方式, 也就是 RDB 文件流

        if (server.repl_diskless_sync && (c->slave_capa & SLAVE_CAPA_EOF)) {

            // 支持延迟无盘同步, 打印日志后结束
            // 后续在定时器执行的 replicationCron 函数时, 会创建出子进程进行同步
            // 延迟一段时间, 可以等待几个从节点, 后面同步处理

            if (server.repl_diskless_sync_delay)
                serverLog(LL_NOTICE,"Delay next BGSAVE for diskless SYNC");
        } else {

            // 没有子进程正在执行 BGSAVE, 且没有进行写 AOF 文件, 则开始为复制执行 BGSAVE, 并且是将 RDB 文件写到磁盘上
            if (server.aof_child_pid == -1) {
                // 内部和 RDB 的操作类型, 分为主子进程, 子进程进行 RDB 的生成, fork 出子进程后,
                // 主进程也会通过 replicationSetupSlaveForFullResync 函数进行 psync 的应答
                startBgsaveForReplication(c->slave_capa);
            } else {
                // 延迟执行 
                serverLog(LL_NOTICE, "No BGSAVE in progress, but an AOF rewrite is active. BGSAVE for replication delayed");
            }
        }
    }
    return;
}
```

上面就是主节点收到从节点的 **psync** 命令后的执行步骤, 很长, 整理如下
> 1. 如果可以部分复制, 进行部分复制同步
> 2. 无法部分部分复制, 进行全量复制同步
> 3. 全量复制同步时, 如果从节点支持无盘复制的话, 直接将数据写入到对应的 socket 中
> 4. 全量复制同步时, 如果从节点不支持无盘复制的话, 则和普通的 RDB 一样, 生成 RDB 文件, 然后发送过去
> 5. 整个全量复制的同步都是在子进程进行的, 主进程 fork 出子进程后, 会对 psync 命令进行响应

**从节点**收到主节点的响应后, 从节点同样是事件轮询触发 **syncWithMaster** 函数。

在从节点发送出 **psync** 命令后, 状态为 **REPL_STATE_RECEIVE_PSYNC**, 继续从这个状态走下去

```C
void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) {

    // 省略

    // 状态不是 REPL_STATE_RECEIVE_PSYNC 直接失败
    if (server.repl_state != REPL_STATE_RECEIVE_PSYNC) {
        serverLog(LL_WARNING,"syncWithMaster(): state machine error, state should be RECEIVE_PSYNC but is %d", server.repl_state);
        goto error;
    }

    // 读取主节点的响应, 如果是全量复制, 响应为 PSYNC_FULLRESYNC
    psync_result = slaveTryPartialResynchronization(fd,1);

    // 等待重新执行
    if (psync_result == PSYNC_WAIT_REPLY) 
        return; 

    // 主节点正处于暂时性错误状态
    if (psync_result == PSYNC_TRY_LATER) 
        goto error;

    // psync 命令主节点直接完成, 从节点可以进行执行
    if (psync_result == PSYNC_CONTINUE) {
        serverLog(LL_NOTICE, "MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.");
        return;
    }

    // 释放所有的从节点连接
    disconnectSlaves();
    // 释放复制积压缓冲区
    freeReplicationBacklog();

    // 主节点无法识别 psync, 需要尝试执行 sync 
    if (psync_result == PSYNC_NOT_SUPPORTED) {

        // 不支持 psync, 则发送 sync
        serverLog(LL_NOTICE,"Retrying with SYNC...");

        if (syncWrite(fd,"SYNC\r\n",6,server.repl_syncio_timeout*1000) == -1) {
            serverLog(LL_WARNING,"I/O error writing to MASTER: %s", strerror(errno));
            goto error;
        }
    }

    char tmpfile[256], *err = NULL;
    int dfd = -1, maxtries = 5;

    /* Prepare a suitable temp file for bulk transfer */

    // 尝试创建一个临时文件, 失败的话, 进行重试, 可以重试 5 次
    while(maxtries--) {
        // 创建一个临时文件
        snprintf(tmpfile,256, "temp-%d.%ld.rdb",(int)server.unixtime,(long int)getpid());
        dfd = open(tmpfile,O_CREAT|O_WRONLY|O_EXCL,0644);
        if (dfd != -1) break;
        sleep(1);
    }

    // 创建文件失败
    if (dfd == -1) {
        serverLog(LL_WARNING,"Opening the temp file needed for MASTER <-> REPLICA synchronization: %s",strerror(errno));
        goto error;
    }

    // 为创建的文件描述符 添加可读事件, 用来下载监听到的数据, 也就是父级发送过来的 RDB, 处理函数为 readSyncBulkPayload
    if (aeCreateFileEvent(server.el,fd, AE_READABLE,readSyncBulkPayload,NULL) == AE_ERR) {
        serverLog(LL_WARNING,
            "Can't create readable event for SYNC: %s (fd=%d)",
            strerror(errno),fd);
        goto error;
    }

    // 修改状态为 REPL_STATE_TRANSFER (正在接收 RDB 文件)
    server.repl_state = REPL_STATE_TRANSFER;

    // 更新复制相关变量的值
    server.repl_transfer_size = -1;
    server.repl_transfer_read = 0;
    server.repl_transfer_last_fsync_off = 0;
    server.repl_transfer_fd = dfd;
    server.repl_transfer_lastio = server.unixtime;
    server.repl_transfer_tmpfile = zstrdup(tmpfile);
    return;
}
```

上面的步骤基本就是主从节点为数据同步做的前期准备, 主从节点只是做好了发送同步数据的准备, 实际此时还是没有数据的复制的。  
前期的准备完成后, 主节点会尝试发送数据到从节点, 触发发送的链路如下:
> 1. 定时函数 serverCron, 判断出当前的 RDB 子进程不为空, 触发 backgroundSaveDoneHandler 函数
> 2. backgroundSaveDoneHandler 中根据当前的子进程 RDB 类型, Socket 走 backgroundSaveDoneHandlerSocket 函数, 无盘文件的走 backgroundSaveDoneHandlerDisk
> 3. 走 backgroundSaveDoneHandlerSocket 函数时, 更新各种复制相关的信息, 然后走到 updateSlavesWaitingBgsave 函数
> 4. 走 backgroundSaveDoneHandlerDisk 函数时, 先更新各种复制相关的信息, 然后走到 updateSlavesWaitingBgsave 函数
> 5. 走 updateSlavesWaitingBgsave 函数时, 遍历所有的从节点, 同时根据入参的 type 是磁盘同步还是 Socket 同步, 做处理
> 6. 如果是 Socket 同步, 更新各种信息, 更新从节点的状态为 SLAVE_STATE_ONLINE, 然后等待收到从节点的 ack 响应
> 7. 如果是磁盘同步, 更新各种信息, 打开临时文件, 注册一个可读的事件, 执行的函数 sendBulkToSlave, 从节点的状态切换为 SLAVE_STATE_SEND_BULK
> 8. 触发了 sendBulkToSlave 函数

```C

void sendBulkToSlave(aeEventLoop *el, int fd, void *privdata, int mask) {
    client *slave = privdata;
    UNUSED(el);
    UNUSED(mask);
    char buf[PROTO_IOBUF_LEN];
    ssize_t nwritten, buflen;

    /* Before sending the RDB file, we send the preamble as configured by the
     * replication process. Currently the preamble is just the bulk count of
     * the file in the form "$<length>\r\n". */

    // 发送序言, 这里只是简单发送出了文件的大小, 格式 $<length>\r\n"
    if (slave->replpreamble) {

        nwritten = write(fd,slave->replpreamble,sdslen(slave->replpreamble));
        if (nwritten == -1) {
            serverLog(LL_VERBOSE,"Write error sending RDB preamble to replica: %s", strerror(errno));
            freeClient(slave);
            return;
        }

        // 更新已经写到网络的字节数
        server.stat_net_output_bytes += nwritten;
        // 保留未写的字节, 删除已写的字节
        sdsrange(slave->replpreamble,nwritten,-1);

        // 如果已经写完了, 则释放replpreamble
        if (sdslen(slave->replpreamble) == 0) {
            sdsfree(slave->replpreamble);
            slave->replpreamble = NULL;
        } else {
            return;
        }
    }

    // 将文件指针移动到刚才发送 replpreamble 的下一个字节, 准备写操作
    lseek(slave->repldbfd,slave->repldboff,SEEK_SET);

    // 将 repldbfd 读出 RDB 文件中的内容保存在 buf 中
    buflen = read(slave->repldbfd,buf,PROTO_IOBUF_LEN);
 
    if (buflen <= 0) {
        serverLog(LL_WARNING,"Read error sending DB to replica: %s", (buflen == 0) ? "premature EOF" : strerror(errno));
        freeClient(slave);
        return;
    }

    // 将保存 RDB 文件数据的 buf 写到从节点中
    if ((nwritten = write(fd,buf,buflen)) == -1) {
        if (errno != EAGAIN) {
            serverLog(LL_WARNING,"Write error sending DB to replica: %s", strerror(errno));
            freeClient(slave);
        }
        return;
    }
    // 更新从节点读取主服务器传来的 RDB 文件的字节数
    slave->repldboff += nwritten;
    // 更新服务器已经写到网络的字节数
    server.stat_net_output_bytes += nwritten;

    // 如果写入完成, 即从网络读到的大小等于文件大小
    if (slave->repldboff == slave->repldbsize) {
        close(slave->repldbfd);
        slave->repldbfd = -1;
        // 删除等待从节点的文件可读事件
        aeDeleteFileEvent(server.el,slave->fd,AE_WRITABLE);
        // 将从节点置于在线状态
        putSlaveOnline(slave);
    }
}

```

**主节点**发送完数据后, 从节点的接收

```C
void readSyncBulkPayload(aeEventLoop *el, int fd, void *privdata, int mask) {

    char buf[4096];
    ssize_t nread, readlen, nwritten;
    off_t left;
    UNUSED(el);
    UNUSED(privdata);
    UNUSED(mask);

    // CONFIG_RUN_ID_SIZE = 40
    static char eofmark[CONFIG_RUN_ID_SIZE];
    static char lastbytes[CONFIG_RUN_ID_SIZE];
    static int usemark = 0;

     // 即使 repl_transfer_size==-1, 我们仍然从主节点读取一个长度的回复, 该长度是 RDB 文件的大小
    if (server.repl_transfer_size == -1) {

        // 读取 1024 字节的数据到 buf 
        if (syncReadLine(fd,buf,1024,server.repl_syncio_timeout*1000) == -1) {
            serverLog(LL_WARNING, "I/O error reading bulk count from MASTER: %s", strerror(errno));
            goto error;
        }

        // // 读出的数据出错
        if (buf[0] == '-') {
            serverLog(LL_WARNING, "MASTER aborted replication with an error: %s", buf+1);
            goto error;
        } else if (buf[0] == '\0') {

            // 只是读出到一个 '\0', 作用和 PING 相同的字符, 是为了测试连接是否中断
            // 所以更新最近交互的时间戳, 直接返回
            server.repl_transfer_lastio = server.unixtime;
            return;
        } else if (buf[0] != '$') {

            // 读出的长度按照协议格式是: $<length>\r\n, 所以第一个字符不是'$' 就出错
            serverLog(LL_WARNING,"Bad protocol from MASTER, the first byte is not '$' (we received '%s'), are you sure the host and port are right?", buf);
            goto error;
        }

        // 无盘传输, 也就是 Socket 传输
        if (strncmp(buf+1,"EOF:",4) == 0 && strlen(buf+5) >= CONFIG_RUN_ID_SIZE) {

            usemark = 1;
            
            // 将 40 字节长的分隔符保存到 eofmark 静态数组中
            memcpy(eofmark,buf+5,CONFIG_RUN_ID_SIZE);

            // 将保存从服务器接收的最后 40 个字节的数组初始化为 0
            memset(lastbytes,0,CONFIG_RUN_ID_SIZE);

            // 同步期间从主节点读到的 RDB 的大小设置为 0, 因为是无盘传输的方式
            server.repl_transfer_size = 0;
            serverLog(LL_NOTICE, "MASTER <-> SLAVE sync: receiving streamed RDB from master");

        } else {
            // 从 RDB 文件读, 读出 RDB 文件长度
            
            usemark = 0;
            // 从主节点读到的 RDB 的大小
            server.repl_transfer_size = strtol(buf+1,NULL,10);
            serverLog(LL_NOTICE, "MASTER <-> SLAVE sync: receiving %lld bytes from master", (long long) server.repl_transfer_size);
        }
        return;
    }

    // 读数据

    // usemark在 无盘传输时会被设置为 1
    if (usemark) {
        // 计算数据大小, 也就是读的长度
        readlen = sizeof(buf);
    
    } else {
        // 从 RDB 文件读
        // 计算读的长度
        left = server.repl_transfer_size - server.repl_transfer_read;
        readlen = (left < (signed)sizeof(buf)) ? left : (signed)sizeof(buf);
    }

    // 从 fd 文件描述符中中读取数据到 buf
    nread = read(fd,buf,readlen);

    if (nread <= 0) {
        serverLog(LL_WARNING,"I/O error trying to sync with MASTER: %s", (nread == -1) ? strerror(errno) : "connection lost");
        cancelReplicationHandshake();
        return;
    }

    // 更新从网络读的字节数
    server.stat_net_input_bytes += nread;

    // EOF 标志是否到达
    int eof_reached = 0;
    
    // 无盘同步
    if (usemark) {
        
        // 将所读的 nread 长度的后 40 字节拷贝到 lastbytes 数组中
        if (nread >= CONFIG_RUN_ID_SIZE) {
            memcpy(lastbytes,buf+nread-CONFIG_RUN_ID_SIZE,CONFIG_RUN_ID_SIZE);
        } else {
            // 读到的不足 40 字节, 那就补足 40 字节
            int rem = CONFIG_RUN_ID_SIZE-nread;
            memmove(lastbytes,lastbytes+nread,rem);
            memcpy(lastbytes+rem,buf,nread);
        }

        // 比较是否匹配, 如果相同表示传输结束, 到达 EOF 结尾
        if (memcmp(lastbytes,eofmark,CONFIG_RUN_ID_SIZE) == 0) 
            eof_reached = 1;
    }

    // 更新最近一次读到 RDB 文件内容的时间
    server.repl_transfer_lastio = server.unixtime;

    // 将 buf 中的数据写到临时 RDB 文件中, 只是写到系统内核的缓冲区中, 等待 fsync
    if ((nwritten = write(server.repl_transfer_fd,buf,nread)) != nread) {
        serverLog(LL_WARNING,"Write error or short write writing to the DB dump file needed for MASTER <-> REPLICA synchronization: %s", 
            (nwritten == -1) ? strerror(errno) : "short write");
        goto error;
    }

    // 更新同步期间从主节点读到的 RDB 文件的总量
    server.repl_transfer_read += nread;

    // 如果是无盘同步, 且到达了 EOF
    if (usemark && eof_reached) {

        // 删除后40字节
        if (ftruncate(server.repl_transfer_fd, server.repl_transfer_read - CONFIG_RUN_ID_SIZE) == -1) {
            serverLog(LL_WARNING,"Error truncating the RDB file received from the master for SYNC: %s", strerror(errno));
            goto error;
        }
    }

    // 定期将文件写入磁盘, 避免阻塞在 IO 上
    // Redis 同步缓冲区到磁盘上, 并不是每次写到同步, 而是当写够 8M 大小, 调用 sync_file_range 函数, 一次性的冲刷数据, 这样大大提高 IO 的性能
    if (server.repl_transfer_read >= server.repl_transfer_last_fsync_off + REPL_MAX_WRITTEN_BEFORE_FSYNC) {
        off_t sync_size = server.repl_transfer_read - server.repl_transfer_last_fsync_off;
        rdb_fsync_range(server.repl_transfer_fd, server.repl_transfer_last_fsync_off, sync_size);
        server.repl_transfer_last_fsync_off += sync_size;
    }

    if (!usemark) {
        if (server.repl_transfer_read == server.repl_transfer_size)
            eof_reached = 1;
    }

    // 已经完成 
    if (eof_reached) {

        int aof_is_enabled = server.aof_state != AOF_OFF;

        // 当前有子进程在进行 RDB 
        if (server.rdb_child_pid != -1) {
            serverLog(LL_NOTICE, "Replica is about to load the RDB file received from the master, but there is a pending RDB child running. "
                "Killing process %ld and removing its temp file to avoid any race", (long) server.rdb_child_pid);
            kill(server.rdb_child_pid,SIGUSR1);
            // 删除 RDB 文件
            rdbRemoveTempFile(server.rdb_child_pid);
        }

        // 在执行一次 fsync 
        if (fsync(server.repl_transfer_fd) == -1) {
            serverLog(LL_WARNING, "Failed trying to sync the temp DB to disk in MASTER <-> REPLICA synchronization: %s", strerror(errno));
            cancelReplicationHandshake();
            return;
        }

        // 重命名临时的 RDB 文件为系统配置的 RDB 文件名
        if (rename(server.repl_transfer_tmpfile,server.rdb_filename) == -1) {
            serverLog(LL_WARNING,"Failed trying to rename the temp DB into dump.rdb in MASTER <-> REPLICA synchronization: %s", strerror(errno));
            cancelReplicationHandshake();
            return;
        }

        serverLog(LL_NOTICE, "MASTER <-> REPLICA sync: Flushing old data");

        /* We need to stop any AOFRW fork before flusing and parsing
         * RDB, otherwise we'll create a copy-on-write disaster. */

        // 在解析 RDB 文件和刷新数据之前, 需要停止任何的 AOF 功能
        // 停止 AOF 功能 
        if(aof_is_enabled) 
            stopAppendOnly();

        // 将旧数据库清空
        signalFlushedDb(-1);
        // repl_slave_lazy_flush 在加载数据之前需要延迟 flushall 
        // 调用 replicationEmptyDbCallback 清空所有数据库
        emptyDb(-1, server.repl_slave_lazy_flush ? EMPTYDB_ASYNC : EMPTYDB_NO_FLAGS, replicationEmptyDbCallback);

        // 先删除之前的可读事件, 因为载入 RDB 会设置监听读事件
        aeDeleteFileEvent(server.el,server.repl_transfer_s,AE_READABLE);

        serverLog(LL_NOTICE, "MASTER <-> REPLICA sync: Loading DB in memory");
        rdbSaveInfo rsi = RDB_SAVE_INFO_INIT;

        // 从 RDB 文件中加载数据到数据库中
        if (rdbLoad(server.rdb_filename,&rsi) != C_OK) {
            serverLog(LL_WARNING,"Failed trying to load the MASTER synchronization DB from disk");
            cancelReplicationHandshake();

            // 重新开启 AOF 功能
            if (aof_is_enabled) 
                restartAOFAfterSYNC();
            return;
        }

        // 释放临时文件和临时文件的 fd
        zfree(server.repl_transfer_tmpfile);
        close(server.repl_transfer_fd);
        // 为当前的节点设置一个主客户端
        replicationCreateMasterClient(server.repl_transfer_s,rsi.repl_stream_db);
        // 状态修改为 已经连接状态
        server.repl_state = REPL_STATE_CONNECTED;
        server.repl_down_since = 0;

        // 将 master 的 replid 也就是 run_id 拷贝到 server.replid
        memcpy(server.replid,server.master->replid,sizeof(server.replid));
        // 当前复制偏移量 = master 的复制偏移量
        server.master_repl_offset = server.master->reploff;
        // 清除 replid2 的值
        clearReplicationId2();

        // 没有缓存区, 创建缓存区
        if (server.repl_backlog == NULL) 
            createReplicationBacklog();

        serverLog(LL_NOTICE, "MASTER <-> REPLICA sync: Finished with success");

        // 重新开启 AOF 功能
        if (aof_is_enabled) 
            restartAOFAfterSYNC();    
    }

    return;

error:
    cancelReplicationHandshake();
    return;
}
```

## 2.8 发送输出缓冲区数据

在上面的发送 psync 命令中, 主节点在 **sendBulkToSlave** 函数中, 发送完成 RDB 文件后, 会调用 putSlaveOnline 函数, 函数内部会
> 1. 将对应的从节点的客户端状态设置为 SLAVE_STATE_ONLINE (在线状态)
> 2. 创建一个可写事件, 执行的函数为 sendReplyToClient

创建完可写事件, 会立即触发一次, 然后在事件轮询中, 会接收到对应的通知, 执行 sendReplyToClient 函数

```C
void sendReplyToClient(aeEventLoop *el, int fd, void *privdata, int mask) {
    UNUSED(el);
    UNUSED(mask);
    writeToClient(fd,privdata,1);
}

// 将输出缓冲区的数据发送给对应的 client
int writeToClient(int fd, client *c, int handler_installed) {
    ssize_t nwritten = 0, totwritten = 0;
    size_t objlen;
    clientReplyBlock *o;

    // 当前客户端有数据要发送, 判断入参的 client 的 bufpos 大于 0 或者 reply 中有数据
    while(clientHasPendingReplies(c)) {
        
        // bufpos 大于 0
        // 也就是固定缓存区有数据未写出
        if (c->bufpos > 0) {
            
            // write(fd, buf, count) 把 buf 指向的内存地址中的数据写入 count 个字节到 fd, 返回值我写入的字节数
            // buf 可以看做缓存区的起始位置 bufpos 缓存区的结束位置, setlen 已经发送的字节数
            nwritten = write(fd,c->buf+c->sentlen,c->bufpos-c->sentlen);

            // 写入失败
            if (nwritten <= 0) 
                break;
            // 更新已经发送的字节数    
            c->sentlen += nwritten;
            // 更新当前已经发送的字节数
            totwritten += nwritten;

            // 当前已经发送的字节数等于结束位置, 也就是没有缓存区数据, 进行重置
            if ((int)c->sentlen == c->bufpos) {
                c->bufpos = 0;
                c->sentlen = 0;
            }

        } else {

            // 获取回复链表的头节点
            o = listNodeValue(listFirst(c->reply));

            // 回复对象的长度
            objlen = o->used;

            // 为 0, 表示没有数据, 跳过
            if (objlen == 0) {
                c->reply_bytes -= o->size;
                listDelNode(c->reply,listFirst(c->reply));
                continue;
            }

            // 发送数据到客户端
            nwritten = write(fd, o->buf + c->sentlen, objlen - c->sentlen);

            if (nwritten <= 0) 
                break;
            // 更新客户端里面已经发送数据 + nwritten    
            c->sentlen += nwritten;
            
            totwritten += nwritten;

            // 发送出去的数据和当前对象的数据一样, 当前对象发送完成
            if (c->sentlen == objlen) {
                // 需要应答的数据 - 当前对象的数据大小
                c->reply_bytes -= o->size;
                // 删除头对象
                listDelNode(c->reply,listFirst(c->reply));
                // 已经发送的数据变为 0 
                c->sentlen = 0;
                
                // 恢复链表为空了, 需要回复的数据长度需要等于 0  
                if (listLength(c->reply) == 0)
                    serverAssert(c->reply_bytes == 0);
            }
        }

        // NET_MAX_WRITES_PER_EVENT = 1024*64
        // 如果这次写的总量大于 NET_MAX_WRITES_PER_EVENT 的限制, 则会中断本次的写操作, 将处理时间让给其他的 client, 以免一个非常大的回复独占服务器, 剩余的数据下次继续在写
        // 但是, 如果当服务器的内存数已经超过 maxmemory, 即使超过最大写 NET_MAX_WRITES_PER_EVENT 的限制, 也会继续执行写入操作, 是为了尽快写入给客户端
        if (totwritten > NET_MAX_WRITES_PER_EVENT && (server.maxmemory == 0 ||zmalloc_used_memory() < server.maxmemory) && !(c->flags & CLIENT_SLAVE)) 
            break;
    }

     // 更新写到网络的字节数
    server.stat_net_output_bytes += totwritten;

    //  处理写入失败
    if (nwritten == -1) {
        if (errno == EAGAIN) {
            nwritten = 0;
        } else {
            serverLog(LL_VERBOSE, "Error writing to client: %s", strerror(errno));
            freeClient(c);
            return C_ERR;
        }
    }

    // 发送给客户端的数据大于 0 了, 也就是有数据发送成功
    if (totwritten > 0) {

        // 客户端不算主节点, 更新最近和服务器交互的时间
        if (!(c->flags & CLIENT_MASTER)) 
            c->lastinteraction = server.unixtime;
    }

    //  当前客户端没有数据要发送
    if (!clientHasPendingReplies(c)) {
        // 已经发送的数据长度为 0 
        c->sentlen = 0;
        // 需要删除写事件, 进行删除
        if (handler_installed) 
            aeDeleteFileEvent(server.el,c->fd,AE_WRITABLE);

        // 客户端的标识为 在响应后需要关闭的话, 释放客户端
        if (c->flags & CLIENT_CLOSE_AFTER_REPLY) {
            freeClient(c);
            return C_ERR;
        }
    }

    return C_OK;
}
```

## 2.9 命令传播

经过 psync 命令后, 也就是第一次全量同步后, 主从节点之间数据都同步了, 但是后续如果主节点继续数据的变更, 又会不一致。为了数据的一致, 主节点应该有种方式将自身的变更同步到  
从节点, 这个实现的步骤就是命令传播。

在执行 Redis 命令的函数 call 中, 里面会根据执行的命令和客户端的类型等元素, 判断是否需要执行 propagate 函数, propagate 函数就是命令传播的方法。  propagate 函数很简单
根据入参的标识判断是否需要进行 AOF 相关需要的命令的传播, 然后根据标识判断是否需要复制相关需要的传播, 如果判断复制相关的传播, 会执行 replicationFeedSlaves 方法

replicationFeedSlaves 方法的执行逻辑如下:

```C
void replicationFeedSlaves(list *slaves, int dictid, robj **argv, int argc) {
    listNode *ln;
    listIter li;
    int j, len;
    char llstr[LONG_STR_SIZE];

    // 有配置主节点, 直接返回
    if (server.masterhost != NULL) 
        return;

    // 没有复制积压缓冲区 backlog 且没有从节点服务器, 直接返回
    if (server.repl_backlog == NULL && listLength(slaves) == 0) 
        return;    

    serverAssert(!(listLength(slaves) != 0 && server.repl_backlog == NULL));

    if (server.slaveseldb != dictid) {
        // 如果当前从节点使用的数据库不是目标的数据库, 则要生成一个 select 命令
        robj *selectcmd;

        // // 0 <= id < 10, 可以使用共享的 select 命令对象
        if (dictid >= 0 && dictid < PROTO_SHARED_SELECT_CMDS) {
            selectcmd = shared.select[dictid];
        } else {
            // 按照协议格式构建 select 命令对象
            int dictid_len;
            dictid_len = ll2string(llstr,sizeof(llstr),dictid);
            selectcmd = createObject(OBJ_STRING, sdscatprintf(sdsempty(), "*2\r\n$6\r\nSELECT\r\n$%d\r\n%s\r\n", dictid_len, llstr));
        }

        // 把当前的 select 命令写入到复制积压缓冲区
        if (server.repl_backlog) 
            feedReplicationBacklogWithObject(selectcmd);

        listRewind(slaves,&li);

        // 遍历所有的从服务器节点
        while((ln = listNext(&li))) {
            client *slave = ln->value;
            // 从节点服务器状态为等待 bgsave 的开始, 因此跳过回复, 遍历下一个节点
            if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START) 
                continue;
            // 添加 select 命令到当前从节点的回复中    
            addReply(slave,selectcmd);
        }

        // 释放临时对象
        if (dictid < 0 || dictid >= PROTO_SHARED_SELECT_CMDS)
            decrRefCount(selectcmd);
    }

    // 设置当前从节点使用的数据库 ID
    server.slaveseldb = dictid;

       // 将命令写到 backlog 中
    if (server.repl_backlog) {

        char aux[LONG_STR_SIZE+3];

        // 拼写命令 *<argc>\r\n
        aux[0] = '*';
        len = ll2string(aux+1,sizeof(aux)-1,argc);
        aux[len+1] = '\r';
        aux[len+2] = '\n';
        // 写入复制积压缓冲区
        feedReplicationBacklog(aux,len+3);

        // 遍历所有的参数个数
        for (j = 0; j < argc; j++) {

            // 参数对象的长度
            long objlen = stringObjectLen(argv[j]);
            
            // 构建成协议标准的字符串, 并添加到 backlog 中
            // $<len>\r\n<argv>\r\n 
            aux[0] = '$';
            len = ll2string(aux+1,sizeof(aux)-1,objlen);
            aux[len+1] = '\r';
            aux[len+2] = '\n';

            // 添加 $<len>\r\n
            feedReplicationBacklog(aux,len+3);
            // 添加参数对象 <argv>
            feedReplicationBacklogWithObject(argv[j]);
            // 添加\r\n
            feedReplicationBacklog(aux+len+1,2);
        }
    }

    listRewind(slaves,&li);
    while((ln = listNext(&li))) {

        client *slave = ln->value;

        // 从节点的状态为等待 bgsave 开始跳过
        if (slave->replstate == SLAVE_STATE_WAIT_BGSAVE_START) 
            continue;

        // 将命令写给正在等待初次 SYNC 的从节点 (所以这些命令在输出缓冲区中排队, 直到初始 SYNC 完成), 或已经与主节点同步

        // 添加回复的长度
        addReplyMultiBulkLen(slave,argc);

        // 将所有的参数列表添加到从节点的输出缓冲区
        for (j = 0; j < argc; j++)
            addReplyBulk(slave,argv[j]);
    }
}
```

和 AOF 持久化一样, 再给从节点client写命令时, 会将SELECT命令强制写入, 以保证命令正确读到数据库中。
不仅写入了从节点 client 的输出缓冲区, 而且还会将命令记录到主节点服务器的复制积压缓冲区 server.repl_backlog 中, 这是为了网络闪断后进行部分重同步。

## 3.10 部分重同步实现

上面是全量同步的过程, 如果在传输 RDB 文件的过程中, 网络发生故障, 主节点和从节点的连接中断, Redis 会咋么做呢？ 
Redis 2.8 版本之前会在进行一次连接然后进行全量复制, 但是这样效率非常地下, 之后的版本都提供了部分重同步的实现。

部分重同步在复制的过程中, 相当于标题 2.8 的发送 PSYNC 命令的部分, 其他所有的部分都要进行, 他只是主节点回复从节点的命令不同, 
回复 +CONTINUE 则执行部分重同步, 回复 +FULLRESYNC 则执行全量同步。

### 3.10.1 心跳机制

在主从节点建立连接后, 他们之间都维护者长连接并彼此发送心跳命令。主从节点彼此都有心跳机制, 各自模拟成对方的客户端进行通信。

主节点默认每隔10秒发送PING命令, 判断从节点的连接状态, 可以通过 **repl-ping-salve-period** 进行时间的配置, 默认为 10 秒

```C

// 如果当前节点是某以节点的主节点, 那么发送 PING 给从节点
if ((replication_cron_loops % server.repl_ping_slave_period) == 0) {
    // 创建 PING 命令对象
    ping_argv[0] = createStringObject("PING",4);
    // 将 PING 发送给从服务器
    replicationFeedSlaves(server.slaves, server.slaveseldb, ping_argv, 1);
    decrRefCount(ping_argv[0]);
}

```

从节点在主线程中每隔 1 秒发送 **REPLCONF ACK <offset>** 命令, 给主节点报告自己当前复制偏移量

```C
// 定期发送ack给主节点, 旧版本的Redis除外
if (server.masterhost && server.master && !(server.master->flags & CLIENT_PRE_PSYNC))
    // 发送一个 REPLCONF ACK 命令给主节点去报告关于当前处理的 offset。
    replicationSendAck();

```

在周期性函数 replicationCron(), 每次都要检查和主节点处于连接状态的从节点和主节点的交互时间是否超时, 
如果超时则会调用 cancelReplicationHandshake() 函数, 取消和主节点的连接。等到下一个周期在和主节点重新建立连接, 进行复制。


### 3.10.2 复制积压缓冲区 (backlog)

复制积压缓冲区是一个大小为 1M 的循环队列。主节点在命令传播时, 不仅会将命令发送给所有的从节点, 还会将命令写入复制积压缓冲区中 (具体请看标题2.10) 。

复制积压缓冲区最多可以备份 1M 大小的数据, 如果主从节点断线时间过长, 复制积压缓冲区的数据会被新数据覆盖, 那么当从主从中断连接起, 主节点接收到的数据超过 1M 
大小, 那么从节点就无法进行部分重同步, 只能进行全量复制

在标题2.8, 介绍的 syncCommand() 命令中, 调用 masterTryPartialResynchronization() 函数会进行尝试部分重同步, 在我们之前分析的第一次全量同步时, 该函数会执行失败, 
然后返回 syncCommand() 函数执行全量同步, 而在进行恢复主从连接后, 则会进行部分重同步, masterTryPartialResynchronization() 函数代码如下:

```C
int masterTryPartialResynchronization(client *c) {



}
```

如果可以进行部分重同步, 主节点则会发送 "+CONTINUE\r\n" 作为从节点发送PSYNC回复 (看标题2.8) 。  
然后调用 addReplyReplicationBacklog() 函数, 将 backlog 中的数据发送给从节点。于是就完成了部分重同步。

addReplyReplicationBacklog() 函数所做的就是将 backlog 写到从节点的 client 的输出缓冲区中。

## 3.11 参考

[Redis 复制(replicate)实现](https://blog.csdn.net/men_wen/article/details/72628439)