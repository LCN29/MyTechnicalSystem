# 3 Redis - 集群 (Cluster)

## 3.1 数据分片的实现方式

> 1. 在客户端实现相关分片逻辑
> 2. 把分片的逻辑抽离出来, 做出一个代理的服务
> 3. 在服务端实现相关分片逻辑

### 3.1.1 在客户端实现相关分片逻辑

![Alt 'ClientShardingModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/ClientShardingModel.png)

如图: 在客户端中通过分片逻辑, 路由到具体的 Redis 组中, 具体的代表有 Jedis。

使用 ShardedJedis 之类的客户端分片代码的优势是配置简单, 不依赖于其他中间件, 分区的逻辑可以自定义, 比较灵活。  
但是基于客户端的方案, 不能实现动态的服务增减, 每个客户端需要自行维护分片策略, 存在重复代码。


### 3.1.2. 将分片逻辑抽离为一个代理服务

![Alt 'ProxyServerModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/ProxyServerModel.png)

如图: 客户连接到代理服务 (或代理服务的集群), 代理服务根据分片逻辑, 再路由到具体的 Redis 组。

典型的代理分区方案有 Twitter 开源的 Twemproxy 和国内的豌豆荚开源的 Codis

#### 3.1.2.1 Twemproxy

![Alt 'TwitterTwemproxyModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/TwitterTwemproxyModel.png)

如图就是 TwemproxyModel 常用的架构图。

优点：比较稳定，可用性高。

缺点:
> 1. 出现故障不能自动转移, 架构复杂, 需要借助其他组件 (LVS/HAProxy + Keepalived) 实现 HA
> 2. 扩缩容需要修改配置, 不能实现平滑地扩缩容 (需要重新分布数据)

#### 3.1.2.2 Codis

![Alt 'CodisModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/CodisModel.png)

如图就是 Codis 常用的架构图。

Codis 配置了 N 个槽, 槽的个数等于后面的 Redis 组的个数。Codis 对 key 进行 CRC32 运算, 得到一个 32 位的数字, 然后模以 N (槽的个数), 得到余数。  
这个余数就是对应的槽, 槽后面就是具体的 Redis 实例或 Redis 分组。  

Codis 的槽位映射关系是保存在 Proxy 中的。如果要解决单点的问题, Codis 也要做集群部署, 多个 Codis 节点之间通过一个 Coordinator (例如: ZooKeeper) 进行槽，实例之间的同步。


### 3.1.3 在服务端实现相关分片逻辑

官方实现 Redis Cluster。

Redis Cluster 是在 Redis 3.0 的版本正式推出的, 用来解决分布式的需求，同时也可以实现高可用。  
跟 Codis 不一样, 它是去中心化的, 客户端可以连接到任意一个可用节点。



数据分片有几个关键的问题需要解决
> 1. 数据怎么相对均匀地分片
> 2. 客户端怎么访问到相应的节点和数据
> 3. 重新分片的过程, 怎么保证正常服务

## 3.2 Redis Cluster

Redis Cluster 可以看成是由多个 Redis 实例组成的数据集合。客户端不需要关注数据的子集到底存储在哪个节点, 只需要关注这个集合整体。

![Alt 'RedisClusterModel'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/RedisClusterModel.png)

如图就是一个三主三从的集群架构。

### 3.2.1 Redis Cluster 的配置

搭建一个三主三从的 Redis Cluster。  
这里用一台集群搭建集群, Ip 地址为 192.169.10.10, 7000/7001/7002 为主节点的端口, 8000/8001/8002 为对应的从节点的端口

**启动节点**  

修改 redis.conf 配置文件中的 3 个参数
> 1. cluster-enabled yes
> 2. cluster-config-file "node-7000.conf"
> 3. cluster-node-timeout 5000

其他的参数和单个 Redis 实例的一样。

**cluster-enabled yes**: Redis 实例可以分为单机模式 (standalone) 和集群模式 (cluster)。 yes 开启为集群模式。 

**cluster-config-file**: 该参数指定了集群配置文件的位置。每个节点在运行过程中, 会维护一份集群配置文件; 每当集群信息发生变化时 (如增减节点), 集群内所有节点会将最新信息更新到自己维护的配置文件。   
当节点重启后, 会重新读取该配置文件, 获取集群信息, 可以方便的重新加入到集群中。  
也就是说当 Redis 节点以集群模式启动时, 会首先寻找是否有集群配置文件, 如果有则使用文件中的配置启动, 如果没有, 则初始化配置并将配置保存到文件中。 集群配置文件由 Redis 节点维护, 不需要人工修改。

**cluster-node-timeout**: 节点之间心跳超时时间

**cluster-require-full-coverage**: 默认值为 yes, 将其修改为 no, 表示 Redis 节点的槽没有完全分配时，集群仍可以上线。

通过 redis-server redis.conf 配置文件启动 Redis。

可以通过 **cluster nodes** 查看当前的节点集群信息

**执行 redis-cli --cluster create 命令**

节点启动以后是相互独立的，并不知道其他节点存在, 需要进行节点握手。将独立的节点组成一个网络。**注下面的操作, 不能使用 localhost 和 127.0.0.1, 需要使用局域网 Ip 或 公网 Ip**。

```sh
redis-cli --cluster create 192.169.10.10:7000 192.169.10.10:7001 192.169.10.10:7003 192.169.10.10:8000 192.169.10.10:8001 192.169.10.10:8002 --cluster-replicas 1
```

--cluster-replicas 1 表示每个主节点有 1 个从节点  
后面的多个 {ip:port} 表示节点地址, 前面的做主节点，后面的做从节点

执行创建命令后, Redis 会给出一个预计的方案, 对 6 个节点分配 3 主 3 从, 如果认为没有问题，输入 yes 确认

```sh
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 192.169.10.10:8001 to 192.169.10.10:7000
Adding replica 192.169.10.10:8002 to 192.169.10.10:7001
Adding replica 192.169.10.10:8003 to 192.169.10.10:7002
>>> Trying to optimize slaves allocation for anti-affinity
[WARNING] Some slaves are in the same host as their master
M: dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1c 192.169.10.10:7000
   slots:[0-5460] (5461 slots) master
M: 8c878b45905bba3d7366c89ec51bd0cd7ce959f8 192.169.10.10:7001
   slots:[5461-10922] (5462 slots) master
M: aeeb7d7076d9b25a7805ac6f508497b43887e599 192.169.10.10:7002
   slots:[10923-16383] (5461 slots) master
S: ebc479e609ff8f6ca9283947530919c559a08f80 192.169.10.10:8000
   replicates aeeb7d7076d9b25a7805ac6f508497b43887e599
S: 49385ed6e58469ef900ec48e5912e5f7b7505f6e 192.169.10.10:8001
   replicates dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1c
S: 8d6227aefc4830065624ff6c1dd795d2d5ad094a 192.169.10.10:8002
   replicates 8c878b45905bba3d7366c89ec51bd0cd7ce959f8
Can I set the above configuration? (type 'yes' to accept): 
```

### 3.2.2 Redis Cluster 相关的命令

集群命令

| 命令 |  效果 |
| :-: | :-: |
| cluster info| 打印集群的信息|
| cluster nodes | 列出集群当前已知的所有节点 (node), 以及这些节点的相关信息|
| cluster meet <ip> <port> | 将 ip 和 port 所指定的节点添加到集群当中, 让它成为集群的一份子, 这时候没有主从关系|
| cluster forget <node_id> | 从集群中移除 node_id 指定的节点 (保证空槽道) |
| cluster replicate <node_id> | 将当前节点设置为 node_id 指定的节点的从节点|
| cluster saveconfig | 将节点的配置文件保存到硬盘里面|

槽slot命令

| 命令 | 效果|
| :-: | :-:|
| cluster addslots [slot …] | 将一个或多个槽 (slot) 指派 (assign) 给当前节点|
| cluster delslots [slot …] | 移除一个或多个槽对当前节点的指派 |
| cluster flushslots | 移除指派给当前节点的所有槽, 让当前节点变成一个没有指派任何槽的节点 |
| cluster setslot node <node_id> | 将槽 slot 指派给 node_id 指定的节点, 如果槽已经指派给另一个节点, 那么先让另一个节点删除该槽, 然后再进行指派|
| cluster setslot migrating <node_id> | 将本节点的槽 slot 迁移到 node_id 指定的节点中|
| cluster setslot importing <node_id> | 从 node_id 指定的节点中导入槽 slot 到本节点 |
| cluster setslot stable | 取消对槽 slot 的导入 (imort) 或者迁移 (migrate) |

键命令

| 命令 | 效果 |
| :-: | :-: |
| cluster keyslot <key>| 计算键 key 应该被放置在哪个槽上 |
| cluster countkeysinslot <slot> | 返回槽 slot 目前包含的键值对数量 |
| cluster getkeysinslot <slot> <count> | 返回 count 个 slot 槽中的键 |



### 3.2.3 Redis Cluster 的配置方案

**高可用要求**  
根据故障转移的原理, 至少需要 3 个主节点才能完成故障转移, 且 3 个主节点不应在同一台物理机上。   
每个主节点至少需要 1 个从节点, 且主从节点不应在一台物理机上。因此高可用集群至少包含 6 个节点。

**数据量和访问量**  
估算应用需要的数据量和总访问量 (考虑业务发展, 留有冗余), 结合每个主节点的容量和能承受的访问量 (可以通过 benchmark 得到较准确估计), 计算需要的主节点数量

**节点数量限制**  
Redis 官方给出的节点数量限制为 1000, 主要是考虑节点间通信带来的消耗。在实际应用中应尽量避免大集群。  
如果节点数量不足以满足应用对 Redis 数据量和访问量的要求, 可以考虑: (1) 业务分割, 大集群分为多个小集群 (2) 减少不必要的数据 (3) 调整数据过期策略等

**适度冗余** 
Redis 可以在不影响集群服务的情况下增加节点, 因此节点数量适当冗余即可, 不用太大


### 3.2.3 Redis Cluster 的基本原理


#### 3.2.3.1 数据分区方案

数据分区有顺序分区, 哈希分区等。其中哈希分区由于其天然的随机性, 使用广泛。集群的分区方案便是哈希分区的一种。

哈希分区的基本思路是: 对数据的特征值 (如 Key) 进行哈希, 然后根据哈希值决定数据落在哪个节点。  
常见的哈希分区包括: 哈希取余分区, 一致性哈希分区, 带虚拟节点的一致性哈希分区等。

衡量数据分区方法好坏的标准有很多, 其中比较重要的两个因素是 **数据分布是否均匀**, **增加或删减节点对数据分布的影响**。  
由于哈希的随机性, 哈希分区基本可以保证数据分布均匀。 因此在比较哈希分区方案时, 重点要看增减节点对数据分布的影响。

**哈希取余分区**  
哈希取余分区思路非常简单: 计算 key 的 hash 值, 然后对节点数量进行取余, 从而决定数据映射到哪个节点上。  
该方案最大的问题是: 当新增或删减节点时, 节点数量发生变化, 系统中所有的数据都需要重新计算映射关系, 引发大规模数据迁移。

**一致性哈希分区**

![Alt 'ConsistentHashPartition'](https://raw.githubusercontent.com/PictureRespository/Redis/main/picture/ConsistentHashPartition.png)

如图, 一致性哈希算法将整个哈希值空间组织成一个虚拟的圆环。 对于每个数据, 根据 key 计算 hash 值, 确定数据在环上的位置, 然后从此位置沿环顺时针行走, 找到的第一台服务器就是其应该映射到的服务器。

与 哈希取余分区相比, 一致性哈希分区将增减节点的影响限制在相邻节点。
以上图为例, 如果在 node1 和 node2 之间增加 node4 则只有 node2 中的一部分数据会迁移到 node4。  
如果去掉 node2, 则原 node2 中的数据只会迁移到 node3 中, 只有 node3 会受影响。

**带虚拟节点的一致性哈希分区**

该方案在一致性哈希分区的基础上, 引入了虚拟节点的概念, **Redis 集群使用的便是该方案, 其中的虚拟节点称为槽 (slot)**。   
槽是介于数据和实际节点之间的虚拟概念, 每个实际节点包含一定数量的槽, 每个槽包含哈希值在**一定范围内**的数据。  
引入槽以后, 数据的映射关系由数据 hash -> 实际节点, 变成了数据 hash -> 槽 -> 实际节点。

在使用了槽的一致性哈希分区中, 槽是数据管理和迁移的基本单位。槽解耦了数据和实际节点之间的关系, 增加或删除节点对系统的影响很小。  
假设现在有 16 个槽, 有 4 个节点, 那么槽 0-4 在 1 号节点, 槽 5-8 在 2 号节点, 槽 9-12 在 3 号节点, 槽 13-16 在 4 号节点。这时候删除了 2 号节点，其自身的 4 个槽 5-8, 可以分配到剩下的节点去, 如 1 号节点保存 5, 6 槽, 3 号节点保留 7 槽, 4 号节点保留 8 槽。

#### 3.2.3.2 节点通信机制

**两个端口**  

在哨兵系统中, 节点分为数据节点和哨兵节点: 前者存储数据, 后者实现额外的控制功能。  
在集群中, 没有数据节点与非数据节点之分: 所有的节点都存储数据, 也都参与集群状态的维护。为此, 集群中的每个节点, 都提供了两个 TCP 端口。

普通端口: 即在配置时指定的端口。普通端口主要用于为客户端提供服务 (与单机节点类似), 但在节点间数据迁移时也会使用。  
集群端口: 端口号是普通端口 + 10000 (10000是固定值, 无法改变), 如 7000 节点的集群端口为 17000。集群端口只用于节点之间的通信, 如搭建集群、增减节点、故障转移等操作时节点间的通信。不要使用客户端连接集群接口。为了保证集群可以正常工作, 在配置防火墙时, 要同时开启普通端口和集群端口。

**Gossip协议**

节点间通信, 按照通信协议可以分为几种类型: 单对单、广播、Gossip协议等。  
广播是指向集群内所有节点发送消息。优点是集群的收敛速度快 (集群收敛是指集群内所有节点获得的集群信息是一致的), 缺点是每条消息都要发送给所有节点, CPU、带宽等消耗较大。  
Gossip 协议的特点是: 在节点数量有限的网络中, 每个节点都 "随机" 的与部分节点通信 (并不是真正的随机，而是根据特定的规则选择通信的节点), 经过一番杂乱无章的通信, 每个节点的状态很快会达到一致。Gossip 协议的优点有负载 (比广播) 低、去中心化、容错性高 (因为通信有冗余) 等。缺点主要是集群的收敛速度慢。

**消息类型**  
集群中的节点采用固定频率 (每秒 10 次) 的定时任务进行通信相关的工作: 判断是否需要发送消息及消息类型、确定接收节点、发送消息等。如果集群状态发生了变化, 如增减节点、槽状态变更, 通过节点间的通信, 所有节点会很快得知整个集群的状态, 使集群收敛。

节点间发送的消息主要分为 5 种: meet 消息, ping 消息, pong 消息, fail 消息, publish 消息。不同的消息类型, 通信协议, 发送的频率和时机, 接收节点的选择等是不同的。  
> 1. meet 消息: 在节点握手阶段, 当节点收到客户端的 CLUSTER MEET 命令时, 会向新加入的节点发送 MEET 消息, 请求新节点加入到当前集群; 新节点收到MEET 消息后会回复一个 PONG 消息。
> 2. ping 消息：集群里每个节点每秒钟会选择部分节点发送 ping 消息, 接收者收到消息后会回复一个 pong 消息。ping 消息的内容是自身节点和部分其他节点的状态信息, 作用是彼此交换信息, 以及检测节点是否在线。ping 消息使用 Gossip 协议发送, 接收节点的选择兼顾了收敛速度和带宽成本, 具体规则如下: (1) 随机找 5 个节点, 在其中选择最久没有通信的 1 个节点 (2) 扫描节点列表, 选择最近一次收到 PONG 消息时间大于 cluster_node_timeout/2 的所有节点, 防止这些节点长时间未更新。
> 3. pong 消息: pong 消息封装了自身状态数据。可以分为两种: 第一种是在接到 meet/ping 消息后回复的 pong消息, 第二种是指节点向集群广播 pong 消息, 这样其他节点可以获知该节点的最新信息, 例如故障恢复后新的主节点会广播 pong 消息。
> 4. fail 消息: 当一个主节点判断另一个主节点进入 fail 状态时, 会向集群广播这一 fail 消息, 接收节点会将这一 fail 消息保存起来, 便于后续的判断。
> 5. publish 消息: 节点收到 publish 命令后, 会先执行该命令, 然后向集群广播这一消息, 接收节点也会执行该 publish 命令。

**数据结构**  

节点需要专门的数据结构来存储集群的状态。所谓集群的状态, 是一个比较大的概念, 包括: 集群是否处于上线状态、集群中有哪些节点、节点是否可达、节点的主从状态、槽的分布等等。  

节点为了存储集群状态而提供的数据结构中, 最关键的是 clusterNode 和 clusterState 结构: 前者记录了一个节点的状态, 后者记录了集群作为一个整体的状态。

clusterNode 结构保存了一个节点的当前状态, 包括创建时间、节点id、ip和端口号等。每个节点都会用一个 clusterNode 结构记录自己的状态, 并为集群内所有其他节点都创建一个 clusterNode 结构来记录节点状态。

```C
typedef struct clusterNode {

   /** 节点创建时间 */
   mstime_t ctime;

   /** 节点id */
   char name[REDIS_CLUSTER_NAMELEN];

   /** 节点的 ip  */
   char ip[REDIS_IP_STR_LEN];

   /** 节点的 端口 */
   int port;

   /** 节点标识: 整型, 每个bit都代表了不同状态, 如节点的主从状态、是否在线、是否在握手等 */
   int flags;

   /** 配置纪元: 故障转移时起作用, 类似于哨兵的配置纪元 */
   uint64_t configEpoch;

   /** 槽在该节点中的分布: 占用 16384/8 个字节, 每个比特对应一个槽: 比特值为1, 则该比特对应的槽在节点中; 比特值为 0, 则该比特对应的槽不在节点中 */
   unsigned char slots[16384/8];

   /** 节点中槽的数量 */
   int numslots;
}
```

除了上述字段, clusterNode 还包含节点连接、主从复制、故障发现和转移需要的信息等

clusterState 结构保存了在当前节点视角下, 集群所处的状态。主要字段包括: 
```C
typedef struct clusterState {

   /** 自身节点 */
   clusterNode *myself;

   /** 配置纪元 */
   uint64_t currentEpoch;

   /** 集群状态: 在线还是下线 */
   int state;

   /** 集群中至少包含一个槽的节点数量 */
   int size;

   /** 哈希表, 节点名称 -> clusterNode 节点指针 */
   dict *nodes;

   /** 槽分布信息: 数组的每个元素都是一个指向 clusterNode 结构的指针。如果槽还没有分配给任何节点, 则为 NULL */
   clusterNode *slots[16384];
 
}
```

除此之外, clusterState 还包括故障转移、槽迁移等需要的信息。

**例子**  
cluster meet (节点握手)。 向 A 节点发送 cluster meet 命令, 将 B 节点加入到 A 所在的集群, 则 A 节点收到命令后

> 1. A 为 B 创建一个 clusterNode 结构, 并将其添加到 clusterState 的 nodes 字典中
> 2. A 向 B 发送 MEET 消息
> 3. B 收到 MEET 消息后, 会为 A 创建一个 clusterNode 结构, 并将其添加到 clusterState 的 nodes 字典中
> 4. B 回复 A 一个 PONG 消息
> 5. A 收到 B 的 PONG 消息后，便知道 B 已经成功接收自己的 MEET 消息
> 6. A 向 B 返回一个 PING 消息
> 7. B 收到 A 的 PING 消息后, 便知道 A 已经成功接收自己的 PONG 消息, 握手完成
> 8. A 通过 Gossip 协议将 B 的信息广播给集群内其他节点, 其他节点也会与 B 握手, 一段时间后, 集群收敛, B 成为集群内的一个普通节点

### 3.2.4 不同客户端连接 Redis Cluster 的实现

#### 3.2.4.1 Dummy 客户端

通过 Redis 自身的 redis-cli 执行命令时
> 1. 计算 key 属于哪个槽: CRC16(key) & 16383
> 2. 判断 key 所在的槽是否在当前节点: 假设 key 位于第 i 个槽, clusterState.slots[i] 则指向了槽所在的节点, 如果 clusterState.slots[i] == clusterState.myself, 说明槽在当前节点, 可以直接在当前节点执行命令。否则, 说明槽不在当前节点, 则查询槽所在节点的地址 (clusterState.slots[i].ip/port), 并将其包装到 MOVED 错误中返回给 redis-cli
> 3. redis-cli 收到 MOVED 错误后, 根据返回的 ip 和 port 重新发送请求

#### 3.2.4.2 Smart 客户端

通过 Java 中的 JedisCluster 为例

> 1. JedisCluster 初始化时, 在内部维护 slot -> node 的缓存, 方法是连接任一节点
> 2. JedisCluster 为每个节点创建连接池 (即 JedisPool) 
> 3. 当执行命令时, JedisCluster 根据 key -> slot -> node 选择需要连接的节点, 发送命令。如果成功, 则命令执行完毕。如果执行失败, 则会随机选择其他节点进行重试, 并在出现 MOVED 错误时, 使用 cluster slots 重新同步 slot -> node 的映射关系

### 3.2.5 Redis Cluster 的使用

#### 3.2.5.1 新增节点

向已有的集群添加一个节点, 主节点 192.169.10.10:7003, 从节点 192.169.10.10:8003

```sh
./redis-cli --cluster add-node 192.169.10.10:7003, 从节点 192.169.10.10:8003
```

这里需要可以先执行 **cluster info**, 获取到当前的集群信息

继续执行

```sh
./redis-cli --cluster reshard 192.169.10.10:7003
```

执行了上面的命令, 会出现好几个需要用户输入信息, 才能继续执行的过程
> 1. 待迁移的槽数量: 也就是为这个节点分配多少个槽, 16384/4 = 4096, 每个节点都是 4096, 所以输入 4096, 回车继续
> 2. 目标节点 id: 输入新加入的节点的 id
> 3. 源节点的 id: 分配的 4096 槽来至于哪些节点, 这里可以把所有存在数据的节点 id 都输入, 也可以直接输入 all

#### 3.2.5.2 下线节点

> 1. 通过 reshard 将要下线的节点的槽分配到其他节点
> 2. 通过 forget 删除节点

#### 3.2.5.3 ASK 错误

集群伸缩的核心是槽迁移。在槽迁移过程中, 如果客户端向源节点发送命令, 源节点执行流程如下: 

![Alt 'AskInRedisCluster'](https://raw.githubusercontent.com/PictureRespository/MiddleWare/main/Redis/AskInRedisCluster.png)

客户端收到 ASK 错误后, 从中读取目标节点的地址信息, 并向目标节点重新发送请求, 就像收到 MOVED 错误时一样。

但是二者有很大区别: ASK错误说明数据正在迁移, 不知道何时迁移完成, 因此重定向是临时的, SMART 客户端不会刷新 slots 缓存。
MOVED 错误重定向则是 (相对) 永久的, SMART 客户端会刷新 slots 缓存。

#### 3.2.5.4 故障转移

集群的实现与哨兵思路类似: 通过定时任务发送 PING 消息检测其他节点状态。节点下线分为主观下线和客观下线, 客观下线后选取从节点进行故障转移。  

与哨兵一样, 集群只实现了主节点的故障转移, 从节点故障时只会被下线, 不会进行故障转移。因此, 使用集群时, 应谨慎使用读写分离技术, 因为从节点故障会导致读服务不可用, 可用性变差。

大体是:
> 1. slave 发现自己的 maste 变为 Fail 状态, 偏尝试进行 Failover, 以期成为新的 master
> 2. slave 将自己记录的集群 currentEpoch + 1, 然后广播 FAILOVER_AUTH_REQUEST 信息
> 3. 其他节点收到改消息后, 只有 master 节点会进行响应, 判断请求这的合法性, 并发送 FAILOVER_AUTH_ACK, 对每一个 epoch 只发送一次 ack
> 4. 尝试 Failover 的 slave 收集 FAILOVER_AUTH_ACK
> 5. 超过半数后变成新的 master
> 6. 广播 Pong 通知其他集群节点


节点数量: 在故障转移阶段, 需要由主节点投票选出哪个从节点成为新的主节点, 从节点选举胜出需要的票数为 N/2+1, 其中 N 为主节点数量 (包括故障主节点), 但故障主节点实际上不能投票。因此为了能够在故障发生时顺利选出从节点, 集群中至少需要3个主节点 (且部署在不同的物理机上)。

故障转移时间: 从主节点故障发生到完成转移, 所需要的时间主要消耗在主观下线识别、主观下线传播、选举延迟等几个环节。具体时间与参数 cluster-node-timeout 有关, 一般来说：  
故障转移时间(毫秒) ≤ 1.5 * cluster-node-timeout + 1000  
cluster-node-timeout 的默认值为 15000ms (15s), 因此故障转移时间会在 20s 量级

#### 3.2.5.5 Hash Tag - 让数据落在同一个节点

有些 multi key 操作是不能跨阶段的, 如果要让某些数据统一分配到同一个节点上, 可以借助 Hast Tag 功能。

Hash Tag 原理是: 当一个 key 包含 {} 的时候, 不对整个 key 做 hash, 而仅对 {} 包括的字符串做 hash。  
Hash Tag 可以让不同的 key 拥有相同的 hash 值, 从而分配在同一个槽里, 这样针对不同 key 的批量操作 (mget/mset等), 
以及事务、Lua 脚本等都可以支持。  

Hash Tag 可能会带来数据分配不均的问题, 这时可以
> 1. 调整不同节点中槽的数量，使数据分布尽量均匀
> 2. 避免对热点数据使用 Hash Tag, 导致请求分布不均

### 3.2.6 Redis Cluster 的优劣

优势
> 1. 无中心架构
> 2. 数据按照 slot 存储分布在多个节点, 节点间数据共享, 可动态调整数据分布
> 3. 可扩展, 节点可以动态添加和删除节点
> 4. 高可用, 部分节点不可用时, 集群仍可用。 通过增加 slave 做 standby 数据副本, 能够实现故障字段 failover。节点之间通过 Gossop 协议交换状态信息, 用投票机制完成 slave 到 master 的角色提升
> 5. 降低运维成本, 提供系统的扩展性和可用性

不足
> 1. Client 实现复杂, 驱动要求实现 Smart Client, 缓存 slots mapping 信息并及时更新, 提高了开发难度, 客户端的不成熟影响业务的稳定性
> 2. 节点会因为某些原因发生阻塞 (阻塞时间大于 cluster-node-timeout), 被判断下线, 这种 failover 是没必要的
> 3. 数据通过异步复制, 不保证数据的强一致性
> 4. 多个业务使用同一套集群时, 无法根据统计区分冷热数据, 资源隔离性较差, 容易出现相互影响的情况
   
## 3.3 参考
[深入学习Redis（5）：集群](https://www.cnblogs.com/kismetv/p/9853040.html)
