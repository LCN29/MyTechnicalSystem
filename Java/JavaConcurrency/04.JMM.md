# 4 JMM

JMM 全称: Java Memory Model, Java 内存模式。它是一种虚拟机规范, 用于屏蔽掉各种硬件和操作系统的内存访问差异, 以实现 Java 程序在各种平台下都能达到一致的并发效果。  
主要规定了以下两点
> 1. 一个线程**如何以及何时**可以看到其他线程修改过后的共享变量的值, 即线程之间**共享变量的可见性**
> 2. 如何在需要的时候对共享变量进行同步

在并发编程中, 所要处理的两个关键问题就是这两条标准的体现: **线程之间如何通信**以及**线程之间如何同步**。  

## 4.1 线程通信
通信是指线程之间以何种机制来交换信息。在命令式的编程中, 线程之间的通信机制有两种: **共享内存**和**消息传递**。

在共享内存并发的模型里, 线程之间共享程序的公共状态, 线程之间通过读 - 写内存中的公共状态来隐式进行通信。  
在消息传递的并发模型里, 线程之间没有公共状态, 线程之间必须通过明确的发送消息来显示进行通信, 在 Java 中典型的消息传递方式就是 wait() 和 notify()。

Java 的并发采用的就是 **共享内存模型**, Java 线程之间的通信总是隐式进行的, 整个通信过程对程序员是完全透明的。这里提到的共享内存模型指的就是 Java 内存模型 (简称 JMM )。

## 4.2 线程同步
同步是指程序用于控制不同线程之间操作发生相对顺序的机制。  

在共享内存并发模型里, 同步是显式进行的。程序必须显式指定某个方法或某段代码需要在线程之间互斥执行。
在消息传递的并发模型里, 由于消息的发送必须在消息的接收之前, 因此同步是隐式进行的。

## 4.3 JMM 抽象结构模型  

JMM 定义了线程和主内存之间的抽象关系: 
线程之间的共享变量存储在主内存 (Main Memory) 中, 每个线程都有一个私有的本地内存 (Local Memory), 本地内存中存储了该线程已 读/写 共享变量的副本。  
本地内存是 JMM 的一个抽象概念, 并不真实存在。它涵盖了缓存, 写缓冲区, 寄存器以及其他的硬件和编译器优化。

![Alt 'JMMAbstractModel'](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/JMMAbstractModel.png)

从上图来看, 线程 A 与线程 B 之间如要通信的话, 必须要经历下面 2 个步骤
> 1. 线程 A 把本地内存 A 中更新过的共享变量刷新到主内存中去
> 2. 线程 B 到主内存中去读取线程 A 更新的共享变量

## 4.4 JVM 对 Java 内存模型的实现

![Alt 'JVMMemoryArea'](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/JVMMemoryArea.png)

JVM 在执行 Java 程序的过程中会把它所管理的内存划分为若干不同的数据区域, 这些区域都有各自的用途以及创建和销毁的时间。

主要包含 2 类: 
> 1. 线程共享区域: 方法区 (Method Area) 和 堆 (Heap) 
> 2. 线程私有区域: 虚拟机栈 (VM Stack) , 本地方法栈 (Native Method Stack) , 程序计数器 (PC Register) 

**虚拟机栈**   
每一个运行在 Java 虚拟机上的线程都拥有自己的线程栈。每个方法在执行的时候都会创建一个栈帧用于存储局部变量表、操作数栈、动态链表、方法出口信息等。  
每一个方法从调用直至执行完成的过程, 就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。  
虚拟机栈的生命周期与线程相同。


**本地方法栈**  
本地方法栈与虚拟机栈的作用相似, 不同之处在于虚拟机栈为虚拟机执行的 Java 方法服务, 而本地方法栈则为虚拟机使用到的 Native 方法服务

**程序计数器**   
程序计数器保存着每一条线程下一次执行指令位置

**堆**   
用来保存程序中所创建的所有对象、数组元素等

**方法区**   
方法区是各个线程共享的内存区域, 它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据

### 4.4.1 数据存储总结
> 1. 一个本地变量是原始类型, 它会被完全存储到栈区
> 2. 一个本地变量是引用类型, 这个本地引用会被存储到栈中, 但是对象本身仍然存储在堆区
> 3. 一个对象的成员方法, 方法中包含局部变量, 存储在栈区
> 4. 一个对象的成员变量, 不管它是原始类型还是包装类型, 都存储到堆区
> 5. static 类型的变量以及类本身相关信息都会随着类本身存储在方法区

## 4.5 JMM 带来的问题

### 4.5.1 可见性问题

![Alt 'ShardVariableVisibilityProblem'](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/ShardVariableVisibilityProblem.png)

如上图, 3 个 共享变量 count, 2 个为副本。  
启动 2 个线程分别对共享变量操作, 假设原本共享变量 count 为 0。
> 1. 线程 A 从主内存将这个共享变量 count 加载到自己的本地内存, 值为 0
> 2. 线程 B 执行同样的加载操作, 值为 0
> 3. 线程 A 对这个共享变量 count + 1, count 的值变为 1, 没有将这个值同步到主内存
> 4. 线程 B 这时候同样需要对这个共享变量 count + 1, 但是这时候 B 中的 count 还是 0, 没有感知到 A 对其做的修改

在多线程的环境下, 如果某个线程首次读取共享变量, 则首先到主内存中获取该变量, 然后存入工作内存中, 以后只需要在工作内存中读取该变量即可。  
同样如果对该变量执行了修改的操作, 则先将新值写入工作内存中, 然后再刷新至主内存中, 这个刷新时间虽然很短但并不确定。

### 4.5.2 竞争问题

![Alt 'SharedVariableCompetitionProblem'](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/SharedVariableCompetitionProblem.png)

如上图:  
如果这两个加 1 操作是串行的, 最终主内存中的 count 的值应该是 3。  
然而图中两个加 1 操作是并行的, 当它们值更新到工作内存的副本后, 会争相刷新主内存。在这里, 不管是线程 1 还是线程 2 先刷新计算结果到主内存, 最终主内存中的值只能是 2。

### 4.5.3 重排序

可见性和竞争都是由共享内存和工作内存带来的。在并发中, 除了这 2 个地方会引起问题外, 在编译器中还存在重排序问题: 在执行程序时, 为了提高性能, 编译器和处理器常常会对指令做重排序。  
这些系统内部的优化大部分都是有效的, 但是有时在并发编程中, 则会带来某些问题。

#### 4.5.3.1 重排序的类型

一个好的内存模型实际上会放松对处理器和编译器规则的束缚, 也就是说软件技术和硬件技术都为同一个目标而进行奋斗: 在不改变程序执行结果的前提下, 尽可能提高并行度。  
JMM 对底层尽量减少约束, 使其能够发挥自身优势。因此, 在执行程序时, **为了提高性能, 编译器和处理器常常会对指令进行重排序**。一般重排序可以分为如下三种:

![Alt "InstructReorderType"](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/InstructReorderType.png)

> 1. 编译器优化的重排序。编译器在不改变单线程程序语义的前提下, 可以重新安排语句的执行顺序
> 2. 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果**不存在数据依赖性**, 处理器可以改变语句对应机器指令的执行顺序
> 3. 内存系统的重排序。由于处理器使用缓存和读 / 写缓冲区, 这使得加载和存储操作看上去可能是在乱序执行的。

注: 指令并行重排序和内存系统重排序统称为处理器排序

#### 4.5.3.2 重排序的影响

我们知道重排序是编译器或者系统的优化。但是如果有些指令存在依赖性的话, 进行重排序会导致错误。

**数据依赖性**   
如果两个操作访问同一个变量, 且这两个操作中有一个为写操作, 此时这两个操作之间就存在数据依赖性。数据依赖分为下列 3 种类型, 这 3 种情况, 只要重排序两个操作的执行顺序, 程序的执行结果就会被改变。

|名称 | 说明 | 示例|
| :-: | :-: | :-: |
|写后读 | 写一个变量, 再读这个变量  | a = 1; b = a; |
|写后写| 写一个变量, 再写这个变量 | a = 1; a = 2; |
|读后写| 读一个变量, 再写这个变量 | a = b; b = 1; |

这三种操作都存在数据依赖性, 如果重排序最终会导致结果受到影响。  

**控制依赖性**

```java
public void method() {
    
    if (flag) {
        int num = a * a;
        return num;
    }
}
```

在上面的代码中, 变量 num 的值依赖于 if(flag) 的判断值, 这里就叫控制依赖。

控制依赖在单线程的情况下, 对存在控制依赖的操作重排序, 不会改变执行结果。但是在多线程的情况下, 就有可能出现问题。

```java
public class Demo {
    int a = 0;
    boolean flag = false;
    
    public void init() {
        // 1
        a = 1;
        // 2
        flag = true;
    }
    
    public void use() {
        // 3
        if (flag) {
            // 4
            int i = a * a;
        }
    }
}
```

在程序中, 当代码中存在控制依赖性时, 会影响指令序列执行的并行度。为此, 编译器和处理器会采用猜测 (Speculation) 执行来克服控制相关性对并行度的影响。  
以处理器的猜测执行为例, 执行线程 B 的处理器可以提前读取并计算 a*a, 然后把计算结果临时保存到一个名为重排序缓冲 (Reorder Buffer, ROB) 的硬件缓存中。  
当操作 3 的条件判断为真时, 就把该计算结果写入变量 i 中。猜测执行实质上对操作 3 和 4 做了重排序, 问题在于这时候, a 的值还没被线程 A 赋值。

当操作 1 和操作 2 重排序, 操作 3 和操作 4 重排序时, 可能会产生什么效果?   
操作 1 和操作 2 做了重排序。程序执行时, 线程 A 首先写标记变量 flag, 随后线程 B 读这个变量。由于条件判断为真, 线程 B 将读取变量 a。  
此时, 变量 a 还没有被线程 A 写入, 这时就会发生错误！

所以在多线程程序中, 对存在控制依赖的操作重排序, 可能会改变程序的执行结果。

#### 4.5.3.3 禁止重排序

通过上面的分析, 重排序可能导致线程安全的问题。所以对于重排序在编译器做了一些限制
> 1. 针对编译器重排序, JMM 的编译器重排序规则会禁止一些特定类型的编译器重排序
> 2. 针对处理器重排序, 编译器在生成指令序列的时候会通过插入**内存屏障指令**来禁止某些特殊的处理器重排序

当然重排序不是随意的, 只有在数据没有任何依赖性时, 才可能发生重排序

```java
// A
double pai = 3.14;
// B
double r = 1;
//
double area = pai * r * r;
```
上面是计算圆面积的代码, 代码 A, B 之间没有任何的关联, 所以 2 者之间可以重排序。  
也可以说 A 和 B 之间没有**数据依赖性**, 具体的定义为**如果两个操作访问同一个变量, 且这两个操作有一个为写操作, 此时这两个操作就存在数据依赖性**。  
这里可以分为 3 种情况: 读后写 / 写后读 / 写后写, 这 3 种情况, 无论哪一种发生了重排序, 最终执行结果会存在影响。

最终的结论为 **编译器和处理器在重排序时, 会遵守数据依赖性, 编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序**。

#### 4.5.3.3 as-if-serial

as-if-serial 是一个和重排序相关的一个语义, 器内容大体如下:   
不管怎么重排序, (单线程) 程序的执行结果不能被改变。 编译器, runtime 和处理器都必须遵守这个语义。  

为了遵守 as-if-serial 语义, 编译器和处理器不会对存在数据依赖关系的操作做重排序, 因为这种重排序会改变执行结果。  
但是, 如果操作之间不存在数据依赖关系, 这些操作就可能被编译器和处理器重排序。  

**as-if-serial** 语义把单线程程序保护了起来。遵守 as-if-serial 语义的编译器, runtime 和处理器可以让我们感觉到: **单线程程序看起来是按程序的顺序来执行的**。  
as-if-serial 语义使单线程程序员无需担心重排序会干扰他们, 也无需担心内存可见性问题。  

#### 4.5.3.4 内存屏障 (Memory Barrier)

Java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序, 从而让程序按我们预想的流程去执行
> 1. 保证特定操作的执行顺序
> 2. 影响某些数据 (或则是某条指令的执行结果) 的内存可见性

编译器和 CPU 能够重排序指令, 保证最终相同的结果前提, 尝试优化性能。  
插入一条 Memory Barrier 会告诉编译器和 CPU: 不管什么指令都不能和这条 Memory Barrier 指令重排序。

Memory Barrier 所做的另外一件事是强制刷出各种 CPU Cache, 如一个 Write-Barrier (写入屏障) 将刷出所有在 Barrier 之前写入 cache 的数据。  
因此, 任何 CPU 上的线程都能读取到这些数据的最新版本。

JMM 把内存屏障指令分为4类:

| 屏障类型| 说明 | 实例 |
| :-: | :-: | :-: |
| LoadLoadBarriers| Load1; LoadLoadBarriers; Load2 | 确保 Load1 数据的读取在 load2 及后续指令的读取之前 |
| StoreStoreBarriers | Store1; StoreStoreBarriers; Store2; | 确保 Store1 数据的写入 (刷新到主内存) 在 Store2 及后续指令的读取之前 |
| LoadStoreBarriers| Load1; LoadStoreBarriers; Store2; | 确保 Load1 数据的读取在 Store2 及后续指令的写入之前 |
| StoreLoadBarriers | Store1; StoreLoadBarriers; Load2; | 确保 Store2 数据的写入在 Load2 及后续指令的读取之前 |

StoreLoad Barriers 是一个 "全能型" 的屏障, 它同时具有其他 3 个屏障的效果。现代的多处理器大多支持该屏障 (其他类型的屏障不一定被所有处理器支持) 。

## 4.6 Happens-Before

在上面的分析中, 涉及了大量的底层知识, 如果在编程中需要考虑这么多底层的知识, 那么对于编写程序的人的负担是很大的。  
因此, JMM 为程序员在提供 Happens-Before 的概念来阐述操作之间的内存可见性。 为程序员屏蔽了底层相关的内容, 完全可以根据规则去推论跨线程的内存可见性问题, 而不用再去理解底层重排序的规则。

JMM 这么做的原因是: 程序员对于这两个操作是否真的被重排序并不关心, 程序员关心的是程序执行时的语义不能被改变 (即执行结果不能被改变)。  
因此, Happens-Before 关系本质上和 as-if-serial 语义是一回事。  
as-if-serial 语义保证单线程内程序的执行结果不被改变, 而 Happens-Before 关系保证正确同步的多线程程序的执行结果不被改变。

### 4.6.1 定义
> 1. 如果一个操作 Happens-Before 另一个操作, 那么第一个操作的执行结果将对第二个操作可见, 而且第一个操作的执行顺序排在第二个操作之前
> 2. 两个操作之间存在 Happens-Before 关系, 并不意味着 Java 平台的具体实现必须要按照 Happens-Before 关系指定的顺序来执行。如果重排序之后的执行结果, 与按 Happens-Before 关系来执行的结果一致, 那么这种重排序并不非法 (也就是说, JMM 允许这种重排序) 

第一条是 JMM 对程序员的保证。如果 A Happens-Before B, 那么 JMM 将向程序员保证 —— A 操作的结果将对 B 可见, 且 A 的执行顺序排在 B 之前。

第二条是 JMM 对编译器和处理器重排序的约束原则。 JMM 允许, 两个操作之间存在 Happens-Before 关系, 不要求 Java 平台的具体实现必须要按照 Happens-Before 关系指定的顺序来执行。如果重排序之后的执行结果, 与按 Happens-Before 关系来执行的结果一致, 那么这种重排序是允许的。

### 4.6.2 as-if-serial VS Happens-Before

> 1. as-if-serial 语义保证单线程内程序的执行结果不被改变, Happens-Before 关系保证正确同步的多线程程序的执行结果不被改变
> 2. as-if-serial 语义给编写单线程程序的程序员创造了一个感觉: 单线程程序是按程序的顺序来执行的。Happens-Before 关系给编写正确同步的多线程程序的程序员创造了一个感觉: 正确同步的多线程程序是按 Happens-Before 指定的顺序来执行的。
> 3. as-if-serial 语义和 Happens-Before 这么做的目的, 都是为了在不改变程序执行结果的前提下, 尽可能地提高程序执行的并行度。  

### 4.6.3 具体的规则

**1. 程序顺序规则 (Program Order Rule)**: 一个线程中的每个操作, Happens-Before 于该线程中的任意后续操作。

个人理解:  
```java
// 情况一: 
int a = 1;
int b = a + 1;

// 情况二:
int c = 1;
int d = 2;
int e = c + d;
```

情况一: 同一个线程中的操作存在依赖性 (b 的值依赖于 a), 那么前面的动作的操作结果对于后面的动作可见。

情况二: 同一个线程的操作不存在依赖性 (c, d 直接没有什么依赖关系) , 允许重排序, 只要这种重排序不违反 **程序顺序规则**。( e 依赖于 c 和 d 的值, 所以 c 和 d 之间可以重排序, 但是不能跨 e 排序, e 的结果依赖于 c, d)  
也就是: 在同一个线程里面: 编译器未必一定会按照这个代码的顺序进行编译, 但是编译器保证: 最终的结果等于顺序执行的结果。

**2. 监视器锁规则 (Monitor Lock Rule)**: 同一个锁的 unlock 操作 Happens-Before 此锁的 lock 操作。

个人理解: 
unlock 和 lock 都是针对同一个锁实例。线程 A 释放了锁 l, 接着线程 B 锁定了锁 l。那么线程 A 解锁和解锁前的操作对线程 B 可见 (线程 A 和 B 可以为同一个线程)。

**3. volatile变量规则 (volatile Variable Rule)**: 对一个 volatile 变量的写操作 Happens-Before 后续每一个针对该变量的读操作。

个人理解: 
线程 A 对 volatile 修饰的变量 v 进行操作, 线程 B (可以是同一个线程, 也就是线程 A) 对变量 v 进行读取, 那么线程 A 对变量 v 的操作的结果, 对线程 B 是可见的。

**4. 线程启动规则 (Thread Start Rule)**: Thread 对象的 start() 方法和书写在 start 方法前面的代码 Happens-Before 此线程的每一个动作

个人理解: 
线程 T1 执行了一段业务代码后, 调用了线程 T2 的 start 方法。那么 T1 执行的业务代码的结果对于线程 T2 可见。

**5. 线程终止规则 (Thread Termination Rule)**: 线程中的任何操作都 Happens-Before 其它线程检测到该线程已经结束

个人理解: 
一个线程结束了, 在他结束的时, 他的所有操作结果对其他线程可见。例如: 2 个线程 A, B。在线程 A 调用 B.join(), 这时 A 会被挂起, 等待线程 B 执行完成才恢复。当 A 收到 B.join() 返回时, B 线程结束了。那么在 B 执行的这段时间所有的操作结果对 A 可见。

**6. 线程中断规则 (Thread Interruption Rule)**: 对线程 interrupt() 方法的调用 Happens—Before 被中断线程的代码检测到中断时间的发生。

个人理解: 
线程 A 调用了线程 B 的 interrupt(), 线程 B 才能收到线程中断信号, 不能存在线程 B 收到线程中断信号, 而线程 A 还未执行 interrupt() 方法。 线程代码检测到中断时间的发生, 可以简单的理解为调用了 Thread.isinterrupted() 方法返回了 true。

**7. 对象终结规则 (Finalizer Rule)**: 一个对象的初始化完成 (构造函数执行结束) Happens-Before 它的 finalize() 方法的开始

**8. 传递性 (Transitivity)**: 如果操作 A Happens-Before B, B Happens-Before C, 那么可以得出操作 A Happens-Before C

### 4.6.4 Happens-Before 规则的真正意义

我们已经知道, 导致多线程间可见性问题的两个 “罪魁祸首” 是 CPU 缓存和重排序。那么如果要保证多个线程间共享的变量对每个线程都及时可见, 一种极端的做法就是禁止使用所有的重排序和 CPU 缓存。  
即关闭所有的编译器、操作系统和处理器的优化, 所有指令顺序全部按照程序代码书写的顺序执行。去掉 CPU 高速缓存, 让 CPU 的每次读写操作都直接与主存交互。  
当然, 上面的这种极端方案是绝对不可取的, 因为这会极大影响处理器的计算性能, 并且对于那些非多线程共享的变量是不公平的。  

重排序和 CPU 高速缓存有利于计算机性能的提高, 但却对多 CPU 处理的一致性带来了影响。为了解决这个矛盾, 我们可以采取一种折中的办法。  
我们用分割线把整个程序划分成几个程序块, 在每个程序块内部的指令是可以重排序的, 但是分割线上的指令与程序块的其它指令之间是不可以重排序的。在一个程序块内部, CPU 不用每次都与主内存进行交互, 只需要在 CPU 缓存中执行读写操作即可。  
但是当程序执行到分割线处, CPU 必须将执行结果同步到主内存或从主内存读取最新的变量值。那么, Happens-Before 规则就是定义了这些程序块的分割线。  

下图展示了一个使用锁定原则作为分割线的例子: 

![Alt 'HappenBeforeDemoAboutLockRuleMonitor'](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/HappenBeforeDemoAboutLockRuleMonitor.png)

如图所示, 这里的 unlock M 和 lock M 就是划分程序的分割线。在这里, 红色区域和绿色区域的代码内部是可以进行重排序的, 但是 unlock 和 lock 操作是不能与它们进行重排序的。  
即第一个图中的红色部分必须要在 unlock M 指令之前全部执行完, 第二个图中的绿色部分必须全部在 lock M 指令之后执行。  
并且在第一个图中的 unlock M 指令处, 红色部分的执行结果要全部刷新到主存中, 在第二个图中的 lock M 指令处, 绿色部分用到的变量都要从主存中重新读取。

在程序中加入分割线将其划分成多个程序块, 虽然在程序块内部代码仍然可能被重排序, 但是保证了程序代码在宏观上是有序的。并且可以确保在分割线处, CPU 一定会和主内存进行交互。Happens-Before 原则就是定义了程序中什么样的代码可以作为分隔线。
并且无论是哪条 Happens-Before 原则, 它们所产生分割线的作用都是相同的。

## 4.7 JMM 的设计

站在 JMM 设计者的角度, 在设计 JMM 时需要考虑两个关键因素:

**1. 程序员对内存模型的使用**  
程序员希望内存模型易于理解、易于编程。程序员希望基于一个强内存模型来编写代码。

**2. 编译器和处理器对内存模型的实现**  
编译器和处理器希望内存模型对它们的束缚越少越好, 这样它们就可以做尽可能多的优化来提高性能。编译器和处理器希望实现一个弱内存模型。

另外还有一个事: Happens-Before 关系的处理的重排序可以分为两类
> 1. 会改变程序执行结果的重排序
> 2. 不会改变程序执行结果的重排序

JMM 对这两种不同性质的重排序, 采取了不同的策略, 如下:
> 1. 对于会改变程序执行结果的重排序, JMM 要求编译器和处理器必须禁止这种重排序
> 2. 对于不会改变程序执行结果的重排序, JMM 对编译器和处理器不做要求

JMM 的设计图为: 

![Alt 'JMMDesignDrawing'](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/JMMDesignDrawing.png)

从图可知道: 

JMM 向程序员提供的 Happens-Before 规则能满足程序员的需求。JMM 的 Happens-Before 规则不但简单易懂, 而且也向程序员提供了足够强的内存可见性保证。

JMM 对编译器和处理器的束缚已经尽可能少。  
从上面的分析可以看出, JMM 其实是在遵循一个基本原则: 只要不改变程序的执行结果 (指的是单线程程序和正确同步的多线程程序) , 编译器和处理器怎么优化都行。  
例如, 如果编译器经过细致的分析后, 认定一个锁只会被单个线程访问, 那么这个锁可以被消除。再如, 如果编译器经过细致的分析后, 认定一个 volatile 变量只会被单个线程访问, 那么编译器可以把这个 volatile 变量当作一个普通变量来对待。  
这些优化既不会改变程序的执行结果, 又能提高程序的执行效率。

### 4.7.1 JMM 和 Happen-Before 的关系

![Alt 'JMMAndHappensBeforeRelation'](https://raw.githubusercontent.com/PictureRespository/Java/main/JavaConcurrency/JMMAndHappensBeforeRelation.png)

一个 Happens-Before 规则对应于一个或多个编译器和处理器重排序规则。  
对于 Java 程序员来说, Happens-Before 规则简单易懂, 它避免 Java 程序员为了理解 JMM 提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现方法。

参考  
《Java 多线程编程实践实战指南 (核心篇) 》  
[JMM的介绍](https://github.com/CL0610/Java-concurrency/blob/master/03.java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8Ahappens-before%E8%A7%84%E5%88%99/Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8Ahappens-before.md)   
[再有人问你Java内存模型是什么, 就把这篇文章发给他。](https://www.hollischuang.com/archives/2550)  
[Java 内存模型详解](https://juejin.im/post/5d3eafe95188255d3d296e09)  
[java内存模型以及happens-bofore原则](https://www.codercc.com/post/812b4c63.html)  
[JMM和底层实现原理](https://www.jianshu.com/p/8a58d8335270)  
[大厂很可能会问到的JMM底层实现原理](https://blog.csdn.net/weixin_44367006/article/details/99656558)  
[从Java多线程可见性谈Happens-Before原则](https://segmentfault.com/a/1190000011458941)


